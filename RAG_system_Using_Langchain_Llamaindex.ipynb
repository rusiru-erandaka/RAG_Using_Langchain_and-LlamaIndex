{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "49fd0a22a703476a98bd316e9261ed74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c0932b1a3314525bd5c9d30d18a11ce",
              "IPY_MODEL_2e6588199da84d258391c37bdd986829",
              "IPY_MODEL_8541fe90412f4b12b8b433d4ad12c328"
            ],
            "layout": "IPY_MODEL_ca6d6668f730434588235d0c9b02dcef"
          }
        },
        "6c0932b1a3314525bd5c9d30d18a11ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a4e9c5b02f74e8a8518733fb3e498cb",
            "placeholder": "​",
            "style": "IPY_MODEL_3f18e9580002401d80fbe78148185a0d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2e6588199da84d258391c37bdd986829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_660eee45c00142b4aba235fba23919b3",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a37a0e09c1e4b719ad2e90a2a6a14cc",
            "value": 3
          }
        },
        "8541fe90412f4b12b8b433d4ad12c328": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1518dac52df455b82fe68ebce94fe8a",
            "placeholder": "​",
            "style": "IPY_MODEL_ef708cb61047483e85ac6a70241febd8",
            "value": " 3/3 [01:12&lt;00:00, 23.63s/it]"
          }
        },
        "ca6d6668f730434588235d0c9b02dcef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a4e9c5b02f74e8a8518733fb3e498cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f18e9580002401d80fbe78148185a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "660eee45c00142b4aba235fba23919b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a37a0e09c1e4b719ad2e90a2a6a14cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b1518dac52df455b82fe68ebce94fe8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef708cb61047483e85ac6a70241febd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd2134e6a93b4782a621e5f846cfda66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_029c3a2d739b49219d34349bcb764616",
              "IPY_MODEL_adef4b2a3d4247e4993232cd16bc2ebb",
              "IPY_MODEL_88a4335481a74622ad6c25baf6d9c807"
            ],
            "layout": "IPY_MODEL_b234f89a3d9246828f0521c64ab8ad51"
          }
        },
        "029c3a2d739b49219d34349bcb764616": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac299f00366e44bd911585a6dea563de",
            "placeholder": "​",
            "style": "IPY_MODEL_c64ce4ea9b4945638cd6ea32e757d461",
            "value": "modules.json: 100%"
          }
        },
        "adef4b2a3d4247e4993232cd16bc2ebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3398db0639e41189ea15ee105db2413",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ad406d7fdea462c9df688ed8ed52946",
            "value": 349
          }
        },
        "88a4335481a74622ad6c25baf6d9c807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1972cf2036045269b46f9ff4949fc0e",
            "placeholder": "​",
            "style": "IPY_MODEL_f60246288cb144f3923dc93c7d3b28cf",
            "value": " 349/349 [00:00&lt;00:00, 20.9kB/s]"
          }
        },
        "b234f89a3d9246828f0521c64ab8ad51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac299f00366e44bd911585a6dea563de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c64ce4ea9b4945638cd6ea32e757d461": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3398db0639e41189ea15ee105db2413": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ad406d7fdea462c9df688ed8ed52946": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1972cf2036045269b46f9ff4949fc0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f60246288cb144f3923dc93c7d3b28cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d25771935f04b58bcbceba442e9aa38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43107ac58603417b8d969172a4111052",
              "IPY_MODEL_0ac106b655bf490e8c0e5d1968eab211",
              "IPY_MODEL_a68f2ceb5b204c1da8e94aee985ad953"
            ],
            "layout": "IPY_MODEL_eb5066bddfd049ed8c7e1c8a5ab2d295"
          }
        },
        "43107ac58603417b8d969172a4111052": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22dd85941b444d18b8cc86c85ca7ad81",
            "placeholder": "​",
            "style": "IPY_MODEL_8b534547180f44f792d95d15f419c586",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "0ac106b655bf490e8c0e5d1968eab211": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10365bca7c754f9c9ff1a9ca4591517d",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f8e3c5ef0ad4bdea8b74ed859adc3a8",
            "value": 116
          }
        },
        "a68f2ceb5b204c1da8e94aee985ad953": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b4c9691a7484215859acb0a6b2d279d",
            "placeholder": "​",
            "style": "IPY_MODEL_90baffc645154e878945bbcb24565718",
            "value": " 116/116 [00:00&lt;00:00, 10.3kB/s]"
          }
        },
        "eb5066bddfd049ed8c7e1c8a5ab2d295": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22dd85941b444d18b8cc86c85ca7ad81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b534547180f44f792d95d15f419c586": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10365bca7c754f9c9ff1a9ca4591517d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f8e3c5ef0ad4bdea8b74ed859adc3a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b4c9691a7484215859acb0a6b2d279d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90baffc645154e878945bbcb24565718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bafa1024bf34306b1f4a2436169ac34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0e829d07e764b9f816265336930fe69",
              "IPY_MODEL_5d8578a89e9f497fbfa7310012be1b3e",
              "IPY_MODEL_979ff162edbd48dcb70a289594ab6b9c"
            ],
            "layout": "IPY_MODEL_5f70d34a3c094d36b11ead728a120431"
          }
        },
        "e0e829d07e764b9f816265336930fe69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1f4928df1fd4fa6bc21abd09c1a4d1b",
            "placeholder": "​",
            "style": "IPY_MODEL_aca7c1e6df67444989400763f9ee20b8",
            "value": "README.md: "
          }
        },
        "5d8578a89e9f497fbfa7310012be1b3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f68c1d4fbe3548a689e85fd1f9872a3b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3bf887e6344348fe8e1f28f81446579a",
            "value": 1
          }
        },
        "979ff162edbd48dcb70a289594ab6b9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7a8f89a6e7b4276a8e1d41efed84bb0",
            "placeholder": "​",
            "style": "IPY_MODEL_2e955c31f097427181c3aca0f87b310d",
            "value": " 11.6k/? [00:00&lt;00:00, 706kB/s]"
          }
        },
        "5f70d34a3c094d36b11ead728a120431": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1f4928df1fd4fa6bc21abd09c1a4d1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aca7c1e6df67444989400763f9ee20b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f68c1d4fbe3548a689e85fd1f9872a3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "3bf887e6344348fe8e1f28f81446579a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e7a8f89a6e7b4276a8e1d41efed84bb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e955c31f097427181c3aca0f87b310d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6910efaac6b64421b78576dc53a6372d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a0e8c788dc4a466a9e903449987048c5",
              "IPY_MODEL_d6397ab75dc843eebed37ec6e40ab970",
              "IPY_MODEL_0afd88d2da8b4cc0b5de3754e6f9db62"
            ],
            "layout": "IPY_MODEL_b5bb0860480a4708ad5b1c45213d121c"
          }
        },
        "a0e8c788dc4a466a9e903449987048c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22c3b48b90d445d281e98b761f4d7361",
            "placeholder": "​",
            "style": "IPY_MODEL_965911bd10bc45d0b663e110adee3b4f",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "d6397ab75dc843eebed37ec6e40ab970": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfc1e79d0cc0486195658c287f0abf5f",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e50729ace2b494c9da2527c553450a2",
            "value": 53
          }
        },
        "0afd88d2da8b4cc0b5de3754e6f9db62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5ea6257ac0145dc9792b769363692d4",
            "placeholder": "​",
            "style": "IPY_MODEL_8b8a01a1f69e4b919e1b09774b78d663",
            "value": " 53.0/53.0 [00:00&lt;00:00, 4.29kB/s]"
          }
        },
        "b5bb0860480a4708ad5b1c45213d121c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22c3b48b90d445d281e98b761f4d7361": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "965911bd10bc45d0b663e110adee3b4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dfc1e79d0cc0486195658c287f0abf5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e50729ace2b494c9da2527c553450a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5ea6257ac0145dc9792b769363692d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b8a01a1f69e4b919e1b09774b78d663": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d454ce9842b469aac37f61c5528f94d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f339376e683547b29a2187c72c975e68",
              "IPY_MODEL_6dad64b3b7294cb4a8ac45c4e0fc6e91",
              "IPY_MODEL_c797014e38a4423daf5baef827d5c4b0"
            ],
            "layout": "IPY_MODEL_8dd118b46068440c95fbc3a7e5aa4ea6"
          }
        },
        "f339376e683547b29a2187c72c975e68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20c9ea8706734da9b4ae5dec93c0064d",
            "placeholder": "​",
            "style": "IPY_MODEL_fb6d6ac09d7541188aefff77de117bb1",
            "value": "config.json: 100%"
          }
        },
        "6dad64b3b7294cb4a8ac45c4e0fc6e91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5c95f1e45c54bbc98e5ec0409148991",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e691d9ec1564da9ad7a0d327fbb4cbd",
            "value": 571
          }
        },
        "c797014e38a4423daf5baef827d5c4b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca684af7fcc64a9dbf1f39f056d7f62b",
            "placeholder": "​",
            "style": "IPY_MODEL_4b2c2d57de9545ccb57bc693c2568bea",
            "value": " 571/571 [00:00&lt;00:00, 29.5kB/s]"
          }
        },
        "8dd118b46068440c95fbc3a7e5aa4ea6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20c9ea8706734da9b4ae5dec93c0064d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb6d6ac09d7541188aefff77de117bb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5c95f1e45c54bbc98e5ec0409148991": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e691d9ec1564da9ad7a0d327fbb4cbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca684af7fcc64a9dbf1f39f056d7f62b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b2c2d57de9545ccb57bc693c2568bea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c6eb26aa29a4afda69adb849a887216": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_66099114eb504dcb94e5a03463e3262b",
              "IPY_MODEL_a319ec6da6ce44e5a59650845c88f15a",
              "IPY_MODEL_c6736e9129324b2a85afdc27847b1322"
            ],
            "layout": "IPY_MODEL_9f7bf65d4b3d48f2bd50a95dd5ff298e"
          }
        },
        "66099114eb504dcb94e5a03463e3262b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa01e8198277423db3fcd402d22a8686",
            "placeholder": "​",
            "style": "IPY_MODEL_371797d2729f4e84aa77b895dd110741",
            "value": "model.safetensors: 100%"
          }
        },
        "a319ec6da6ce44e5a59650845c88f15a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bf1af5ac103433d95b00500316354b8",
            "max": 437971872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_153213fe543b4ee7b0a34f58201534d0",
            "value": 437971872
          }
        },
        "c6736e9129324b2a85afdc27847b1322": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55997742487047088d85891f6f0ffdd2",
            "placeholder": "​",
            "style": "IPY_MODEL_f87dba09070549e8b2f80713588fd6b4",
            "value": " 438M/438M [00:02&lt;00:00, 245MB/s]"
          }
        },
        "9f7bf65d4b3d48f2bd50a95dd5ff298e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa01e8198277423db3fcd402d22a8686": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "371797d2729f4e84aa77b895dd110741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bf1af5ac103433d95b00500316354b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "153213fe543b4ee7b0a34f58201534d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55997742487047088d85891f6f0ffdd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f87dba09070549e8b2f80713588fd6b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2b4b1733e5b47fb99f3b17ac8f18c4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c6c7e43545e4e21832d9835a610956e",
              "IPY_MODEL_23360c8b656b4a00940a7384f6bd4d41",
              "IPY_MODEL_b1d679b4ecdd40d280059094ae5a9fb5"
            ],
            "layout": "IPY_MODEL_64130740650649bf9f4f8eb0056151d6"
          }
        },
        "4c6c7e43545e4e21832d9835a610956e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcb41b6dfb774e6ca16a36ace8d5f077",
            "placeholder": "​",
            "style": "IPY_MODEL_bbc62430d9f14704b4606fab90edb024",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "23360c8b656b4a00940a7384f6bd4d41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83ed99d9525742b3b9951e7cdb52fec8",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed322c2276cd41e8a205063f8b6578cd",
            "value": 363
          }
        },
        "b1d679b4ecdd40d280059094ae5a9fb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7e01b5258e94596b5e22f3a086878f1",
            "placeholder": "​",
            "style": "IPY_MODEL_90077be705f142b8905f6adad8cd9294",
            "value": " 363/363 [00:00&lt;00:00, 26.9kB/s]"
          }
        },
        "64130740650649bf9f4f8eb0056151d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcb41b6dfb774e6ca16a36ace8d5f077": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbc62430d9f14704b4606fab90edb024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83ed99d9525742b3b9951e7cdb52fec8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed322c2276cd41e8a205063f8b6578cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7e01b5258e94596b5e22f3a086878f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90077be705f142b8905f6adad8cd9294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfd55beb52034cf99d7ed758f306198f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1af68146784f4386a63709ab39aa2c81",
              "IPY_MODEL_8d5080a5563848b598ac402c632e20fc",
              "IPY_MODEL_77c8efd93702405399ce607dde784d98"
            ],
            "layout": "IPY_MODEL_b6d5bf55d810409899cd4882c09f2e8a"
          }
        },
        "1af68146784f4386a63709ab39aa2c81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1393e2465af41d9bb7d1c5f3ae675b4",
            "placeholder": "​",
            "style": "IPY_MODEL_c995725e9a1a4407962e4d61c0a7207d",
            "value": "vocab.txt: "
          }
        },
        "8d5080a5563848b598ac402c632e20fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef4dd98b26ee476c8cf1e6586e7dce9b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9855a3c97d784f8aa04dd053d0e21f08",
            "value": 1
          }
        },
        "77c8efd93702405399ce607dde784d98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83f4587b85364b6da8ea0171ec16dbb9",
            "placeholder": "​",
            "style": "IPY_MODEL_0c6838823c8740568af6b3f8638217dd",
            "value": " 232k/? [00:00&lt;00:00, 12.5MB/s]"
          }
        },
        "b6d5bf55d810409899cd4882c09f2e8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1393e2465af41d9bb7d1c5f3ae675b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c995725e9a1a4407962e4d61c0a7207d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef4dd98b26ee476c8cf1e6586e7dce9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "9855a3c97d784f8aa04dd053d0e21f08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83f4587b85364b6da8ea0171ec16dbb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c6838823c8740568af6b3f8638217dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bb540da26fd43718874ee412466d5cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7af6c630b2145ef95e37bb590b75c09",
              "IPY_MODEL_35e70828b8b64670adb7e201b20fc4ed",
              "IPY_MODEL_6b86b57e43ba4d37bbc18a48b3a7ae4f"
            ],
            "layout": "IPY_MODEL_4fd7a6f76f664a2e8943483097100d5f"
          }
        },
        "c7af6c630b2145ef95e37bb590b75c09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5256782bb432494c8a3dc13480926ea0",
            "placeholder": "​",
            "style": "IPY_MODEL_d0d4cd481762471b94409ea301dd7714",
            "value": "tokenizer.json: "
          }
        },
        "35e70828b8b64670adb7e201b20fc4ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5e816d23f124001a6edc6950b2ec15b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d4de21ef00a489ebcb3fcf2ebb32846",
            "value": 1
          }
        },
        "6b86b57e43ba4d37bbc18a48b3a7ae4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65d055bf6cdd4384b87800784ad04181",
            "placeholder": "​",
            "style": "IPY_MODEL_2f255a3010074f03bd1ac29068fb0fe1",
            "value": " 466k/? [00:00&lt;00:00, 8.59MB/s]"
          }
        },
        "4fd7a6f76f664a2e8943483097100d5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5256782bb432494c8a3dc13480926ea0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0d4cd481762471b94409ea301dd7714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5e816d23f124001a6edc6950b2ec15b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "0d4de21ef00a489ebcb3fcf2ebb32846": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65d055bf6cdd4384b87800784ad04181": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f255a3010074f03bd1ac29068fb0fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "209c9260b073497ea1aa8222ad082852": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8002272b82864edc9d74a7ef517f0e13",
              "IPY_MODEL_00d9680e3b584e22be1077b9c7f3d1c7",
              "IPY_MODEL_dd6e75cd15a7492a99ee325260e377e8"
            ],
            "layout": "IPY_MODEL_c1d8c66d3de349398baa8eeddcabb26b"
          }
        },
        "8002272b82864edc9d74a7ef517f0e13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56a197f01dff436aba2ee4793364c1e2",
            "placeholder": "​",
            "style": "IPY_MODEL_03a19c4ded8c4ecfbcdbc14dfa66fab5",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "00d9680e3b584e22be1077b9c7f3d1c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_370492c059f44ec0bd686d9ebbfab9cc",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8fcecf14ce548468bff6deefcb87b37",
            "value": 239
          }
        },
        "dd6e75cd15a7492a99ee325260e377e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8275feb675f49dda07dc608e8a3af88",
            "placeholder": "​",
            "style": "IPY_MODEL_c43632a6c7e941058009a5dfc500e914",
            "value": " 239/239 [00:00&lt;00:00, 25.4kB/s]"
          }
        },
        "c1d8c66d3de349398baa8eeddcabb26b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56a197f01dff436aba2ee4793364c1e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03a19c4ded8c4ecfbcdbc14dfa66fab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "370492c059f44ec0bd686d9ebbfab9cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8fcecf14ce548468bff6deefcb87b37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f8275feb675f49dda07dc608e8a3af88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c43632a6c7e941058009a5dfc500e914": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d9bf0561f5c4bedb24523a0a7158d29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03d43d2f67334953b70139a9f7fdd28e",
              "IPY_MODEL_ec5464faf9224fc4b5ca4cc06044d72e",
              "IPY_MODEL_c06cb371a57a4235a5a3da3211dece1a"
            ],
            "layout": "IPY_MODEL_59dd32a0629f4c52ad1eded48dcf40d9"
          }
        },
        "03d43d2f67334953b70139a9f7fdd28e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12b0e3d3ad664ea898666b0c0216a216",
            "placeholder": "​",
            "style": "IPY_MODEL_22026d7249d24415a5e87ddb1b3c331e",
            "value": "config.json: 100%"
          }
        },
        "ec5464faf9224fc4b5ca4cc06044d72e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9935101def584032a27ce7cec267defd",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5517af0c55ab4ba9bff2b3f48777e5f4",
            "value": 190
          }
        },
        "c06cb371a57a4235a5a3da3211dece1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0777704a77414c13a65715b60627da31",
            "placeholder": "​",
            "style": "IPY_MODEL_c3b35735dfb744ce911fcbe9384045fe",
            "value": " 190/190 [00:00&lt;00:00, 20.9kB/s]"
          }
        },
        "59dd32a0629f4c52ad1eded48dcf40d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12b0e3d3ad664ea898666b0c0216a216": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22026d7249d24415a5e87ddb1b3c331e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9935101def584032a27ce7cec267defd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5517af0c55ab4ba9bff2b3f48777e5f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0777704a77414c13a65715b60627da31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3b35735dfb744ce911fcbe9384045fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvKonuJQCK8r",
        "outputId": "665956df-43ec-4957-e323-ef99c510e111"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlI4WL0dEVAs",
        "outputId": "60f44956-16f1-4dc7-d8d2-dd6abe536223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers einops accelerate langchain bitsandbytes"
      ],
      "metadata": {
        "id": "JqXGOn1_FeWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence_transformers"
      ],
      "metadata": {
        "id": "AMe5G0xZFvud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezJbWIsZF_mw",
        "outputId": "4aab66a2-16ba-4f5a-8f35-b376730a99c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama_index\n",
            "  Using cached llama_index-0.14.7-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting llama-index-cli<0.6,>=0.5.0 (from llama_index)\n",
            "  Using cached llama_index_cli-0.5.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting llama-index-core<0.15.0,>=0.14.7 (from llama_index)\n",
            "  Using cached llama_index_core-0.14.7-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.6,>=0.5.0 (from llama_index)\n",
            "  Using cached llama_index_embeddings_openai-0.5.1-py3-none-any.whl.metadata (400 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama_index)\n",
            "  Using cached llama_index_indices_managed_llama_cloud-0.9.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-index-llms-openai<0.7,>=0.6.0 (from llama_index)\n",
            "  Using cached llama_index_llms_openai-0.6.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting llama-index-readers-file<0.6,>=0.5.0 (from llama_index)\n",
            "  Using cached llama_index_readers_file-0.5.4-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama_index)\n",
            "  Using cached llama_index_readers_llama_parse-0.5.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama_index) (3.9.1)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (3.13.1)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (2025.3.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (0.28.1)\n",
            "Collecting llama-index-workflows!=2.9.0,<3,>=2 (from llama-index-core<0.15.0,>=0.14.7->llama_index)\n",
            "  Using cached llama_index_workflows-2.10.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (11.3.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (4.5.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (2.11.10)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.7->llama_index) (2.0.44)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (0.12.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (4.15.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.7->llama_index) (1.17.3)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-embeddings-openai<0.6,>=0.5.0->llama_index) (1.109.1)\n",
            "Requirement already satisfied: llama-cloud==0.1.35 in /usr/local/lib/python3.12/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama_index) (0.1.35)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.12/dist-packages (from llama-cloud==0.1.35->llama-index-indices-managed-llama-cloud>=0.4.0->llama_index) (2025.10.5)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama_index) (4.13.5)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama_index) (0.7.1)\n",
            "Requirement already satisfied: pandas<2.3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama_index) (2.2.2)\n",
            "Requirement already satisfied: pypdf<7,>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama_index) (6.1.3)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama_index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama_index) (0.6.54)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama_index) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama_index) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama_index) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.7->llama_index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.7->llama_index) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.7->llama_index) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.7->llama_index) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.7->llama_index) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.7->llama_index) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.7->llama_index) (1.22.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.7->llama_index) (1.14.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.7->llama_index) (3.1.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama_index) (2.8)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15.0,>=0.14.7->llama_index) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15.0,>=0.14.7->llama_index) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15.0,>=0.14.7->llama_index) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.15.0,>=0.14.7->llama_index) (0.16.0)\n",
            "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15.0,>=0.14.7->llama_index)\n",
            "  Using cached llama_index_instrumentation-0.4.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.54 in /usr/local/lib/python3.12/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index) (0.6.54)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama_index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama_index) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama_index) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama_index) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama_index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama_index) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.7->llama_index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.7->llama_index) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.7->llama_index) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.7->llama_index) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.7->llama_index) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.7->llama_index) (3.2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15.0,>=0.14.7->llama_index) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.15.0,>=0.14.7->llama_index) (3.26.1)\n",
            "Requirement already satisfied: python-dotenv<2,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index) (1.2.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.12/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15.0,>=0.14.7->llama_index) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama_index) (1.17.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.7->llama_index) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.7->llama_index) (3.0.3)\n",
            "Using cached llama_index-0.14.7-py3-none-any.whl (7.4 kB)\n",
            "Using cached llama_index_cli-0.5.3-py3-none-any.whl (28 kB)\n",
            "Using cached llama_index_core-0.14.7-py3-none-any.whl (11.9 MB)\n",
            "Using cached llama_index_embeddings_openai-0.5.1-py3-none-any.whl (7.0 kB)\n",
            "Using cached llama_index_indices_managed_llama_cloud-0.9.4-py3-none-any.whl (17 kB)\n",
            "Using cached llama_index_llms_openai-0.6.6-py3-none-any.whl (26 kB)\n",
            "Using cached llama_index_readers_file-0.5.4-py3-none-any.whl (51 kB)\n",
            "Using cached llama_index_readers_llama_parse-0.5.1-py3-none-any.whl (3.2 kB)\n",
            "Using cached llama_index_workflows-2.10.2-py3-none-any.whl (90 kB)\n",
            "Using cached llama_index_instrumentation-0.4.2-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: llama-index-instrumentation, llama-index-workflows, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-cli, llama-index-readers-llama-parse, llama_index\n",
            "Successfully installed llama-index-cli-0.5.3 llama-index-core-0.14.7 llama-index-embeddings-openai-0.5.1 llama-index-indices-managed-llama-cloud-0.9.4 llama-index-instrumentation-0.4.2 llama-index-llms-openai-0.6.6 llama-index-readers-file-0.5.4 llama-index-readers-llama-parse-0.5.1 llama-index-workflows-2.10.2 llama_index-0.14.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y llama-index\n",
        "!pip uninstall -y llama_index\n",
        "!rm -rf /usr/local/lib/python3.12/dist-packages/llama_index*\n",
        "!pip install llama-index==0.9.48 --no-cache-dir\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzo1WluSMsC8",
        "outputId": "d33c4095-24bc-4523-a2b0-7384361d0ef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: llama-index 0.14.7\n",
            "Uninstalling llama-index-0.14.7:\n",
            "  Successfully uninstalled llama-index-0.14.7\n",
            "\u001b[33mWARNING: Skipping llama_index as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting llama-index==0.9.48\n",
            "  Downloading llama_index-0.9.48-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index==0.9.48) (2.0.44)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.12/dist-packages (from llama-index==0.9.48) (3.13.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index==0.9.48) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.12/dist-packages (from llama-index==0.9.48) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from llama-index==0.9.48) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index==0.9.48) (2025.3.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from llama-index==0.9.48) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index==0.9.48) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index==0.9.48) (3.5)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index==0.9.48) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from llama-index==0.9.48) (2.0.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index==0.9.48) (1.109.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from llama-index==0.9.48) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from llama-index==0.9.48) (2.32.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index==0.9.48) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.12/dist-packages (from llama-index==0.9.48) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index==0.9.48) (4.15.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index==0.9.48) (0.9.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.48) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.48) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.48) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.48) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.48) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.48) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.48) (1.22.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.12/dist-packages (from deprecated>=1.2.9.3->llama-index==0.9.48) (1.17.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.48) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.48) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.48) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.48) (4.67.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index==0.9.48) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index==0.9.48) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index==0.9.48) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index==0.9.48) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index==0.9.48) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index==0.9.48) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index==0.9.48) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index==0.9.48) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->llama-index==0.9.48) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index==0.9.48) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index==0.9.48) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index==0.9.48) (3.2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index==0.9.48) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index==0.9.48) (3.26.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->llama-index==0.9.48) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->llama-index==0.9.48) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->llama-index==0.9.48) (2025.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.12/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index==0.9.48) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index==0.9.48) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index==0.9.48) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index==0.9.48) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index==0.9.48) (1.17.0)\n",
            "Downloading llama_index-0.9.48-py3-none-any.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m190.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: llama-index\n",
            "Successfully installed llama-index-0.9.48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "from llama_index.prompts.prompts import SimpleInputPrompt"
      ],
      "metadata": {
        "id": "P130Q6D0Hv3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/drive/My Drive/data\"\n",
        "documents = SimpleDirectoryReader(pdf_path).load_data()\n",
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4n3DUee_TIKu",
        "outputId": "cf483772-2e02-4ba4-b5b4-0ae03a9bbefc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id_='390246b8-7e17-4d7c-8c68-af9b4fae8b05', embedding=None, metadata={'page_label': '1', 'file_name': '2510.12573v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.12573v1.pdf', 'file_type': 'application/pdf', 'file_size': 2897796, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Learning Human Motion with Temporally Conditional Mamba\\nQUANG NGUYEN,FPT Software AI Center, Vietnam\\nTRI LE,FPT Software AI Center, Vietnam\\nBAORU HUANG∗,University of Liverpool, United Kingdom\\nMINH NHAT VU,Vienna University of Technology, Austria\\nNGAN LE,University of Arkansas, USA\\nTHIEU VO,National University of Singapore, Singapore\\nANH NGUYEN,University of Liverpool, United Kingdom\\nLearning human motion based on a time-dependent input signal presents a\\nchallenging yet impactful task with various applications. The goal of this task\\nis to generate or estimate human movement that consistently reflects the\\ntemporal patterns of conditioning inputs. Existing methods typically rely on\\ncross-attention mechanisms to fuse the condition with motion. However, this\\napproach primarily captures global interactions and struggles to maintain\\nstep-by-step temporal alignment. To address this limitation, we introduce\\nTemporally Conditional Mamba, a new mamba-based model for human\\nmotion generation. Our approach integrates conditional information into\\nthe recurrent dynamics of the Mamba block, enabling better temporally\\naligned motion. To validate the effectiveness of our method, we evaluate it\\non a variety of human motion tasks. Extensive experiments demonstrate that\\nour model significantly improves temporal alignment, motion realism, and\\ncondition consistency over state-of-the-art approaches. Our project page is\\navailable at https://zquang2202.github.io/TCM.\\nCCS Concepts:•Computing methodologies →Motion processing;\\nArtificial intelligence;Machine learning algorithms.\\nAdditional Key Words and Phrases: Human motion learning, State space\\nmodel, Temporal condition.\\nACM Reference Format:\\nQuang Nguyen, Tri Le, Baoru Huang, Minh Nhat Vu, Ngan Le, Thieu Vo,\\nand Anh Nguyen. 2025. Learning Human Motion with Temporally Condi-\\ntional Mamba. InProceedings of SIGGRAPH Asia 2025 Conference Papers\\n(SA Conference Papers ’25).ACM, New York, NY, USA, 10 pages. https:\\n//doi.org/3757377.3763948\\n∗Corresponding author.\\nAuthors’ Contact Information: Quang Nguyen, FPT Software AI Center, Hanoi, Vietnam,\\nquangnv89@fpt.com; Tri Le, FPT Software AI Center, Hanoi, Vietnam, trilq3@fpt.com;\\nBaoru Huang, University of Liverpool, Liverpool, United Kingdom, baoru.huang@\\nliverpool.ac.uk; Minh Nhat Vu, Vienna University of Technology, Wien, Austria, minh.\\nvu@tuwien.ac.at; Ngan Le, University of Arkansas, Arkansas, USA, thile@uark.edu;\\nThieu Vo, National University of Singapore, Singapore, Singapore, thieuvo@nus.edu.\\nsg; Anh Nguyen, University of Liverpool, Liverpool, United Kingdom, anh.nguyen@\\nliverpool.ac.uk.\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. Request permissions from permissions@acm.org.\\nSA Conference Papers ’25, Hong Kong\\n©2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 978-x-xxxx-xxxx-x/YYYY/MM\\nhttps://doi.org/3757377.3763948\\n1 Introduction\\nLearning human motion has received significant interest from mul-\\ntiple research communities, including computer vision, computer\\ngraphics, machine learning, and multimedia [Gao et al. 2022; Guo\\net al. 2022; Kim et al. 2022]. This area typically involves two main\\ntasks: human motion synthesis and human motion estimation, both\\nof which support various applications such as animation, virtual re-\\nality, and human-robot interaction [Nishimura et al. 2020; Zhu et al.\\n2023]. The goal of human motion synthesis is to generate realistic\\nand natural human motion conditioned on the inputs, such as au-\\ndio [Li et al. 2021, 2024c; Tseng et al. 2023], text [Pinyoanuntapong\\net al. 2024; Tevet et al. 2023; Zhang et al . 2024b], environmental\\ncontext [Huang et al . 2023; Li et al . 2023b], or video [Hong et al .\\n2025; Li et al. 2023a; Wang et al. 2023]. In contrast, human motion es-\\ntimation aims to predict human motion from observed input signals.\\nA core challenge in human motion synthesis and estimation tasks is\\neffectively capturing the complex dependencies between motion dy-\\nnamics and external stimuli. For example, in music-driven motion\\ngeneration, the synthesized human motion should be physically\\nplausible and rhythmically aligned with the music’s beat.\\nSeveral works on human motion have recently focused onstatic\\nconditional inputs such as text description in the text-to-human\\nmotion task [Petrovich et al. 2022; Tevet et al. 2023; Yuan et al. 2024;\\nZhang et al . 2024a]. The static condition input provides a high-\\nlevel semantic intent or partial temporal and spatial constraints,\\nbut remains constant throughout the generated motion sequence.\\nIn contrast, another line of work addressestemporalconditioning\\ninputs such as music [Chan et al. 2019; Tseng et al. 2023], egocentric\\nvideo [Li et al. 2023a; Luo et al. 2021; Wang et al. 2024a], time-series\\nsignal [Huang et al. 2024; Lam et al. 2022; Tang et al. 2024], or track-\\ning inputs [Du et al. 2023; Starke et al. 2022, 2024]. These temporal\\nconditions evolve and carry rich, fine-grained temporal dynamics\\nthat directly influence the temporal dynamics of the human mo-\\ntion. This setting poses a greater challenge, requiring the model to\\ncontinuously adapt to changing inputs while maintaining physi-\\ncal plausibility and temporal coherence. In this work, we explore\\nhuman motion problems under thetemporal conditions, aiming to\\ncapture precise alignment between human motion and the evolving\\ntemporally conditional input.\\nTo synthesize or estimate human motion from temporal con-\\nditions, previous works have mainly utilized the Cross-Attention\\nmechanism within diffusion framework [Ho et al . 2020]. These\\nframeworks typically use either Mamba-based [Gu and Dao 2023;\\nHu et al. 2024; Zhang et al. 2024b] or Transformer-based [Li et al.\\nSA Conference Papers ’25, December 15–18, 2025, Hong Kong.\\narXiv:2510.12573v1  [cs.CV]  14 Oct 2025', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='20aef11d-55aa-4a58-a9e9-5ceeb8562626', embedding=None, metadata={'page_label': '2', 'file_name': '2510.12573v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.12573v1.pdf', 'file_type': 'application/pdf', 'file_size': 2897796, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2•Quang Nguyen et al.\\nx n\\nCondition\\nMamba/Transformer\\nCross Attention\\nCondition\\nSpatial Mamba\\nx n\\nTemporally\\nCondition Mamba\\n(a) (b) (c)\\nHead Position\\nTime\\nFig. 1. High-level comparison between our approach and previous methods. (a) Previous works usually use Cross-Attention to integrate input condition into\\nMamba/Transformer backbone. (b) Our approach embeds the condition directly within the Mamba block; (c) We show the head trajectory over time in an\\nego-to-motion task. Compared to Cross-Attention and Vanilla Mamba, which generate motions that deviate noticeably from the ground truth, our method\\nproduces a trajectory that closely follows the actual motion pattern.\\n2024c; Tseng et al. 2023] architectures as their backbone (Fig. 1a).\\nFor example, EgoEgo [Li et al . 2023a] generates human motion\\nfrom egocentric views using a transformer decoder network. Other\\nworks [Huang et al. 2024; Li et al. 2024c; Tseng et al. 2023] use Cross-\\nAttention to align music with dance motion. While notable results\\nhave been achieved with handling temporal conditions using the\\nCross-Attention mechanism, this approach often overlooks recur-\\nrent dependencies and fine-grained temporal alignment. In Fig. 1c,\\nwe illustrate the head position over time for a generated sequence\\nin the ego-to-motion task. We can see that both Cross-Attention\\nand Vanilla Mamba fail to produce trajectories that closely match\\nthe ground truth. Therefore, we hypothesize that the conditioning\\nsignal should be modeled as a recurrent influence on the motion\\nstream to better preserve temporal structure and alignment of hu-\\nman motion. To this end, we propose a new method that directly\\nintegrates the condition into Mamba’s dynamics, enabling the au-\\ntoregressive injection of temporal signals to improve the coherence\\nand alignment between human motion and condition inputs.\\nBuilding on the widely adopted diffusion framework known for\\nits effectiveness in generating high-quality motion, we introduce\\na new approach for human motion learning. Specifically, we pro-\\npose Temporally Conditional Mamba (TCM), a new Mamba block\\nthat can be integrated into the diffusion model for human motion\\ntasks. Our method integrates temporal conditions into each human\\nmotion timestep, allowing the model to learn human motion that\\nis temporally aligned with the input conditions. We demonstrate\\nTCM’s generalizability across diverse tasks in both human motion\\nsynthesisor human motionestimationsettings with different tem-\\nporally conditional inputs. Extensive experiments show that our\\napproach consistently outperforms state-of-the-art methods in mo-\\ntion quality and condition alignment. Our main contributions can\\nbe summarized as follows:\\n•We introduce TCM, a new mechanism that injects temporal\\nconditions into the Mamba’s dynamics, enabling autoregres-\\nsive alignment of human motion with condition signals.\\n•We show that TCM can be integrated into a diffusion model\\nfor diverse motion synthesis and motion estimation tasks with\\ntemporal conditions, and achieves significant improvements\\nover state-of-the-art methods.\\n2 Related Works\\n2.1 Human Motion.\\nHuman motion is a popular research topic [Chen et al. 2023a; Jiang\\net al. 2023; Petrovich et al. 2021; Rempe et al. 2021; Zhang et al. 2023].\\nSeveral works have focused on human motion synthesis with static\\nconditions, such as text-to-human motion [Dai et al . 2024; Petro-\\nvich et al. 2022; Tevet et al. 2023; Zhang et al. 2024a]. In practice,\\nhuman motion can be synthesised or estimated based on temporal\\nconditions such as music [Chan et al. 2019; Lam et al. 2022; Le et al.\\n2023a,b; Siyao et al. 2022; Tseng et al. 2023], object trajectories [Li\\net al. 2023b; Taheri et al. 2022], videos [Chan et al . 2019; Li et al .\\n2023a; Mehraban et al . 2024; Zhao et al . 2024], speech [Chhatre\\net al. 2024], and multimodal combinations of egocentric video and\\nmusic [Nguyen et al. 2025]. Current works leverage simple concate-\\nnation or cross-attention [Vaswani et al. 2017] to condition human\\nmotion on input modalities. The authors in [Luo et al. 2021; Yuan\\nand Kitani 2019] concatenate optical flow features and pose states\\nas input to a policy network. The works in [Li et al. 2023a,b] employ\\na diffusion model where the noisy data is concatenated with object\\nmovement trajectories and head pose. The authors in [Huang et al.\\n2024; Li et al. 2024c; Tseng et al. 2023] leverage the cross-attention\\nmechanism to inject music information into dance generation at\\neach decoder layer and denoising step. Unlike previous works using\\nthe cross-attention mechanism or simple concatenation, our work\\ninjects the temporal condition input directly into each decoder layer\\nof the network to enable better alignment between the temporal\\ncondition input and the human motion.\\n2.2 State Space Models.\\nState space models (SSM) [Gu et al . 2022, 2021] are a promising\\narchitecture for sequence modeling [Hu et al . 2024]. Mamba [Gu\\nand Dao 2023] introduces a selective SSM architecture, integrating\\ntime-varying parameters into the SSM framework. Several works\\napply Mamba in various applications, including image process-\\ning [Hatamizadeh and Kautz 2024; Hu et al . 2024; Lee et al. 2024;\\nPhung et al. 2024; Wang et al. 2024d], graph processing [Behrouz and\\nHashemi 2024; Wang et al. 2024c], point cloud analysis [Liang et al.\\n2024; Liu et al. 2024], and human motion generation [Wang et al.\\n2024b; Zhang et al. 2024b]. Vim [Liao et al. 2024] and DiM [Teng\\nSA Conference Papers ’25, December 15–18, 2025, Hong Kong.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='aad270aa-02b0-4cea-9207-3ec26a1e6784', embedding=None, metadata={'page_label': '3', 'file_name': '2510.12573v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.12573v1.pdf', 'file_type': 'application/pdf', 'file_size': 2897796, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Learning Human Motion with Temporally Conditional Mamba•3\\net al. 2024] present different scanning strategies for the SSM block\\nin vision tasks. Mamba-ND [Li et al . 2025] extends the capabili-\\nties of SSM to higher-dimensional data by exploring different scan\\ndirections within a single SSM block. ZigMa [Hu et al . 2024] and\\nAiM [Li et al. 2024b] utilize Mamba-based SSM blocks for efficient\\nimage generation. The authors in [Yan et al. 2024] propose a fully\\nMamba-based model that eliminates attention mechanisms, demon-\\nstrating its effectiveness for high-resolution image generation. Mo-\\ntionMamba [Zhang et al. 2024b] proposes a symmetric multi-branch\\nMamba that processes temporal and spatial and performs excep-\\ntionally on text-to-motion generation tasks. To enable conditional\\ngeneration, current Mamba-based architectures [Hu et al. 2024; Li\\net al. 2024b; Zhang et al . 2024b] adopt the condition mechanism\\nfrom their origin Transformer-based ones. In contrast, our method\\nexploits conditioned temporal data in the Mamba block.\\n2.3 Feature Modulation.\\nFeature-wise modulation learns a function of conditioning inputs to\\nreplace parameters in feature-wise affine transformation, as intro-\\nduced originally in [Ioffe and Szegedy 2015]. Feature-wise modula-\\ntion has various forms applied across domains: Conditional Instance\\nNormalization [Dumoulin et al. 2017] and Adaptive Instance Nor-\\nmalization [Huang and Belongie 2017] that are applied in image\\nstylization; Dynamic Layer Normalization [Kim et al. 2017], which\\nis successfully applied for speech recognition. Recently, FiLM [Perez\\net al. 2018] and Adaptive Normalization [Xu et al. 2019] are widely\\nused in image generation [Chen et al. 2023b; Dhariwal and Nichol\\n2021; Karras et al. 2019; Peebles and Xie 2023], audio generation [Liu\\net al. 2023], and vision language action model [Brohan et al. 2022;\\nChi et al. 2023; Team et al. 2024; Turkoglu et al. 2022], motion gen-\\neration [Li et al. 2024c; Yan et al. 2024]. Inspired by the success of\\nfeature-wise modulation, we integrate an adaptive layer normaliza-\\ntion to modulate the features directly in each Mamba step during\\nthe human motion generation process.\\n3 Methodology\\n3.1 Preliminaries\\n3.1.1 Mamba.A Mamba block [Gu and Dao 2023] integrates S6\\nlayers as its central element. An S6 layer operates by transform-\\ning an input sequence of tokensx =(𝑥 1,...,𝑥 𝐿) ∈R𝐿×𝐷 to the\\noutput sequence of tokensy =(𝑦1,...,𝑦 𝐿)∈R 𝐿×𝐷, where 𝐿is the\\nsequence length. In this context, each vector𝑥𝑙 ∈R 𝐷 or𝑦𝑙 ∈R 𝐷 rep-\\nresents a token in the sequence. For each feature channel indexed\\nby 𝑑∈1,...,𝐷 , the output sequence 𝑦𝑑 =(𝑦 𝑑1,...,𝑦 𝑑𝐿) ∈R𝐿\\nis computed recursively from the corresponding input sequence\\n𝑥𝑑 =(𝑥 𝑑1,...,𝑥 𝑑𝐿)∈R 𝐿. This computation progresses through a\\nseries of hidden statesℎ𝑑1,...,ℎ 𝑑𝐿 ∈R 𝑁, defined as:\\nℎ𝑑𝑙 = A𝑑𝑙 ·ℎ𝑑,𝑙−1 +B𝑑𝑙 ·𝑥𝑑𝑙, ℎ 𝑑0 =0,\\n𝑦𝑑𝑙 =C 𝑙 ·ℎ𝑑𝑙 , (1)\\nwhere, at each timestep 𝑙= 1,...,𝐿 , the dynamic system matri-\\nces (A𝑑𝑙,B𝑙)are derived from continuous-time matrices (A𝑑,B𝑙)\\nthrough discretization using a time scaling factorΔ 𝑑𝑙:\\nA𝑑𝑙 =𝑒Δ𝑑𝑙 ·A𝑑 , B𝑑𝑙 =Δ 𝑑𝑙 ·B 𝑙 , (2)\\nwhere Δ𝑑𝑙(𝑥𝑙),B 𝑙(𝑥𝑙), andC 𝑙(𝑥𝑙)are dynamically computed from\\nthe input𝑥𝑙:\\nΔ𝑑𝑙(𝑥𝑙)=softplus(𝑆 Δ,𝑑(𝑥𝑙))=𝑙𝑛(1+𝑒 𝑆Δ,𝑑 (𝑥𝑙 )),\\nB𝑙 =𝑆B (𝑥𝑙),C 𝑙 =(𝑆 C (𝑥𝑙))⊤.\\n(3)\\nThe hidden matrixA𝑑 ∈R 𝑁×𝑁 , where 𝑁 is the state dimension, and\\nthe parameterized functions 𝑆Δ,𝑑 : R𝐷 →R , 𝑆B : R𝐷 →R 𝑁, and\\n𝑆C : R𝐷 →R 𝑁, which are implemented as linear transformations,\\nare learned during training [Gu and Dao 2023].\\n3.1.2 Human Motion Representation.Human motion data captures\\nboth temporal and spatial dynamics. A motion sequence is repre-\\nsented asX =(𝑋 1,𝑋2,...,𝑋 𝐿)∈R 𝐿×𝐷, where 𝐿 is the sequence\\nlength and 𝐷denotes the spatial dimension of each pose. Each pose\\n𝑥𝑙 is represented in the SMPL format [Loper et al. 2023] and contains\\ninformation such as joint rotations, positions, or velocities.\\n3.2 Problem Definition\\nGiven the 𝐿-length condition embeddingm =(𝑚 1,𝑚2,...,𝑚 𝐿)∈\\nR𝐿×𝐸, our goal is to generate/estimate a human motion sequence\\nX =(𝑋 1,𝑋2,...,𝑋 𝐿)∈R 𝐿×𝐷 that is temporally aligned with the\\ngiven condition. We note that the condition inputmand the hu-\\nman motion sequenceXare assumed tohave the same length 𝐿. In\\npractice, the conditionmrepresents temporal inputs such as music,\\negocentric video, or sequences of object geometry.\\nFollowing previous works [Tevet et al. 2023; Tseng et al. 2023],\\nwe adopt a diffusion model [Ho et al . 2020] as a backbone frame-\\nwork due to its superior results in various tasks [Siyao et al. 2022;\\nTseng et al. 2023; Zhu et al. 2023]. The diffusion model includes a\\nforward process and a backward process. In the forward process,\\nclean motion dataX 0 is progressively added with Gaussian noise\\nover 𝑡 time steps, resulting in noisy motionX 𝑡. This process can be\\nformulated as:\\n𝑞(X𝑡|X0)=N( √𝛼𝑡X0,(1−𝛼 𝑡)I), (4)\\nwhere 𝛼𝑡 = Î𝑡\\n𝑠=1 (1 −𝛿𝑠), and 𝛿𝑡 controls the noise schedule. In\\nthe reverse process, a mamba-based neural network 𝑓𝜃 is learned to\\nprogressively remove noise from the noisy motion sequenceX𝑡 and\\nrecover the original clean motionX 0, conditioned on an auxiliary\\nmodalitym. Following the approach of [Ho et al . 2020], we adopt\\nthe training objective:\\nˆ𝜃=arg min\\n𝜃\\nE𝑡,X𝑡 [∥X0 −𝑓𝜃(X𝑡,𝑡,m)∥ ]. (5)\\n3.3 Temporally Conditional Mamba for Human Motion\\n3.3.1 Overview.We introduce a new mamba-based diffusion model\\nfor learning human motion. Our design is a mamba-based architec-\\nture where the condition embedding is directly fused within the\\nmamba block, particularly in a temporally-aware manner. Fig. 2\\nshows an overview of the proposed architecture. The model takes\\nas input the noised motionx, the condition embeddingm, and the\\ntimestep embeddingt, and predicts the denoised motion ˆx. The\\narchitecture consists of three main components: (1)Temporally Con-\\nditional Mamba, which injects the condition modality embedding\\nat the token level to align the motion with the conditioning signal\\ntemporally; (2)Spatial Mamba, which captures spatial dependencies\\nSA Conference Papers ’25, December 15–18, 2025, Hong Kong.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1cd8c83c-d543-4d7f-9148-d0d4bb05d482', embedding=None, metadata={'page_label': '4', 'file_name': '2510.12573v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.12573v1.pdf', 'file_type': 'application/pdf', 'file_size': 2897796, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4•Quang Nguyen et al.\\nx n\\nTemporally\\nConditional Mamba\\nAdaptive Layer Norm\\nSpatial Mamba\\nMLP\\nCondition\\n(a) Diffusion Human Motion Framework\\nTimestep\\nAdaptive Layer Norm\\nScan\\nConv1d\\nCondition\\nLinear Linear\\nLinear\\nLinear\\nScan\\nConv1d\\nLinear Linear\\nLinear\\nRearrange\\nSpatial Tokens\\nSpatial Tokens\\nRearrange\\n(b) Temporally Conditional Mamba (c) Spatial Mamba\\nTemporal Tokens\\nTemporal Tokens\\nTemporal Tokens\\nTemporal Tokens\\nFig. 2. Architecture overview of the proposed approach. (a) We show the overview of the diffusion human motion framework with Mamba blocks. (b) Our key\\ncontribution is the Temporally Conditional Mamba, which incorporates temporal conditions into the internal dynamics of the Mamba block. (c) The Spatial\\nMamba block is used to learn human spatial features.\\nof human motion; and (3)Adaptive Layer Norm, which globally in-\\ncorporates both the condition embedding and timestep embedding\\nuniformly across all tokens. In the following, we provide a detailed\\ndescription of each model component.\\n3.3.2 Temporally Conditional Mamba.In control theory, Linear\\nParameter-varying State Space Models [Briat 2014; Mohammad-\\npour and Scherer 2012] adapt system matrices, such asBandC,\\ndynamically based on external signals, allowing the system to adjust\\nits internal dynamics and output behavior flexibly. Motivated by\\nthis principle, we propose Temporally Conditional Mamba (TCM),\\nwhere the condition signal is used to modulate theBandCmatrices\\nover time; this mechanism intuitively enables the model to dynami-\\ncally tailor its evolution at each frame, leading to outputs that more\\ntightly aligned with the temporal patterns present in the condition-\\ning information. The detailed procedure of TCM is presented in\\nAlgorithm 1. It transforms a sequence of temporal tokens embed-\\ndingx ∈R 𝐵×𝐿×𝐷 with the input condition embeddingm ∈R 𝐵×𝐿×𝐸\\nto output fused motion-condition embeddingy ∈R 𝐵×𝐿×𝐷 , where,\\n𝐵denotes the batch size,𝐿is the temporal length, and𝐷,𝐸are the\\nembedding dimensions. Specifically, the selection matricesBandC\\nare modified to depend on both motion embeddingxand condition\\nembeddingm. The update rule in Equation 1 is redefined as:\\nℎ𝑑𝑙 = ¯A𝑑𝑙 ·ℎ𝑑,𝑙−1 +(Δ 𝑑𝑙 ·eB𝑙(𝑥𝑙,𝑚𝑙))·𝑥 𝑑𝑙 ,\\n𝑦𝑑𝑙 = eC𝑙(𝑥𝑙,𝑚𝑙)·ℎ 𝑑𝑙 ,\\n(6)\\nfor 𝑙= 1,2,...,𝐿,and𝑑= 1,2,...,𝐷 . Here, eB𝑙 and eC𝑙 are condition-\\naware selection matrices. They are obtained via an element-wise\\naffine modulation of the original matrices:\\neB𝑙 =𝛾B (𝑚𝑙)⊙B 𝑙 +𝛽B (𝑚𝑙),\\neC𝑙 =𝛾C (𝑚𝑙)⊙C 𝑙 +𝛽C (𝑚𝑙),\\n(7)\\nwhere𝛾B,𝛽B,𝛾C,𝛽C ∈R 𝑁, where𝑁 is state dimension, are predicted\\nby a learnable mapping𝑆𝑚 : R𝐸 →R 𝑁 applied to𝑚𝑙. For simplicity\\nand computational efficiency, we implement 𝑆𝑚(·)as a single linear\\nlayer with a non-linear activation, as illustrated in Fig. 2b. By making\\nthe selection matricesBandCdynamically dependent onm, the\\nmodel is endowed with fine-grained control over how the input\\nsequencexand conditionminteract to produce the outputy. This\\nmechanism allows the model to adapt its recurrent dynamics based\\non the external condition.\\n3.3.3 Spatial Mamba.Apart from temporal dynamics, learning\\nspatial dependencies among joints is critical for human motion\\ngeneration, as spatial structures encode essential information com-\\nplementary to temporal dynamics. A Spatial Mamba block is used to\\nmodel spatial dependencies. The detailed implementation of Spatial\\nMamba is provided in Fig. 2c. First, the motion representation is\\nrearranged from the temporal domain (𝐵,𝐿,𝐷) to the spatial do-\\nmain (𝐵,𝐷,𝐿). A standard Mamba block [Gu and Dao 2023] is then\\napplied to model interactions across joints. Finally, the spatially\\ntransformed sequence is rearranged back to the original temporal\\nformat. This design enables the model to reason about both spatial-\\ntemporal structures in human motion effectively.\\nSA Conference Papers ’25, December 15–18, 2025, Hong Kong.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='945b4956-9241-4db9-a1da-ba943831e5f0', embedding=None, metadata={'page_label': '5', 'file_name': '2510.12573v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.12573v1.pdf', 'file_type': 'application/pdf', 'file_size': 2897796, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Learning Human Motion with Temporally Conditional Mamba•5\\nALGORITHM 1:Temporally Conditional Mamba\\nInput:Motion embeddingx:(𝐵,𝐿,𝐷),\\nCondition embeddingm:(𝐵,𝐿,𝐸)\\nOutput:Transformed motiony:(𝐵,𝐿,𝐷)\\n1/* Input projection */;\\n2u←𝑆 u (x):(𝐵,𝐿,𝐷);\\n3z←𝑆 z (x):(𝐵,𝐿,𝐷);\\n4u←SiLU(Conv1d(u)):(𝐵,𝐿,𝐷);\\n5A←Parameter:(𝐷,𝑁);\\n6/* GetBandC*/;\\n7B←𝑆 B (u):(𝐵,𝐿,𝑁);\\n8C←𝑆 C (u):(𝐵,𝐿,𝑁);\\n9/* Get condition-aware eBand eC*/;\\n10(𝛾 B,𝛽B,𝛾C,𝛽C )←𝑆 𝑚 (m):(𝐵,𝐿,𝑁);\\n11 eB←𝛾 B ⊙B+𝛽 B :(𝐵,𝐿,𝑁);\\n12 eC←𝛾 C ⊙C+𝛽 C :(𝐵,𝐿,𝑁);\\n13/* Get discrete ¯Band ¯A*/;\\n14Δ←softplus(𝑆 Δ (u)):(𝐵,𝐿,𝐷);\\n15 ¯B←Δ· eB:(𝐵,𝐿,𝐷,𝑁);\\n16 ¯A←exp(Δ·A):(𝐷,𝑁);\\n17/* SSM forward */;\\n18y←SSM( ¯A,¯B,eC)(u):(𝐵,𝐿,𝐷);\\n19/* Gating and output projection */;\\n20y←𝑆 y (y⊙SiLU(z)):(𝐵,𝐿,𝐷);\\n21returny\\n3.3.4 Adaptive Layer Norm.Following prior work on adaptive nor-\\nmalization in diffusion models [Dhariwal and Nichol 2021; Peebles\\nand Xie 2023], we incorporate an Adaptive Layer Normalization\\n(AdaLN) layer [Karras et al. 2019; Peebles and Xie 2023] to replace\\nthe standard LayerNorm. AdaLN effectively integrates timestep and\\ncondition information by modulating the feature distribution based\\non their combined embedding. Specifically, dimension-wise scale\\nand shift parameters 𝜆𝑖 and 𝜌𝑖 are generated from the sum of the\\ntimestep embeddingtand condition embeddingmthrough an MLP:\\n𝜆𝑖,𝜌𝑖 =MLP(Sum(t,m)), (8)\\nwhere 𝑖∈{𝑆,𝑇} , with𝑆referring to the spatial and𝑇 to the temporal\\nnormalization. The motion embeddingxis then modulated as:\\nx′=𝜆𝑖 ⊙Norm(x)+𝜌 𝑖 . (9)\\nWe apply AdaLN before both the Spatial Mamba and Temporally\\nConditional Mamba blocks. This mechanism operates uniformly\\nacross all tokens in the sequence. Combined with the token-wise\\nconditioning inside Temporally Conditional Mamba, this design\\nimproves the alignment between the generated motion and the\\nconditioning inputs across the entire sequence.\\n4 Experiment\\nIn this section, we first conduct a standard task setup to compare our\\nmethod with the Vanilla Mamba and Cross-Attention approaches.\\nWe then demonstrate the generalization of our proposed method\\nin four popular human motion learning tasks, all of which involve\\ntemporally aligned conditional inputs. The four tasks include: (i)\\nhuman motion synthesis from music, (ii) human motion estimation\\nTable 1. Comparative results of dance synthesis from music task. We com-\\npare our proposed TCM with Cross-Attention and Vanilla Mamba. Bold\\nindicates best, and underline indicates second best.\\nMethod Params (M) Infer (s) FID 𝑘 ↓FID𝑔↓Div𝑘 ↑Div𝑔↑BAS↑\\nGround Truth - - 17.10 10.60 8.19 7.45 0.2374\\nVanilla Mamba 26.58 1.04 25.61 16.35 7.52 6.05 0.2434\\nCross-Attention 41.28 1.42 23.43 12.86 7.87 6.48 0.2411\\nTCM(ours) 26.84 1.1120.66 9.75 8.98 7.24 0.2761\\nTable 2. Ablation experiment on network components. We assess the per-\\nformance of the proposed method under different settings.\\nFID𝑘 ↓FID 𝑔 ↓Div 𝑘 ↑Div 𝑔 ↑BAS↑\\nw.o. TCM block 25.61 16.35 7.52 6.05 0.2434\\nw.o. AdaLN block 21.89 11.76 8.11 6.33 0.2615\\nw.o.𝛾B,C 22.37 11.48 8.02 6.36 0.2582\\nw.o.𝛽B,C 21.52 10.63 8.34 6.58 0.2624\\nBest architecture20.66 9.75 8.98 7.24 0.2761\\nfrom egocentric video, (iii) human motion estimation from egocen-\\ntric video and music, and (iv) human motion synthesis from object\\ntrajectories. These tasks cover both human motionsynthesisand\\nhuman motionestimationsettings.\\n4.1 Comparison with Vanilla Mamba and Cross-Attention\\nTo make a fair comparison between our method and traditional\\napproaches, we first set up a simple experiment that focuses only\\non techniques used in fusing the temporal condition to generate\\nhuman motion. We select human dance synthesis from music as\\nthe primary task for this experiment, as it is a widely studied and\\nrepresentative benchmark in motion synthesis [Zhu et al. 2023].\\n4.1.1 Implementation.We implement a diffusion-based framework [Ho\\net al. 2020] in which music serves as the conditioning signal for all\\nmethods: our TCM, Vanilla Mamba, and Cross-Attention. Follow-\\ning [Tseng et al. 2023], we use Jukebox [Dhariwal et al . 2020], a\\nGPT-style pretrained generative model, to extract a 4800-dim fea-\\nture that effectively captures the characteristics of the input music.\\nIn Vanilla Mamba, we replace our TCM block with standard Mamba\\nblocks [Gu and Dao 2023]; conditioning is integrated solely through\\nadaptive layer normalization in this setting. In the Cross-Attention\\nsetup, we build upon the Vanilla Mamba setup by inserting a cross-\\nattention module into each layer for condition injection.\\n4.1.2 Dataset and Metrics.We utilize the AIST++ dataset [Li et al .\\n2021], which contains 1,408 high-quality dance motion sequences\\npaired with music spanning a wide range of genres. We adopt the\\noriginal training and testing splits provided by the dataset. To assess\\nthe quality of the generated dance motions, we use FID𝑘 and FID𝑔\\nto measure fidelity, andDiv𝑘 and Div𝑔 to assess diversity of the gen-\\nerated motion, where 𝑘 and 𝑔refer to the kinematic and geometric\\nfeature of the generated motion, respectively. We employ the Beat\\nAlignment Score (BAS) [Siyao et al. 2022] to quantify how well the\\ngenerated motion aligns with the input music.\\nSA Conference Papers ’25, December 15–18, 2025, Hong Kong.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5aca96ab-1fdd-4f1a-be23-0a83086ddbae', embedding=None, metadata={'page_label': '6', 'file_name': '2510.12573v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.12573v1.pdf', 'file_type': 'application/pdf', 'file_size': 2897796, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6•Quang Nguyen et al.\\nTable 3. Ablation experiments on sequence length of motion. Length analy-\\nsis. We evaluate the FID score of our TCM and Cross-Attention at different\\nsequence lengths.\\nMotion length (s) 5 15 25\\nCross-Attention 25.61 27.04 31.36\\nTCM(ours) 20.66 21.09 23.82\\n4.1.3 Main Results.Table 1 shows that our TCM outperforms both\\nVanilla Mamba and Cross-Attention. We also report model parame-\\nters and inference time (evaluated using DDIM with 50 sampling\\nsteps). Our method achieves better performance while maintaining\\na parameter count and inference speed comparable to that of Vanilla\\nMamba. Although the Cross-Attention model demonstrates slightly\\nhigher motion realism than Vanilla Mamba, it exhibits weaker syn-\\nchronization with the music inputs. In Fig. 3, we show the mean ve-\\nlocity of generated motion over time. We can see that our generated\\ndance has motion beats, identified as local minima in kinetic veloc-\\nity, that closely match the music beats, while both Cross-Attention\\nand Vanilla Mamba fail. Table 1 and Fig. 3 suggest that autoregres-\\nsively injecting the condition into the motion sequence enables\\nmore precise and temporally consistent alignment with the condi-\\ntioning signal, particularly for temporal data where time-dependent\\ncorrelations are critical.\\n4.2 Ablation analysis\\n4.2.1 The effect of each component.To understand the contribu-\\ntions of each component to the overall results, we conduct an anal-\\nysis in Table 2. The results indicate that with our TCM block, the\\nBAS score improves from 0.2423 to 0.2761, and the same trend is\\nobserved for other metrics. Additionally, integrating AdaLN layers\\nyields slight performance gains. These results confirm the TCM\\nblock’s key role in aligning generated motion with music. Table 2\\nalso shows the impact of scale and shift parameters in the TCM\\nblock. We find that the performance drop when not using𝛾is higher\\nthan when not using 𝛽. This result suggests that the scale parameter\\n𝛾plays a more critical role than the shift parameter𝛽.\\n4.2.2 Could TCM distinguish input conditions?To gain insights\\ninto how our TCM processes conditional information, we inves-\\ntigate learned 𝛾 and 𝛽 parameters in the TCM block. We extract\\n𝛾B,𝛽B,𝛾C,𝛽C from 5000 music samples across three distinct music\\ngenres (i.e., mWA, mJS, mJB). We visualize the t-SNE plot of these\\nparameters in Fig. 4. The results reveal clear clustering patterns\\nbased on music genre, indicating that the TCM adapts its internal\\ndynamics to different conditioning signals.\\n4.2.3 Sequence Length Analysis.To analyze the effect of sequence\\nlength, we evaluate the FID score at 5, 15, and 25 seconds. We\\ncompare our TCM with the Cross-Attention baseline. As shown in\\nTable 3, the FID score of motions generated by our method increases\\nmore slowly as the sequence length grows, indicating superior ca-\\npability for long-range motion generation.\\nKinetic velocity\\nTime\\nFig. 3. Motion and music beat alignment. We plot the mean joint velocity\\nover time for different methods. Kinematic beats are identified as local\\nminima in the velocity curves. Our method produces motion with kinematic\\nbeats that align more closely with the music beats.\\nmWA mJS mJB\\nFig. 4. t-SNE visualization of 𝛾B,C and 𝛽B,C in TCM blocks. Clustering in\\nthe t-SNE plot reveals that the learned 𝛾B,C and 𝛽B,C parameters encode\\ndiscriminative information that reflects distinct music genres.\\n4.3 Comparison with State-of-the-art Methods across\\nDifferent Tasks\\n4.3.1 Human Dance Synthesis from Music.Human dance synthesis\\nfrom music is a prominent task with wide-ranging applications in an-\\nimation, film production [Chan et al. 2019], and the metaverse [Lam\\net al. 2022]. The goal is to generate realistic dance movements that\\nare temporally synchronized with a given piece of music. As in Sec-\\ntion 4.1, our model, which incorporates condition-aware temporal\\nmodeling, is particularly well-suited for this task.\\nResults.We compare our proposed method, TCM, with several\\nstate-of-the-art methods on the AIST++ dataset. These baselines\\nadopt a variety of condition fusion mechanisms, ranging from simple\\nconcatenation to cross-attention, as detailed in Table 4. The results\\nshow that our model outperforms previous approaches, demonstrat-\\ning better motion quality and diversity. Notably, our TCM achieves\\na BAS metric of 0.2761, indicating a stronger temporal alignment\\nbetween the generated dance and music.\\n4.3.2 Human Motion Estimation from Egocentric Video.Estimating\\nhuman motion from egocentric video captured by a single head-\\nmounted camera requires understanding of physically plausible full-\\nbody motion, with a strong emphasis on aligning head movements to\\nthe video on a frame-by-frame basis. We integrate our TCM into the\\nframework in [Li et al. 2023a], replacing its transformer backbone\\nwith our TCM. For evaluation, we compare our approach with four\\nSA Conference Papers ’25, December 15–18, 2025, Hong Kong.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dc3206d7-9f72-4fff-9231-74fefcbf7374', embedding=None, metadata={'page_label': '7', 'file_name': '2510.12573v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.12573v1.pdf', 'file_type': 'application/pdf', 'file_size': 2897796, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Learning Human Motion with Temporally Conditional Mamba•7\\nTable 4. Comparison with State-of-the-art methods on music-to-dance task.\\nBold indicates the best results, and underlined indicates the second-best\\nresults.\\nMethod Motion Quality Motion DiversityBAS↑\\nFID𝑘↓FID 𝑔↓Div 𝑘↑Div 𝑔↑\\nGround Truth 17.10 10.60 8.19 7.45 0.2374\\nTSMT [Li et al. 2020] 86.43 43.46 6.85 3.32 0.1607\\nDanceNet [Zhuang et al. 2022] 69.18 25.49 2.86 2.85 0.1430\\nDanceRev [Huang et al. 2020] 73.42 25.92 3.52 4.87 0.1950\\nFACT [Li et al. 2021] 35.35 22.11 5.94 6.18 0.2209\\nBailando [Siyao et al. 2022] 28.16 9.627.83 6.34 0.2332\\nEDGE [Tseng et al. 2023] 42.16 22.12 3.96 4.61 0.2334\\nLDA [Alexanderson et al. 2023] 25.67 11.25 8.02 6,58 0.2431\\nBADM [Zhang et al. 2024c] - - 8.29 6.76 0.2366\\nLodge [Li et al. 2024c] 37.09 18.79 5.58 4.85 0.2423\\nTCM(ours)20.669.75 8.98 7.24 0.2761\\nTable 5. Comparison with State-of-the-art methods on ego-to-motion task.\\nBold indicates the best, and underline indicates the second-best results.\\nMethodO head↓T head↓MPJPE↓Accel↓FS↓\\nPoseReg [Yuan and Kitani 2019] 0.77 354.7 147.7 127.6 87.1\\nKinpoly-OF [Luo et al. 2021] 0.62 323.4 141.6 7.3 4.2\\nAvatarPoser [Jiang et al. 2022] 0.43 140.2 126.5 7.1 3.8\\nEgoEgo [Li et al. 2023a] 0.20 148.0 121.1 6.2 2.7\\nTCM(ours)0.15 112.6 116.3 6.2 2.4\\nTable 6. Performance comparison between proposed and other methods\\non human dance estimation from egocentric and music. Bold indicates the\\nbest results, and underline indicates the second-best results.\\nBaselinesO ℎ𝑒𝑎𝑑↓T ℎ𝑒𝑎𝑑↓MPJPE↓Accel↓FS↓\\nPose-Reg [Yuan and Kitani 2019] 1.21 642.47 377.56 30.36 56.18\\nKinpoly [Luo et al. 2021] 0.78 354.19 251.27 17.84 25.31\\nEgoEgo [Li et al. 2023a] 0.67 347.23 234.58 16.76 20.15\\nFACT [Li et al. 2021] 1.37 685.81 244.89 17.54 18.53\\nBailando [Siyao et al. 2022] 1.44 688.54 231.77 14.23 18.67\\nEDGE [Tseng et al. 2023] 1.27 644.62 213.37 13.78 14.33\\nEMM [Nguyen et al. 2025] 0.61322.19191.55 12.76 13.18\\nTCM(ours)0.50335.41 130.22 11.05 12.34\\nbaselines: PoseReg [Yuan and Kitani 2019], KinPoly-OF [Luo et al.\\n2021], AvatarPoser [Jiang et al. 2022], and EgoEgo [Li et al. 2023a].\\nDataset and metrics.We use ARES dataset [Li et al . 2023a],\\nwhich comprises approximately 2,354 indoor motion sequences\\npaired with corresponding egocentric videos. To evaluate the re-\\nsults, we adopt the evaluation metrics used in EgoEgo [Li et al .\\n2023a], including: mean per joint position error (MPJPE) for over-\\nall positional accuracy; head pose error, measured by orientation\\n(Ohead) and translation ( Thead), for assessing head alignment; ac-\\nceleration error (Accel) to evaluate motion smoothness; and foot\\nskating (FS) to measure artifacts in foot-ground contact consistency.\\nResults.Table 5 compares our method and several baselines\\non the ARES dataset. Our approach consistently outperforms all\\nbaselines by a large margin across multiple metrics. We present\\nqualitative comparisons with the second-best method, EgoEgo, in\\nTable 7. Comparative results on human motion generation from object\\nmovement task. Bold indicates best and underline indicates second best.\\nMethod Hand JPE↓MPJPE↓MPVPE↓T root↓Oroot↓Cprec↑Crec↑F1 Score↑\\nGOAL [Taheri et al. 2022] 49.90 15.64 21.82 34.35 0.760.830.23 0.32\\nOMOMO [Li et al. 2023b] 24.0112.42 16.67 18.44 0.50 0.820.70 0.72\\nCHOIS [Li et al. 2024a] - 15.82 - 24.75 - 0.77 0.65 0.67\\nTCM (ours) 23.40 11.34 15.18 17.46 0.450.82 0.67 0.70\\nFig. 5. Our method produces more accurate and realistic motion,\\ndemonstrating better alignment with egocentric cues.\\n4.3.3 Human Motion Estimation from Egocentric and Music.In prac-\\ntice, the temporal condition can be single modality (e.g., music-to-\\ndance) or multiple modalities. In this experiment, we explore how\\nour TCM performs when the input conditions have multiple modal-\\nities. In particular, we set up the task of human motion estimation\\nfrom egocentric and music input. This task requires the alignment\\nbetween the head motion with the egocentric view, while ensuring\\nthe body moves naturally with the music. We use a diffusion model\\nthat adopts the same architecture as the proposed TCM. For feature\\nextraction, we use Jukebox [Dhariwal et al. 2020] to obtain music\\nembeddings and ResNet-50 [He et al. 2016] to extract egocentric vi-\\nsual embeddings. The music and visual embeddings are first aligned\\nvia temporal contrastive loss [Han et al. 2022], then fused through\\nan MLP to produce a unified temporal condition embedding.\\nBaselines.We compare our TCM method with egocentric motion\\nestimation methods (PoseReg [Yuan and Kitani 2019], Kinpoly [Luo\\net al. 2021], EgoEgo [Li et al . 2023a]) and music-driven motion\\ngeneration methods (FACT [Li et al . 2021], Bailando [Siyao et al .\\n2022], EDGE [Tseng et al . 2023]). Since our task leverages both\\negocentric video and music, we ensure fair comparison by equipping\\negocentric baselines with the Jukebox [Dhariwal et al. 2020] encoder\\nfor audio processing. In addition to these baselines, we compare our\\nwork with EMM [Nguyen et al. 2025], a multimodal Mamba-based\\nmodel for human motion generation. For music-based models, we\\ninject visual features from the egocentric video using [He et al. 2016].\\nMore implementation details will be released with our source code.\\nDataset and metrics.We use the EgoExo4D dataset [Grauman\\net al. 2024], which contains approximately two hours of dance se-\\nquences, along with egocentric videos and music. Following [Li\\net al. 2023a], we evaluate our method using five standard metrics\\ncommonly used in human pose estimation task:Ohead, Thead, MPJPE,\\nAccel, FS.\\nResults.We report the quantitative performance of our method\\nand other baselines in Table 6. The results indicate that our approach\\ngenerates more physically plausible motions and achieves superior\\nalignment with both the egocentric video and the accompanying\\nmusic. For qualitative results please refer to our demo video.\\n4.3.4 Human Motion Synthesis from Object Trajectories.In this task,\\nwe aim to generate full-body human motion from a sequence of\\nobject trajectories [Li et al. 2023b]. We adopt a two-stage approach\\nbased on a conditional diffusion framework, following the method-\\nology proposed in [Li et al . 2023b]. In the first stage, the model\\npredicts the positions of the hands in contact with the object based\\non its geometry. The second stage then generates full-body poses\\nconditioned on the predicted hand joint positions. Our TCM is used\\nas a backbone in both stages. Specifically, the sequence of object\\nSA Conference Papers ’25, December 15–18, 2025, Hong Kong.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a04d7a30-d4bb-4e89-84ca-a9f9680d2268', embedding=None, metadata={'page_label': '8', 'file_name': '2510.12573v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.12573v1.pdf', 'file_type': 'application/pdf', 'file_size': 2897796, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8•Quang Nguyen et al.\\nEgocentric video input\\n Ours\\nGround truth EgoEgo\\nFig. 5. Qualitative comparison of human motion estimation from the egocentric task. Our method produces more coherent and accurate motion compared to\\nEgoEgo. For additional visualizations, please refer to our demo video.\\n(a) Ground truth\\n (b) OMOMO\\n (c) Ours\\nFig. 6. Qualitative comparison on human motion generation from object trajectories. The top row shows an example where OMOMO produces motion that\\nresembles carrying a box rather than a monitor. In the bottom example, the contact generated by our method appears more plausible compared to OMOMO.\\nmeshes in stage one and the sequence of predicted hands position\\nin stage two are fed into TCM as temporal conditions.\\nDataset and metrics.We use the Omomo dataset [Li et al. 2023b],\\na motion capture dataset comprising approximately 10 hours of\\nmotion data collected from 17 subjects. For consistency with prior\\nwork, we adopt the same data partitioning strategy for training and\\nevaluation, particularly 15 subjects for training and 2 subjects for\\ntesting. We evaluate the results using metrics in [Li et al. 2023b].\\nResults.We compare our method with two transformer-based\\nbaselines: GOAL [Taheri et al. 2022] and OMOMO [Li et al. 2023b].\\nWe present the qualitative comparison results in Fig. 6. Our pro-\\nposed method generates more plausible human motions compared\\nto OMOMO. Quantitative results in Table 7 further confirm that\\nour approach outperforms existing baselines in generating realistic\\nobject-conditioned motion.\\n4.4 Discussion\\n4.4.1 Limitations.Although our proposed approach adapts well to\\nmany tasks, it has certain limitations. The Temporally Conditioned\\nMamba is specifically designed for time-dependent conditions, and\\nmay not be optimal for static inputs such as text [Karunratanakul\\net al. 2023] or scene descriptions [Huang et al . 2023], where the\\ncondition does not share the same temporal resolution or length\\nas the motion sequence. Second, in the egocentric2motion task, ex-\\ntreme or abrupt head movements (e.g., sudden jerks or whiplash-like\\nmotions) remain challenging and may result in less accurate motion\\ngeneration. We consider this an important direction for future work.\\nFinally, the generated motion may still exhibit visible foot-skating\\nand jitter artifacts. Incorporating additional kinematic constraints\\n(e.g., velocity and acceleration losses and adding modules for ex-\\nplicit foot-contact modeling will likely improve motion stability and\\nreduce such artifacts [Li et al. 2024c].\\nSA Conference Papers ’25, December 15–18, 2025, Hong Kong.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b009e998-ca0b-4a2d-a39d-e2db6a57254a', embedding=None, metadata={'page_label': '9', 'file_name': '2510.12573v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.12573v1.pdf', 'file_type': 'application/pdf', 'file_size': 2897796, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Learning Human Motion with Temporally Conditional Mamba•9\\n4.4.2 Broader Impact.We believe that our work presents a sig-\\nnificant step forward in human motion learning. Our method has\\nbroad potential across areas such as animation, AR/VR, and human-\\nrobot interaction, where the generated motion must respond to\\ndynamic inputs. While we focus on human motion learning, the\\ncore idea of TCM can be applied to other time-based tasks, such as\\nspeech [Mehrish et al. 2023; Zhang et al. 2025] and physiological\\nsignal modeling [Zou et al. 2024, 2025]. We hope this work inspires\\nfurther research into learning from complex, real-world temporal\\ndata, especially as temporally conditioned generation becomes in-\\ncreasingly relevant across applications.\\n5 Conclusion\\nIn this paper, we propose a new diffusion-based framework for hu-\\nman motion learning, built on the Mamba architecture and tailored\\nfor temporal conditioning. Our Temporally Conditioned Mamba\\n(TCM) recursively integrates the conditional signal and motion se-\\nquence, enabling stronger temporal alignment and more coherent\\nmotion generation. Extensive experiments show that our method\\nconsistently outperforms other baselines, achieving state-of-the-\\nart results across four different tasks in both motion synthesis and\\nestimation settings. Our code and trained models will be released\\npublicly to encourage further research.\\nReferences\\nSimon Alexanderson, Rajmund Nagy, Jonas Beskow, and Gustav Eje Henter. 2023.\\nListen, denoise, action! audio-driven motion synthesis with diffusion models.ACM\\nTOG(2023).\\nAli Behrouz and Farnoosh Hashemi. 2024. Graph mamba: Towards learning on graphs\\nwith state space models. InKDD.\\nCorentin Briat. 2014. Linear parameter-varying and time-delay systems.Analysis,\\nobservation, filtering & control(2014).\\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea\\nFinn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al.\\n2022. Rt-1: Robotics transformer for real-world control at scale.arXiv preprint\\narXiv:2212.06817(2022).\\nCaroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A Efros. 2019. Everybody\\ndance now. InICCV.\\nJunsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhong-\\ndao Wang, James Kwok, Ping Luo, Huchuan Lu, et al . 2023b. Fast training of\\ndiffusion transformer for photorealistic text-to-image synthesis.arXiv preprint\\narXiv:2310.00426(2023).\\nXin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. 2023a.\\nExecuting your commands via motion diffusion in latent space. InCVPR.\\nKiran Chhatre, Nikos Athanasiou, Giorgio Becherini, Christopher Peters, Michael J\\nBlack, Timo Bolkart, et al. 2024. Emotional speech-driven 3d body animation via\\ndisentangled latent diffusion. InCVPR.\\nCheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel,\\nRuss Tedrake, and Shuran Song. 2023. Diffusion policy: Visuomotor policy learning\\nvia action diffusion.The International Journal of Robotics Research(2023).\\nWenxun Dai, Ling-Hao Chen, Jingbo Wang, Jinpeng Liu, Bo Dai, and Yansong Tang.\\n2024. Motionlcm: Real-time controllable motion generation via latent consistency\\nmodel. InECCV.\\nPrafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and\\nIlya Sutskever. 2020. Jukebox: A generative model for music.arXiv:2005.00341\\n(2020).\\nPrafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on image\\nsynthesis.NeurIPS(2021).\\nYuming Du, Robin Kips, Albert Pumarola, Sebastian Starke, Ali Thabet, and Artsiom\\nSanakoyeu. 2023. Avatars grow legs: Generating smooth human motion from sparse\\ntracking inputs with diffusion model. InCVPR.\\nVincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. 2017. A learned represen-\\ntation for artistic style.ICLR(2017).\\nJibin Gao, Junfu Pu, Honglun Zhang, Ying Shan, and Wei-Shi Zheng. 2022. PC-dance:\\nPosture-controllable music-driven dance synthesis. InACM ICM.\\nKristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik,\\nTriantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram\\nBoote, et al. 2024. Ego-exo4d: Understanding skilled human activity from first-and\\nthird-person perspectives. InCVPR.\\nAlbert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective\\nstate spaces.arXiv:2312.00752(2023).\\nAlbert Gu, Karan Goel, and Christopher Ré. 2022. Efficiently Modeling Long Sequences\\nwith Structured State Spaces. InICLR.\\nAlbert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher\\nRé. 2021. Combining recurrent, convolutional, and continuous-time models with\\nlinear state space layers.NeurIPS(2021).\\nChuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. 2022.\\nGenerating diverse and natural 3d human motions from text. InCVPR.\\nTengda Han, Weidi Xie, and Andrew Zisserman. 2022. Temporal alignment networks\\nfor long-term video. InCVPR.\\nAli Hatamizadeh and Jan Kautz. 2024. Mambavision: A hybrid mamba-transformer\\nvision backbone.arXiv:2407.08083(2024).\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning\\nfor image recognition. InCVPR.\\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic\\nmodels.NeurIPS(2020).\\nFangzhou Hong, Vladimir Guzov, Hyo Jin Kim, Yuting Ye, Richard Newcombe, Ziwei\\nLiu, and Lingni Ma. 2025. Egolm: Multi-modal language model of egocentric motions.\\nInCVPR.\\nVincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan\\nMa, Johannes Fischer, and Björn Ommer. 2024. Zigma: A dit-style zigzag mamba\\ndiffusion model. InECCV.\\nRuozi Huang, Huang Hu, Wei Wu, Kei Sawada, Mi Zhang, and Daxin Jiang. 2020. Dance\\nrevolution: Long-term dance generation with music via curriculum learning. In\\nICLR.\\nSiyuan Huang, Zan Wang, Puhao Li, Baoxiong Jia, Tengyu Liu, Yixin Zhu, Wei Liang,\\nand Song-Chun Zhu. 2023. Diffusion-based generation, optimization, and planning\\nin 3d scenes. InCVPR.\\nXun Huang and Serge Belongie. 2017. Arbitrary style transfer in real-time with adaptive\\ninstance normalization. InICCV.\\nZikai Huang, Xuemiao Xu, Cheng Xu, Huaidong Zhang, Chenxi Zheng, Jing Qin,\\nand Shengfeng He. 2024. Beat-It: Beat-Synchronized Multi-Condition 3D Dance\\nGeneration. InECCV.\\nSergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep\\nnetwork training by reducing internal covariate shift. InICML.\\nBiao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. 2023. Motiongpt:\\nHuman motion as a foreign language.NeurIPS(2023).\\nJiaxi Jiang, Paul Streli, Huajian Qiu, Andreas Fender, Larissa Laich, Patrick Snape, and\\nChristian Holz. 2022. Avatarposer: Articulated full-body pose tracking from sparse\\nmotion sensing. InECCV.\\nTero Karras, Samuli Laine, and Timo Aila. 2019. A style-based generator architecture\\nfor generative adversarial networks. InCVPR.\\nKorrawe Karunratanakul, Konpat Preechakul, Supasorn Suwajanakorn, and Siyu Tang.\\n2023. Guided motion diffusion for controllable human motion synthesis. InICCV.\\nJinwoo Kim, Heeseok Oh, Seongjean Kim, Hoseok Tong, and Sanghoon Lee. 2022.\\nA brand new dance partner: Music-conditioned pluralistic dancing controlled by\\nmultiple dance genres. InCVPR.\\nTaesup Kim, Inchul Song, and Yoshua Bengio. 2017. Dynamic layer normalization\\nfor adaptive neural acoustic modeling in speech recognition.arXiv preprint\\narXiv:1707.06065(2017).\\nKit Yung Lam, Liang Yang, Ahmad Alhilal, Lik-Hang Lee, Gareth Tyson, and Pan Hui.\\n2022. Human-avatar interaction in metaverse: Framework for full-body interaction.\\nInACM ICMA.\\nNhat Le, Tuong Do, Khoa Do, Hien Nguyen, Erman Tjiputra, Quang D Tran, and Anh\\nNguyen. 2023a. Controllable group choreography using contrastive diffusion.ACM\\nTOG(2023).\\nNhat Le, Thang Pham, Tuong Do, Erman Tjiputra, Quang D Tran, and Anh Nguyen.\\n2023b. Music-driven group choreography. InCVPR.\\nByung-Kwan Lee, Chae Won Kim, Beomchan Park, and Yong Man Ro. 2024. Meteor:\\nMamba-based Traversal of Rationale for Large Language and Vision Models. In\\nNeurIPS, A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and\\nC. Zhang (Eds.).\\nHaopeng Li, Jinyue Yang, Kexin Wang, Xuerui Qiu, Yuhong Chou, Xin Li, and Guoqi\\nLi. 2024b. Scalable autoregressive image generation with mamba.arXiv preprint\\narXiv:2408.12245(2024).\\nJiaman Li, Alexander Clegg, Roozbeh Mottaghi, Jiajun Wu, Xavier Puig, and C Karen\\nLiu. 2024a. Controllable human-object interaction synthesis. InECCV.\\nJiaman Li, Karen Liu, and Jiajun Wu. 2023a. Ego-body pose estimation via ego-head\\npose estimation. InCVPR.\\nJiaman Li, Jiajun Wu, and C Karen Liu. 2023b. Object motion guided human motion\\nsynthesis.ACM TOG(2023).\\nJiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang, Sanja Fidler, and Hao Li. 2020.\\nLearning to generate diverse dance motions with transformer.arXiv:2008.08171\\nSA Conference Papers ’25, December 15–18, 2025, Hong Kong.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='21ae8e34-ab09-4500-ab32-9d6394255614', embedding=None, metadata={'page_label': '10', 'file_name': '2510.12573v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.12573v1.pdf', 'file_type': 'application/pdf', 'file_size': 2897796, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10•Quang Nguyen et al.\\n(2020).\\nRuilong Li, Shan Yang, David A Ross, and Angjoo Kanazawa. 2021. Ai choreographer:\\nMusic conditioned 3d dance generation with aist++. InICCV.\\nRonghui Li, YuXiang Zhang, Yachao Zhang, Hongwen Zhang, Jie Guo, Yan Zhang,\\nYebin Liu, and Xiu Li. 2024c. Lodge: A coarse to fine diffusion network for long\\ndance generation guided by the characteristic dance primitives. InCVPR.\\nShufan Li, Harkanwar Singh, and Aditya Grover. 2025. Mamba-ND: Selective State\\nSpace Modeling for Multi-Dimensional Data.ECCV(2025).\\nDingkang Liang, Xin Zhou, Wei Xu, Xingkui Zhu, Zhikang Zou, Xiaoqing Ye, Xiao\\nTan, and Xiang Bai. 2024. Pointmamba: A simple state space model for point cloud\\nanalysis. InNeurIPS.\\nBencheng Liao, Wenyu Liu, Xinggang Wang, Xinlong Wang, Qian Zhang, and Lianghui\\nZhu. 2024. Vision mamba: Efficient visual representation learning with bidirectional\\nstate space model.arXiv preprint arXiv:2401.09417(2024).\\nHaohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu\\nWang, and Mark D Plumbley. 2023. Audioldm: Text-to-audio generation with latent\\ndiffusion models.ICML(2023).\\nJiuming Liu, Ruiji Yu, Yian Wang, Yu Zheng, Tianchen Deng, Weicai Ye, and Hesheng\\nWang. 2024. Point mamba: A novel point cloud backbone based on state space model\\nwith octree-based ordering strategy.arXiv preprint arXiv:2403.06467(2024).\\nMatthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J\\nBlack. 2023. SMPL: A skinned multi-person linear model. InSeminal Graphics\\nPapers.\\nZhengyi Luo, Ryo Hachiuma, Ye Yuan, and Kris Kitani. 2021. Dynamics-regulated\\nkinematic policy for egocentric pose estimation.NeuRIPS(2021).\\nSoroush Mehraban, Vida Adeli, and Babak Taati. 2024. Motionagformer: Enhancing 3d\\nhuman pose estimation with a transformer-gcnformer network. InWACV.\\nAmbuj Mehrish, Navonil Majumder, Rishabh Bharadwaj, Rada Mihalcea, and Soujanya\\nPoria. 2023. A review of deep learning techniques for speech processing.Information\\nFusion(2023).\\nJavad Mohammadpour and Carsten W Scherer. 2012.Control of linear parameter varying\\nsystems with applications. Springer Science & Business Media.\\nQuang Nguyen, Nhat Le, Baoru Huang, Minh Nhat Vu, Chengcheng Tang, Van Nguyen,\\nNgan Le, Thieu Vo, and Anh Nguyen. 2025. EgoMusic-driven Human Dance Motion\\nEstimation with Skeleton Mamba.arXiv:2508.10522(2025).\\nYusuke Nishimura, Yutaka Nakamura, and Hiroshi Ishiguro. 2020. Long-term motion\\ngeneration for interactive humanoid robots using GAN with convolutional network.\\nInACM/IEEE ICHRI.\\nWilliam Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. In\\nICCV.\\nEthan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville.\\n2018. Film: Visual reasoning with a general conditioning layer. InAAAI.\\nMathis Petrovich, Michael J Black, and Gül Varol. 2021. Action-conditioned 3D human\\nmotion synthesis with transformer VAE. InICCV.\\nMathis Petrovich, Michael J Black, and Gül Varol. 2022. Temos: Generating diverse\\nhuman motions from textual descriptions. InECCV. Springer.\\nHao Phung, Quan Dao, Trung Dao, Hoang Phan, Dimitris Metaxas, and Anh Tran. 2024.\\nDiMSUM: Diffusion Mamba–A Scalable and Unified Spatial-Frequency Method for\\nImage Generation.arXiv:2411.04168(2024).\\nEkkasit Pinyoanuntapong, Muhammad Usama Saleem, Pu Wang, Minwoo Lee, Srijan\\nDas, and Chen Chen. 2024. BAMM: bidirectional autoregressive motion model. In\\nECCV.\\nDavis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, and\\nLeonidas J Guibas. 2021. Humor: 3d human motion model for robust pose esti-\\nmation. InICCV.\\nLi Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang, Chen Qian, Chen Change\\nLoy, and Ziwei Liu. 2022. Bailando: 3d dance generation by actor-critic gpt with\\nchoreographic memory. InCVPR.\\nSebastian Starke, Ian Mason, and Taku Komura. 2022. Deepphase: Periodic autoencoders\\nfor learning motion phase manifolds.ACM ToG(2022).\\nSebastian Starke, Paul Starke, Nicky He, Taku Komura, and Yuting Ye. 2024. Categorical\\ncodebook matching for embodied character controllers.ACM TOG(2024).\\nOmid Taheri, Vasileios Choutas, Michael J Black, and Dimitrios Tzionas. 2022. Goal:\\nGenerating 4d whole-body motion for hand-object grasping. InCVPR.\\nJiangnan Tang, Jingya Wang, Kaiyang Ji, Lan Xu, Jingyi Yu, and Ye Shi. 2024. A unified\\ndiffusion framework for scene-aware human motion estimation from sparse signals.\\nInCVPR.\\nOcto Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees,\\nSudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al . 2024. Octo: An\\nopen-source generalist robot policy.arXiv preprint arXiv:2405.12213(2024).\\nYao Teng, Yue Wu, Han Shi, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and\\nXihui Liu. 2024. Dim: Diffusion mamba for efficient high-resolution image synthesis.\\narXiv preprint arXiv:2405.14224(2024).\\nGuy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim\\nBermano. 2023. Human Motion Diffusion Model. InICLR.\\nJonathan Tseng, Rodrigo Castellon, and Karen Liu. 2023. Edge: Editable dance generation\\nfrom music. InCVPR.\\nMehmet Ozgur Turkoglu, Alexander Becker, Hüseyin Anil Gündüz, Mina Rezaei, Bernd\\nBischl, Rodrigo Caye Daudt, Stefano D’Aronco, Jan Wegner, and Konrad Schindler.\\n2022. Film-ensemble: Probabilistic deep learning via feature-wise linear modulation.\\nNeurIPS(2022).\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\\nGomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need.NeurIPS\\n(2017).\\nChloe Wang, Oleksii Tsepa, Jun Ma, and Bo Wang. 2024c. Graph-mamba: Towards\\nlong-range graph sequence modeling with selective state spaces.arXiv preprint\\narXiv:2402.00789(2024).\\nJian Wang, Zhe Cao, Diogo Luvizon, Lingjie Liu, Kripasindhu Sarkar, Danhang Tang,\\nThabo Beeler, and Christian Theobalt. 2024a. Egocentric whole-body motion capture\\nwith fisheyevit and diffusion-based motion refinement. InCVPR.\\nJian Wang, Diogo Luvizon, Weipeng Xu, Lingjie Liu, Kripasindhu Sarkar, and Christian\\nTheobalt. 2023. Scene-aware egocentric 3d human pose estimation. InCVPR.\\nXinghan Wang, Zixi Kang, and Yadong Mu. 2024b. Text-controlled motion mamba:\\ntext-instructed temporal grounding of human motion.arXiv:2404.11375(2024).\\nZiyang Wang, Jian-Qing Zheng, Yichi Zhang, Ge Cui, and Lei Li. 2024d. Mamba-\\nunet: Unet-like pure visual mamba for medical image segmentation.arXiv preprint\\narXiv:2402.05079(2024).\\nJingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. 2019. Un-\\nderstanding and improving layer normalization.NeurIPS(2019).\\nJing Nathan Yan, Jiatao Gu, and Alexander M Rush. 2024. Diffusion models without\\nattention. InCVPR.\\nWeihao Yuan, Yisheng He, Weichao Shen, Yuan Dong, Xiaodong Gu, Zilong Dong,\\nLiefeng Bo, and Qixing Huang. 2024. Mogents: Motion generation based on spatial-\\ntemporal joint modeling.NeurIPS(2024).\\nYe Yuan and Kris Kitani. 2019. Ego-pose estimation and forecasting as real-time pd\\ncontrol. InICCV.\\nCanyu Zhang, Youbao Tang, Ning Zhang, Ruei-Sung Lin, Mei Han, Jing Xiao, and Song\\nWang. 2024c. Bidirectional autoregessive diffusion model for dance generation. In\\nCVPR.\\nJianrong Zhang, Yangsong Zhang, Xiaodong Cun, Yong Zhang, Hongwei Zhao, Hong-\\ntao Lu, Xi Shen, and Ying Shan. 2023. Generating human motion from textual\\ndescriptions with discrete representations. InCVPR.\\nMingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang,\\nand Ziwei Liu. 2024a. Motiondiffuse: Text-driven human motion generation with\\ndiffusion model.TPAMI(2024).\\nXiangyu Zhang, Jianbo Ma, Mostafa Shahin, Beena Ahmed, and Julien Epps. 2025.\\nRethinking mamba in speech processing by self-supervised models. InICASSP.\\nZeyu Zhang, Akide Liu, Ian Reid, Richard Hartley, Bohan Zhuang, and Hao Tang. 2024b.\\nMotion mamba: Efficient and long sequence motion generation. InECCV.\\nAmy Zhao, Chengcheng Tang, Lezi Wang, Yijing Li, Mihika Dave, Lingling Tao, Christo-\\npher D Twigg, and Robert Y Wang. 2024. EgoBody3M: Egocentric Body Tracking\\non a VR Headset using a Diverse Dataset. InECCV.\\nWentao Zhu, Xiaoxuan Ma, Dongwoo Ro, Hai Ci, Jinlu Zhang, Jiaxin Shi, Feng Gao, Qi\\nTian, and Yizhou Wang. 2023. Human motion generation: A survey.TPAMI(2023).\\nWenlin Zhuang, Congyi Wang, Jinxiang Chai, Yangang Wang, Ming Shao, and Siyu\\nXia. 2022. Music2dance: Dancenet for music-driven dance generation.ACM TOMM\\n(2022).\\nBochao Zou, Zizheng Guo, Xiaocheng Hu, and Huimin Ma. 2024. Rhythmmamba: Fast\\nremote physiological measurement with arbitrary length videos.arXiv:2404.06483\\n(2024).\\nBochao Zou, Zizheng Guo, Xiaocheng Hu, and Huimin Ma. 2025. RhythmMamba: Fast,\\nLightweight, and Accurate Remote Physiological Measurement. InAAAI.\\nSA Conference Papers ’25, December 15–18, 2025, Hong Kong.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9ca8b948-6b3f-4f5c-896d-8666ff353385', embedding=None, metadata={'page_label': '1', 'file_name': '2510.13253v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.13253v1.pdf', 'file_type': 'application/pdf', 'file_size': 6563163, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='End-to-End Multi-Modal Diffusion Mamba\\nChunhao Lu1 Qiang Lu1B Meichen Dong1,2 Jake Luo3\\n1China University of Petroleum-Beijing, 2Leyard Optoelectronic, 3University of Wisconsin-Milwaukee\\n{luchunhao, meichen.dong}@student.cup.edu.cn luqiang@cup.edu.cn jakeluo@uwm.edu\\nAbstract\\nCurrent end-to-end multi-modal models utilize different en-\\ncoders and decoders to process input and output informa-\\ntion. This separation hinders the joint representation learn-\\ning of various modalities. To unify multi-modal processing,\\nwe propose a novel architecture called MDM (Multi-modal\\nDiffusion Mamba). MDM utilizes a Mamba-based multi-\\nstep selection diffusion model to progressively generate and\\nrefine modality-specific information through a unified vari-\\national autoencoder for both encoding and decoding. This\\ninnovative approach allows MDM to achieve superior per-\\nformance when processing high-dimensional data, particu-\\nlarly in generating high-resolution images and extended text\\nsequences simultaneously. Our evaluations in areas such\\nas image generation, image captioning, visual question an-\\nswering, text comprehension, and reasoning tasks demon-\\nstrate that MDM significantly outperforms existing end-to-\\nend models (MonoFormer, LlamaGen, and Chameleon etc.)\\nand competes effectively with SOTA models like GPT-4V ,\\nGemini Pro, and Mistral. Our results validate MDM’s ef-\\nfectiveness in unifying multi-modal processes while main-\\ntaining computational efficiency, establishing a new direc-\\ntion for end-to-end multi-modal architectures.\\n1. Introduction\\nTraditional large-scale multi-modal models [2, 4, 43, 49, 55,\\n64, 67–70, 86, 96–98] typically use multiple encoders and\\ndecoders to process multi-modal data. This approach makes\\nlearning a unified joint representation of the multi-modal\\ndata difficult and can significantly slow inference time (as\\nshown in Fig. 1A). To alleviate these problems, end-to-end\\nmodels without modal-fusion en(de)coder architecture have\\nbeen proposed (as shown in Fig. 1B). This approach offers\\na streamlined, unified processing framework that enhances\\nefficiency and consistency in multi-modal representation\\nlearning. Existing end-to-end models follow three primary\\nstrategies: (1) Autoregressive models [5, 33, 77, 79] lever-\\nage a single Transformer for both text and image generation,\\nbut struggle with the inherent sequential dependency of au-\\ntoregressive decoding. (2) Hybrid image generation mod-\\nFeed Forward\\nSelf-Attention\\nFeed Forward\\nBi Self-Attention\\nFeed Forward\\nCa Self-Attention\\nImageEncoderTextEncoder\\nTextDecoderITM ITC\\nA kitten looks up at a puppy, while the puppy gently gazes back.\\nFeed Forward\\nBi Self-AttentionCa Self-Attention\\nImage & Text Decoder\\nM× N× M×\\n(A) Traditional Multi-Modal Model(B) End-To-End Multi-Modal Model (C) Ours\\nVAE Encoder VAE Encoder\\nVAE Decoder\\nSE\\nM× Mamba Block\\nImgScan Switch\\nLM\\nDDPM LM\\nText Output for understanding and generation task.\\nTxt Scan Switch\\nPadding & Flatten\\n<BOI>kitten …A\\n?What …<EOI>\\n<BOI>kitten …A\\n?What …<EOI>\\n<Mask>kitten …A\\n<Mask>What …<EOI>\\n<BOI>kitten …A\\n?What …<EOI>\\nLLM EncoderInput Input\\nInput\\nVAE DecoderLLM Decoder\\nSimultaneouslySeparatelySeparately\\nFigure 1. Comparison of three types of models.\\nels [25, 88] integrate an additional image synthesis module,\\nimproving image quality but introducing extra complexity.\\n(3) Mixed autoregressive-diffusion models [10, 101, 102]\\nemploy diffusion-based image generation while maintain-\\ning an autoregressive framework for text, yet still struggles\\nwith unifying multi-modal.\\nDespite recent advancements, Transformer-based end-\\nto-end models face several critical challenges: (1) their\\nquadratic computational complexity makes them inefficient\\nfor generating high-resolution image and long-sequence\\ntext. Although various studies have attempted to optimize\\nthis computational complexity [1, 3, 14, 29, 31, 60, 63,\\n74, 82, 83], the challenge remain substantial. (2) their\\nreliance on multi-objective learning introduces conflicting\\noptimization goals, impeding convergence and hindering\\neffective joint representation learning. In contrast, state-\\nspace models like Mamba [28, 66] offer a compelling al-\\nternative due to their ability to scale linearly with sequence\\nlength while effectively capturing long-range dependen-\\ncies. However, the current multi-modal implementations of\\nMamba [20, 24, 32, 39, 52, 65, 81, 84, 90, 92] still adopt a\\nmulti-objective approach, limiting their capacity for end-to-\\nend joint representation learning.\\nTo effectively process multi-modal data, we propose\\nan end-to-end model called the Multi-Modal Diffusion\\nMamba (MDM) (as shown in Fig. 1c). MDM first employs\\npatchify [21] and embedding to pre-process multi-modal\\ndata. Then, it uses a variational autoencoder (V AE) [44]\\nas a multi-modal encoder, which uniformly maps the multi-\\n1\\narXiv:2510.13253v1  [cs.CV]  15 Oct 2025', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='690db9ac-825c-4f2f-a566-d6e4b54e0150', embedding=None, metadata={'page_label': '2', 'file_name': '2510.13253v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.13253v1.pdf', 'file_type': 'application/pdf', 'file_size': 6563163, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Mamba Block\\nA kitten looks up at a puppy, while the puppy gently gazes back.What is the kitten doing now in this graph?\\nVAE Encoder Image Scan Switch\\nMamba Block\\n A kitten looks up at a puppy, while the puppy gently gazes back.What is the kitten doing now in this graph?\\nText Scan Switch\\nMamba Block\\nText Scan Switch\\nImage Scan Switch\\nMamba Block\\nText Scan Switch\\nImagePad & Flatten\\nTextPad & Flatten\\nC\\nC\\npct pppppppp pp p\\n…\\n…\\nImage Pad & Flatten Image Scan Switch\\nParallelComputation \\nImage Scan Switch\\nAkitten <BOI>uplooks…\\nWhat<EOI>isthe…?\\n<BOI>Akittenuplooks…pct\\nWhat<EOI>isthe…?pct\\nTime TokenClass TokenPad Token\\ntcp\\nA puppy, whilethegentlygazesback. <BOI>puppykittenlooksupata\\nA puppy, whilethegentlygazesback. <BOI>puppykittenlooksupata\\n<EOI>Whatisthekittennowin ?doingthisgraph\\n<EOI>Whatisthekittennowin ?doingthisgraph\\nText Scan Switch\\nText Pad & Flatten\\nTime TokenClass TokenPad Token\\ntcp\\nTime TokenClass TokenPad Token\\ntcp\\n𝑧! 𝑧!\"# 𝑧!\"$𝐵𝐵 𝐵 𝑧!\"%𝐵\\n𝐻! 𝐻!\"# 𝐻!\"$ 𝐻!\"%\\nSweepdownSweepup+B𝑧!\"#𝐻! +B𝑧!\"$𝐻!\"#\\n+B𝑧!\"%𝐻!\"$B𝑧!\"$+B𝑧!\"%\\n𝐴 𝐴 𝐴𝐴\\nDenoise\\nPatchEmbed\\na b\\nd e f h\\nig\\nkitten kitten\\nkittenkitten\\nSelectionSelection\\nSelectionSelection\\nSelection\\nUnpatchUnembed\\nUnpad & Unflatten\\nc\\nVAE Decoder\\nFigure 2. Framework of Multi-Modal Diffusion Mamba. MDM first encodes inputs (caption, VQV AE-processed image, question) using\\nV AE (a), while performing padding (class, diffusion timestep, token completion) and flatten operations (d, e). Next, data reconstruction is\\nprogressively completed via diffusion mamba operations (b), modeling images and text temporally through scanning processes (f, g) for\\nefficient information selection (red boxes indicate selection). Selected data undergoes computation (i) guided by (h) within the Mamba-2\\nframework to update model parameters. Finally, the MDM output passes through the V AE decoder (c) to reconstruct real data.\\nmodal data to a noisy latent space (as illustrated in Fig. 2a).\\nMDM constructs a multi-step selection diffusion model\\nbased on the Mamba architecture as a uniform decoder for\\nthe rapid generation of multi-modal information.\\nThis decoder generates the target text or image step-by-\\nstep based on the diffusion process through the multi-step\\nselection diffusion model (as shown in Fig. 2b). To en-\\nhance decoding speed, the decoder employs the Score En-\\ntropy Loss [53] as the objective function instead of Markov\\nchain-based [37] methods for updating the network to han-\\ndle multi-modal data throughout the diffusion process. The\\ndecoder comprises two components: an image and text scan\\nswitch, and a Mamba-2 block [28]. The text scan switch has\\ntwo modes for sequence modeling (as shown in Fig. 2f),\\nwhile the image scan switch has four, based on the settings\\nof DiM [81] (as shown in Fig. 2e). The scan switches enable\\nthe model to capture sequential relationships across various\\ntemporal directions in the data. The selection state-space\\nstructure in Mamba then analyzes these sequential relation-\\nships within the current denoising step. This analysis guides\\nthe selection of relevant information to focus on and irrele-\\nvant information to ignore, effectively directing the model’s\\ndenoising process at each step.\\nSince MDM unifies the modality encoder and decoder,\\nthe model is capable of generating an image and text si-\\nmultaneously. For example, as shown in Fig. 2h, when\\ngenerating an image of a dog alongside its description, the\\nscan switch in the decoder first assesses whether the de-\\nscription contains conditions that necessitate image gener-\\nation. If such conditions exist, the image scan switch is\\nactivated. Consequently, the model directs its selection to\\nthe image patches corresponding to the dog during each de-\\nnoising step. This targeted focus guides the model to effec-\\ntively denoise relevant pixels while disregarding other areas\\nof the image. A similar selection process is employed for\\ntext data. Ultimately, the data, once denoised via the t-step\\ndiffusion process, is reconstructed into authentic text (or an\\nimage) through the V AE decoder simultaneously. The main\\ncontributions of this paper are as follows.\\n1) We introduce the Multi-Modal Diffusion Mamba\\n(MDM), an end-to-end model that achieves a computa-\\ntional complexity ofO(MLN 2), outperforming previous\\nend-to-end models like MonoFormer [101], which operate\\natO(ML 2N/G). This advancement enables the efficient\\ngeneration of long-sequence text and high-resolution im-\\nages.\\n2) We propose a novel multi-step selection diffusion\\nmodel that combines autoregressive and diffusion-based\\ngenerative paradigms into a unified learning objective. This\\nmethod effectively integrates both paradigms within a diffu-\\nsion process, generating multi-modal data simultaneously.\\n3) Our experimental results demonstrate MDM’s supe-\\nrior performance in image generation on the ImageNet [15]\\nand COCO datasets [42]. Additionally, it excels in vari-\\nous tasks, including image captioning on Flickr30K [94]\\nand COCO [42], VQA on VQAv2 [27], VizWiz [30], and\\nOKVQA [57], as well as text comprehension and reason-\\ning on seven datasets [7, 11, 12, 58, 73, 99]. Furthermore,\\nMDM shows strong results in math-related world knowl-\\nedge tasks on GSM8k [13], MATH [35], and MMLU [34].\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ce3335c0-0c1f-4fc5-9a3b-6dea5b9b2009', embedding=None, metadata={'page_label': '3', 'file_name': '2510.13253v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.13253v1.pdf', 'file_type': 'application/pdf', 'file_size': 6563163, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2. Related Works\\n2.1. Traditional large multi-modal model\\nMost existing LMMs are built by integrating architectures\\nfrom multiple modalities. SOTA image and video genera-\\ntion models employ pre-trained text encoders to represent\\ninput prompts in latent space, which then condition a diffu-\\nsion model for generating videos and images [9, 48, 72, 85].\\nMany researchers have adopted this approach, fusing fea-\\nture representations from multiple pre-trained encoders to\\nenhance model performance across different modalities [23,\\n62]. This pattern is also prevalent in visual language mod-\\nels, where pre-trained language models are typically aug-\\nmented with linear projection layers from other pre-trained\\nen/decoders for training in the text space. Examples in-\\nclude Flamingo [2] and LLaV A [51] for visual understand-\\ning, GILL [45] for visual generation, and DreamLLM [19]\\nfor both understanding and generation.\\n2.2. End-to-End multi-modal model\\nEnd-to-end models have emerged recently to facilitate joint\\nrepresentation learning while improving training and infer-\\nence efficiency. It can be categorized into three main types:\\n1)The autoregressive model[5, 33, 77, 79] utilizes one\\nTransformer with an autoregressive approach to generate\\nimages and text. For instance, the Fuyu model [5] processes\\nimage patches directly as input to achieve visual compre-\\nhension. Models like Chameleon [79], Mars [33], and Lla-\\nmaGen [77] convert images into discrete sequence tokens,\\nthen concatenate them with text.\\n2)The hybrid image generation model[25, 88] addresses\\nthe limitations of autoregressive approaches in image gen-\\neration. While maintaining an autoregressive structure for\\ntext generation, the models enhance image quality by incor-\\nporating an image-generation network. For example, Seed-\\nx model [25] focuses on enhancing specific aspects of im-\\nage generation, while Next-GPT [88] aims to expand multi-\\nmodal capabilities within an end-to-end framework.\\n3)The mixed autoregressive-diffusion model[101, 102]\\ncombines the strengths of previous approaches. It per-\\nforms text autoregressive generation and image diffusion\\nrestoration simultaneously. Models like MonoFormer [101]\\nand Transfusion [102] achieve this by incorporating causal\\nself-attention [91] for text tokens and bidirectional self-\\nattention [16] for image patches, enabling high-quality\\nmulti-modal understanding and generation.\\n2.3. Mamba in multi-modal model\\nMamba has emerged as a powerful alternative to Trans-\\nformer for multi-modal data alignment [20, 52, 84, 87, 92].\\nRecent works showcase Mamba’s capabilities across differ-\\nent multi-modal applications. VL-Mamba [65] combines a\\npre-trained Mamba model for language understanding with\\na connector module to align visual patches and language\\ntokens. However, these models lack end-to-end training\\ncapabilities and struggle to learn unified joint representa-\\ntions. MDM provides a truly end-to-end architecture, en-\\nabling rapid generation of high-quality, long sequences.\\n3. Multi-step Selection Diffusion Model\\nThe multi-step selection diffusion model enables rapid gen-\\neration of multi-modal information through two key pro-\\ncesses: diffusion&denoising and selection. During the\\ndiffusion&denoising, the model employs a unified Score\\nEntropy Loss [53](SE) to gradually reconstruct target data\\nfrom noise through a series of denoising steps (as illustrated\\nin Fig. 2b). The selection process enables the model to cap-\\nture sequential relationships across different temporal di-\\nmensions in the latent space, determining which informa-\\ntion should be focused on or ignored during each diffusion\\ndenoising step (as shown in Fig. 2h).\\n3.1. Diffusion & Denoising\\nThe diffusion&denoising process comprises two main\\ncomponents: diffusion and denoising. The diffusion com-\\nponent can be expressed by the following equation:\\nzg\\nn,t =\\nq\\n¯αg\\nt zg\\nn,0 +\\nq\\n1−¯αg\\nt ϵg\\nn,t, (1)\\nwheregdenotes either image patch or text embedding,\\nandz g\\nn,0 represents the latent space vector of then-th im-\\nage patch or text embedding, obtained through V AE sam-\\npling [44].z g\\nn,t is derived fromz g\\nn,0 aftertsteps of noise\\naddition;ϵ g\\nn,t ∼ N(0, I)represents the added noise;¯αg\\nt =Qt\\nk=1 αg\\nk,α g\\nk = 1−β g\\nk, and{β g\\nk ∈(0,1)} T\\nk=1 are Gaussian\\ndistribution hyperparameters controlling the forward diffu-\\nsion noise. Following the diffusion Markov principle [37],\\nt-step forward diffusion process can be characterized by\\nconditional probabilities as follows:\\np(zg\\nn,t|zg\\nn,0) =N(z g\\nn,t;\\nq\\n¯αg\\nt zg\\nn,0,(1−¯αg\\nt )I), (2)\\nwhich means that givenz g\\nn,0,z g\\nn,t follows a Gaussian distri-\\nbution with\\np\\n¯αg\\nt zg\\nn,0 as mean and(1−¯α g\\nt )Ias variance.\\nIn the classic diffusion denoising component [37], the\\nmodel needs to learn the posteriorp(z g\\nn,t−1|zg\\nn,t)to gradu-\\nally reconstruct the data. Sincep(z g\\nn,t|zg\\nn,0)follows a Gaus-\\nsian distribution, we can assume that the approximate dis-\\ntribution of the denoising process is:\\npθ(zg\\nn,t−1|zg\\nn,t) =N(z g\\nn,t−1;µ θ(zg\\nn,t),(σ g\\nθ,n)2). (3)\\nwhereµ θ(zg\\nn,t)andσ g\\nθ,n represent the model predicted\\nnoise mean and variance at thet-th denoising step.\\nThis method achieves the gradual recovery of data by\\noptimizing the conditional probability of each time step by\\nmaximum likelihood. However, Markov chain-based [37]\\nmethods limit computational efficiency in high-dimensional\\nspaces and are difficult to extend to discrete data.\\nTo further optimize the denoising process, this paper\\nuses SE [53] as the optimization target. It is a general-\\nized score matching objective that aims to directly learn the\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1d4ffd56-da51-47a5-aa91-9069230e7a4f', embedding=None, metadata={'page_label': '4', 'file_name': '2510.13253v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.13253v1.pdf', 'file_type': 'application/pdf', 'file_size': 6563163, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='probability density ratio between discrete states. The SE\\ncan not only stabilize the diffusion denoising process but\\nalso improve the sampling quality through the global infor-\\nmation of data distribution. In general form, for any state\\npair(z g\\nn,t, zg\\nn,0), define the model’s score ratios θ(zg\\nn,t),\\nwhich represents the relative probability of transferring\\nfromz g\\nn,t toz g\\nn,0. SE is defined as:\\nse=\\nX\\ny∈zg\\nn,0:t−1\\nωg\\nzg\\nn,t\\n\\x12\\nsθ(zg\\nn,t)− pdata(y)\\npdata(zg\\nn,t) logs θ(zg\\nn,t)\\n+K\\n\\x12 pdata(y)\\npdata(zg\\nn,t)\\n\\x13\\x13\\n,\\n(4)\\nwhereω g\\nzg\\nn,t\\nis the weight of the loss term, which is used to\\nbalance the loss of different states.K(a) =a(loga−1)is\\na normalization term that ensures the loss is non-negative.\\npdata(y)\\npdata(zg\\nn,t) represents the actual score ratio.p data(y)and\\npdata(zg\\nn,t)are the actual data distributions of the former\\nnoisy state and the current noisy state. The actual score\\nratio calculation relationship is shown in Theorem 1.\\nTheorem 1.According to Bayes’ theorem and the Gaus-\\nsian distribution density formula, the following calculation\\nrelationship of pdata(y)\\npdata(zg\\nn,t) is obtained:\\npdata(y)\\npdata(zg\\nn,t) = exp\\n \\n∥zg\\nn,t∥2\\n2 − ∥zg\\nn,t −\\np\\n¯αg\\nt zg\\nn,0∥2\\n2(1−¯αg\\nt )\\n!\\n.\\n(5)\\nThe proof is provided in Appendix A.\\nBased on the SE [53], the model predicted score ratio\\nindicates how the model adjusts the probability of the cur-\\nrent state to tend to the original data distribution during the\\ndenoising process. The definition is as follows:\\nsθ(zg\\nn,t) = pθ(zg\\nn,0)\\npθ(zg\\nn,t) , (6)\\nwhere the denominator represents the probability of the cur-\\nrent noise state and the numerator represents the original\\nstate probability estimated by the model. According to The-\\norem 2, the model usessoftmaxfor normalization ensur-\\ning numerical stability and enabling gradient optimization\\nwhen predicting the score ratio.\\nTheorem 2.Given the denoising process modelled by\\na score-based probability ratio functions θ(zg\\nn,t), defined\\nas Eq.(6), this paper defines a learnable approximation us-\\ning a parameterized score functionf θ, such that the proba-\\nbility ratio can be estimated as:\\nsθ(zg\\nn,t) = exp (fθ(zg\\nn,t, zg\\nn,0))P\\ny∈zg\\nn,0:t−1\\nexp (fθ(zg\\nn,t, y)), (7)\\nThe proof is provided in Appendix A.\\n3.2. Selection\\nThe selection process comprises two key steps: scan switch\\nand selection. The scan switch mechanism captures tem-\\nporal relationships between adjacent image patches (or\\ntext embeddings) by generating latent space representations\\nwithkdifferent sequential relationships, such as four im-\\nage patch sequences and two text embedding sequences il-\\nlustrated in Fig. 2fg. The mechanism createsktemporal\\nsequencesS={⟨z g\\n1,t, zg\\n2,t, . . . , zg\\ni,t⟩}k.\\nThe selection step then analyzes these different sequen-\\ntial relationships at the current denoising steptto determine\\nwhich information should be focused on or ignored, thereby\\nguiding the model’s denoising direction in each diffusion\\nstep. The selection step choosesjitemsz g\\nn,t from each se-\\nquence inSaccording to the following Theorem 3. So, the\\nselection step obtainkselection sequences with different\\nlengths, i.e.,S ′ ={⟨z g\\nj1,t, zg\\nj2,t, . . . , zg\\nj,t⟩}k andS ′ ∈S.\\nTheorem 3.To achieve the optimalscoreentropy [53]\\nwhich is demonstrated on Eq.(4), the selection step choose\\njitems where eachz g\\nn,t satisfiesse= 0, i.e.,\\nsθ(zg\\nn,t)≈ pdata(y)\\npdata(zg\\nn,t) (8)\\nThe proof is provided in Appendix A.\\n4. Architecture\\nThe neural network architecture consists of two primary\\ncomponents: a V AE noisy latent encoder [44] and a multi-\\nstep selection diffusion decoder, as illustrated in Fig. 2ab.\\nThe encoder first processes image dataX img through\\npatchify [21] operations and processes text dataX txt\\nthrough tokenization based on SentencePiece with Unigram\\nBPE [47] and embedding operations, then uniformly maps\\nthem to the latent space before applying forward noise.\\nThe decoder, based on the multi-step selection diffusion\\nmodel, leverages Mamba to achieve unified learning objec-\\ntives while enhancing computational efficiency for process-\\ning long sequence data. It employs the SE [53] as the uni-\\nfied objective for both image and text modalities during the\\ndiffusion process. During selection, the model captures se-\\nquential relationships across different temporal dimensions\\nusing various scan switches. These relationships are then\\nefficiently processed through the selection state-space struc-\\nture in the Mamba Block determining which information to\\nfocus on or ignore according to Eq. (8), thereby guiding\\nsubsequent diffusion denoising steps (as shown in Fig. 2h).\\nFinally, the reconstructed image patches and text embed-\\ndings are transformed back into their original data formats\\nthrough a V AE noisy latent decoder [44].\\n4.1. The noisy latent encoder\\nThe noisy latent encoder first processes input imageX img\\nthrough patchify and processes textX txt through tokeniza-\\ntion and embedding operations to obtain the patch sequence\\n4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e18351e4-7e04-4d8b-ba59-6b9d0c310664', embedding=None, metadata={'page_label': '5', 'file_name': '2510.13253v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.13253v1.pdf', 'file_type': 'application/pdf', 'file_size': 6563163, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='G(Ximg/Xtxt) =⟨g 1, g2, . . . , gi⟩, whereg n represents the\\nn-th image patch or text embedding, respectively. The en-\\ncoder V AE [44] generates Gaussian distribution parameters\\n(meanµand varianceσ) for these patches, with a similar\\nprocess applied to text embeddings, i.e.,VAE(G) = (µ, σ).\\nFor each image patch or text embeddinggn, its noisez n is a\\nsamples n from the distributionN(µ, σ)with the addition\\nnoiseϵ n ∼ N(0,1), i.e,zn =s n +ϵ n. Finally, the image\\nXimg and textX txt are transformed into the noise sequence\\n⟨z1,···, z i⟩through the above process.\\nMoreover, three types of learnable padding tokens, time,\\ncategory, and pad, are inserted into these noise sequences,\\nas illustrated in Fig. 2de. The time token encodes the cur-\\nrent diffusion step, the class token is used to learn the data\\ncategory, and the pad token represents the start or end posi-\\ntion for splitting these noise sequences.\\n4.2. The multi-step selection diffusion decoder\\nThe decoder aims at progressively recovering the image\\nXimg or textX txt from noise sequences through two main\\nmodules: 1) the multi-step selection diffusion Mamba and\\n2) the V AE noisy latent decoder. 1) The Mamba is used\\nto recover the patch sequence⟨g 1,···, g i⟩from the noise\\nsequence⟨z 1,···, z i⟩. 2) The V AE noisy latent decoder as-\\nsembles patches and generates the image ˆXimg or text ˆXtxt.\\n4.2.1. Multi-step selection diffusion Mamba\\nThe module leverages two components, image/text scan\\nswitch and Mamba Block, to implement each denoising step\\nin the multi-step selection diffusion model (Sec. 3).\\nThe image/text scan switch componentestablishes se-\\nquences with different directions to capture different tempo-\\nral relationships between patches. Following Dim [81], we\\nimplement four distinct scan switches for images (as shown\\nin Fig. 2f) and two for text (as shown in Fig. 2g).\\nThe Mamba blockis used to select patches from these\\ndifferent scan switch sequences and denoise the input noise\\nzg\\nn,t. The block adopts the state space architecture from\\nMamba-2 [28]. According to Sec. 3.2, it iss θ, where\\nθ={H g\\nn,t, A, B, C, D,∆}represent the state space in the\\nblock. The block comprises six key components: 1) lin-\\near input and output projection layers, 2) convolution ker-\\nnel layer, 3) nonlinear activation layer, 4) state space model\\n(SSM), 5) skip connection layer, and 6) normalization layer.\\n1) The linear input projection layer reduces the dimen-\\nsionality of the latent space noise vector while simultane-\\nously applying initial state matricesA,B,Cto the linear\\nprojection of input dataz g\\nn,t. Additionally, the linear output\\nprojection layer represents the denoising step, which trans-\\nforms the selection noisez g\\nn,t intoz g\\nn,t−∆t and outputs it to\\nthe next Mamba block according to the following equation.\\nzg\\nn,t−∆t =z g\\nn,t−∆t\\n2 [fθ(zg\\nn,t, t)+fθ(zg\\nn,t−∆t, t−∆t)](9)\\nwhere the equation adopts the second-order numerical\\nmethod of DPM-Solver [54] to improve sampling accuracy.\\nDetails are provided in Appendix B.\\n2) The convolution kernel layer implements parallel scan\\nswitches, routing the initial linear projection of the input\\nand the state matrix’s linear projection through the SSM, as\\nshown in Fig. 2i. The sweep down and sweep up [28] enable\\nparallel computation between Eqs. (10) to (13).\\n3) The nonlinear layer enhances model generalization.\\n4) The SSM lets the Mamba blocks θ approximate the\\nactual score ratio based on Theorem 3. To implement the\\ntarget, SSM updates the state spaceθby the following equa-\\ntions (based on Theorem 3 and details in Appendix A).\\nHg\\nn,t = ¯AHg\\nn,t−1 + ¯Bzg\\nn,t (10)\\nzg\\nn−1,t =CH g\\nn,t +Dz g\\nn,t (11)\\n¯A= exp (∆A) (12)\\n¯B= (∆A) −1 ·(exp (∆A)−I)·∆B (13)\\nwhereH g\\nn,t represents the hidden state representation,A\\nandBcontrol the evolution of hidden states and latent space\\nnoise vector inputs, respectively,Cgoverns the hidden state\\nrepresentation of the target output andDmanages the non-\\nlinear skip connection for latent space noise vector inputs.\\n∆denotes the learnable time parameter.\\n5) The skip connection layer facilitates input feature\\nreuse and mitigates model degradation.\\n6) The Normalization layer ensures training stability.\\nAccording to Eq. (8) in Theorem 3 and Eq. (4), the goal\\nof training the Mamba block is:\\nLse =E zg\\nn,0∼p0,zg\\nn∼p(·|zg\\nn,0)se= 0 (14)\\n4.2.2. The noisy latent decoder\\nAfter applying the diffusion-based denoising process, the\\nrecovered latent variablez g\\nn,0 is passed to the V AE de-\\ncoder [44] as illustrated in Fig. 2c. For image reconstruc-\\ntion, the decoder applies anℓ 2 loss:\\nLimg\\nrec =E zg\\nn,0∼qϕ(z|X) ∥Ximg − ˆXimg∥2. (15)\\nwhereq ϕ(z|X)represents the posterior distribution of the\\nV AE encoder.\\nFor text, the decoder minimizes the cross-entropy loss:\\nLtxt\\nrec =−E zg\\nn,0∼qϕ(z|X)\\nX\\nt\\np(X(t)\\ntxt|zg\\nn,0) logpψ( ˆX(t)\\ntxt|zg\\nn,0).\\n(16)\\nwherep(X (t)\\ntxt|zg\\nn,0)represents the probability distribution\\nof real text data under the condition of latent variablez g\\nn,0.\\n5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='23d716d1-ed1a-4afc-8f31-967ed043cac3', embedding=None, metadata={'page_label': '6', 'file_name': '2510.13253v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.13253v1.pdf', 'file_type': 'application/pdf', 'file_size': 6563163, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"What is the man called who has a glove on his left hand?\\nVQA QA\\nThe man who has a glove on his left hand is catcher. \\nWhat colorare the boy's trunks?\\nThe color of the trunks is yellow.\\nHow would you efficiently remove dust from a keyboard without damaging it?\\nWhat is the capital city of the country where the Eiffel Tower is located?\\nUse compressed air to blow out dust or gently clean with a soft brush or microfiber cloth.\\nParis.\\nMulti-Modal Generation\\nWhat could the person be feeling in this moment?\\nThe person could be feeling relaxed, enjoying the calmness of the setting sun and the soothing sound of the waves.I want to see more beautiful sea view. Can you provide me one with a beautiful sea view picture?\\nSure.Here is another beautiful sea view.\\nA B C\\nFigure 3. VQA, QA and Multi-Modal generation test from MDM.\\nThe results of VQA are part of VQAv2 [27]. The QA results are\\npart of PIQA [7] and MMLU [34]. The Multi-Modal generation\\nresults are tested with ground-truth data.\\nA cup of water\\nA pick-up truck rolling over a grassy field\\nAn old-fashioned windmill surrounded by flowers\\nGPT-4\\n MonoFormer\\nMDM\\nLlamaGen\\nDREAMLLM\\nA cup of waterNullNull NullNull\\nNullNull NullNull\\nNullNull NullNullOriginal Captioning\\nImageGeneration\\nA windmill surrounded by flowers\\nA pickup truck driving on the grass\\nFigure 4. Comparison between each model on generating caption-\\ning and image results on COCO dataset. Unlike other models,\\nMDM generates both image and caption data simultaneously.\\nAndp ψ( ˆX(t)\\ntxt|zg\\nn,0)represents the probability distribution\\nof the text token generated by the V AE decoder under the\\ncondition of the latent variablez g\\nn,0.\\nBesides, a KL divergence regularizes the latent space:\\nLKL =D KL (qϕ(z|X)∥p(z)). (17)\\nwherep(z)represents the prior distribution of the latent\\nvariable by V AE, which is assumed to be a standard Gaus-\\nsian distributionN(0, I)to regularize the latent variable\\nspace and enable it to have smooth generation capabilities.\\nThe final optimization objective integrates V AE recon-\\nstruction, KL divergence and SE:\\nLtotal =L img\\nrec +L txt\\nrec +βL KL +λL se. (18)\\n5. Experiments\\n5.1. Experimental Setup\\nModel configuration.Our model applies a V AE [44] as the\\nnoisy latent encoder and decoder. Moreover, it integrates\\nthe DiM selection state space [81] in each Mamba block as\\nthe diffusion decoder. The resulting model contains 7 bil-\\nlion parameters, with 49 Mamba blocks in the multi-step se-\\nlection diffusion decoder, each having a dimension of 2048\\n(Details of parameter settings listed in Appendix C).\\nBefore the training MDM process, we trained a tokeniza-\\ntion model based on SentencePiece (Unigram BPE) [47].\\nThe tokenization model can help the model construct a sta-\\nble text latent variable representation, thereby optimizing\\nthe forward diffusion and reverse denoising process. See\\nAppendix D for detailed experimental settings.\\nIn the training process, we import the DDPM sched-\\nuler [37] and DPM-Solver [54] to improve the sampling ef-\\nficiency in the diffusion model. We then use the AdamW\\noptimizer without weight decay, maintaining a constant\\nlearning rate of 0.0001. Meanwhile, we keep an EMA of\\nthe model weights with a coefficient of 0.9999.\\nBaseline and dataset.Our evaluation encompasses four\\ntasks: image generation with classifier-free guidance [36]\\n(CFG), text-to-image, image-to-text, and text-to-text gener-\\nation. For the baseline model training, we train MDM on\\nImageNet [15], JourneyDB [76] and UltraChat [18].\\nFor the image generation and the text-to-image task\\nat256×256resolution, we compare the MDM baseline\\nmodel against established baselines across three categories:\\ndiffusion models (Imagen [72], ADM [17], CDM [38],\\nLDM [71], DiT-XL/2 [61], SDXL [62], and SD-3 [23]), au-\\ntoregressive models (VQGAN [22] and ViT-VQGAN [95]),\\nand end-to-end multi-modal models (NExT-GPT [88],\\nChameleon [79], LlamaGen [77], Transfusion [102], Mono-\\nFormer [101], Dual-DiT [50], JanusFlow [56] and Show-\\nO [89]). For the image generation task, we evaluate per-\\nformance on ImageNet [15] using four metrics: Frechet\\nInception Distance (FID), Inception Score (IS), and Preci-\\nsion/Recall. For the text-to-image task, we evaluate perfor-\\nmance on COCO [42] using FID and Gen Eval [26].\\nFor the image-to-text task (image captioning and vision\\nquestion answering, VQA) and text-to-text task, we em-\\nploy MDM baseline model and MDM instruction model\\nby visual instruction tuning [51] on multiple datasets:\\nCOCO [42], GQA [40], OCR-VQA [59], TextVQA [75],\\nand VisualGenome [46]. We evaluate the model against\\ntwo groups of baselines: traditional models and end-to-end\\nmulti-modal models. Performance evaluation of image cap-\\ntioning is conducted on Flickr 30K [94] and COCO [42]\\ndatasets using the Consensus-based Image Description\\nEvaluation (CIDEr) metric. And performance evaluation\\nof VQA is conducted on VQAv2 [27], VizWiz [30], and\\nOKVQA [57] using answer accuracy rate as the evaluation\\nmetric.\\nFor the text-to-text task, we evaluate the model on text\\ncomprehension and reasoning tasks using HellaSwag [99],\\nOpenBookQA [58], Wino-Grande [73], ARCEasy, ARC-\\n6\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9a5f0e88-841c-4689-b52c-56918f0aeb74', embedding=None, metadata={'page_label': '7', 'file_name': '2510.13253v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.13253v1.pdf', 'file_type': 'application/pdf', 'file_size': 6563163, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Model Arc Params Image Generation with CFG Text-to-Image Generation\\nFID↓IS↑Pre↑Re↑ FID↓Gen Eval↑\\nImagen [72] Diff 7.3B - - - - 7.27 -\\nADM [17] Diff 554M 10.94 101.0 0.69 0.63 - -\\nCDM [38] Diff - 4.88 158.7 - - - -\\nLDM [71] Diff 400M 3.60 147.6 0.87 0.68 - 0.43\\nDiT-XL/2 [61] Diff 675M 2.27 278.2 0.83 0.57 - -\\nSDXL [62] Diff 3.4B - - - - 4.40 0.55\\nSD-3 [23] Diff 12.7B - - - - - 0.68\\nVQGAN [22] AR 227M 18.65 80.4 0.78 0.26 - -\\nViT-VQGAN [95] AR 1.7B 4.17 175.1 - - - -\\nNExT-GPT [88] AR 7B - - - - 10.07 -\\nChameleon [79] AR 7B - - - - 26.74 0.39\\nLlamaGen [77] AR 3.1B 2.81 311.5 0.84 0.54 4.19 -\\nTransfusion [102] AR+Diff 7.3B - - - - 6.78 0.63\\nMonoFormer [101] AR+Diff 1.1B 2.57 272.6 0.84 0.56 - -\\nDual-DiT [50] Diff 2B - - - - 9.40 0.65\\nJanusFlow [56] AR+Diff 1.3B - - - - - 0.70\\nShow-O [89] AR+Diff 1.3B - - - - 9.24 0.68\\nMDMDiff 7B 2.49 281.4 0.86 0.59 5.91 0.68\\nTable 1. Performance on ImageNet and COCO 256×256. FID, IS, Pre, and Re stands for Frechet Inception Distance, Inception Score,\\nPrecision, and Recall, respectively.\\nModel IC VQA Text Comprehension and Reasoning Math and World\\nFlickr COCO VQAv2 VizWiz OK HS OBQA WG ARCE ARCC BoolQ PIQA GSM8k MATH MMLU\\nLlama-2 [82] (7B) - - - - - 77.2 58.6 78.5 75.2 45.9 77.4 78.8 14.6 2.5 45.3\\nMistral [41] (7B) - - - - - 81.3 - 75.3 80.0 55.5 84.7 83.0 52.1 13.1 60.1\\nFlamingo [2] (80B) 75.1 113.8 67.6 - - - - - - - - - - - -\\nGemini Pro [80] 82.2 99.8 71.2 - - 84.7 - - - - - - 86.5 32.6 71.8\\nGPT4V [8] 55.3 78.5 77.2 - - 95.3 - - - - - - 92.0 52.9 86.4\\nInstructBLIP [51] (7B) 82.4 102.2 - 33.4 33.9 - - - - - - - - - -\\nmPLUG-Owl [93] (7B) 80.3 119.3 - 39.0 - - - - - - - - - - -\\nTinyLlama [100] (1.1B) - - - - - 59.2 36.0 59.1 55.3 30.1 57.8 73.3 - - -\\nPythia [6] (12B) - - - - - 52.0 33.2 57.4 54.0 28.5 63.3 70.9 - - -\\nDREAMLLM [19](7B) - 115.4 56.6 45.8 44.3 - - - - - - - - - -\\nEmu [78](7B) - 117.7 40.0 35.4 34.7 - - - - - - - - - -\\nChameleon [79](34B) 74.7 120.2 66.0 - - 74.2 51.0 70.4 76.1 46.5 81.4 79.6 41.6 11.5 52.1\\nNExT-GPT [88](7B) 84.5 124.9 66.7 48.4 52.1 - - - - - - - - - -\\nTransfusion [102](7B) - 33.7 - - - - - - - - - - - - -\\nMonoFormer [101](1.1B) - - - - - 50.6 37.2 56.9 48.2 31.5 62.3 71.2 - - -\\nDual-DiT [50](2B) - 56.2 60.1 29.9 25.3 - - - - - - - - - -\\nJanusFlow [56](1.3B) - - 79.8 - - - - - - - - - - - -\\nShow-O [89](1.3B) 67.6 - 74.7 - - - - - - - - - - - -\\nMDM(7B) 62.4 109.6 60.3 39.8 47.1 70.6 41.5 68.8 55.1 46.2 65.7 79.9 40.5 12.1 54.4\\nInstructMDM(7B) 75.2 122.1 66.7 46.3 51.6 74.8 48.3 74.9 65.4 47.1 71.5 83.7 46.0 13.1 59.2\\nTable 2. Performance on image-to-text and text-to-text tasks. The evaluation of image captioning (IC) and VQA is CIDEr and answer\\naccuracy%(Flickr is evaluated on 30K and OK represents OKVQA).\\nChallenge [12], BoolQ [11], and PIQA [7]. We also eval-\\nuate the model on math and world knowledge tasks using\\nGSM8K [13], MATH [35], and MMLU [34]. The evalua-\\ntion metrics for all the tasks are accuracy rates.\\n5.2. Experimental Results\\nImage Generation.In the image generation task on Im-\\nageNet, MDM achieves top-three rankings across all eval-\\nuation metrics: second in FID, IS, and Precision, and third\\nin Recall when compared against one-modal diffusion mod-\\nels and end-to-end multi-modal models (see Tab. 1). MDM\\ndemonstrates superior overall performance, notably sur-\\npassing other end-to-end multi-modal models in three of\\nthe four metrics. In the text-to-image task, we tested the\\nmodel on the COCO dataset to generate both image and\\n7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6ba88c09-8420-4c8e-8c09-73c1e62ef8bd', embedding=None, metadata={'page_label': '8', 'file_name': '2510.13253v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.13253v1.pdf', 'file_type': 'application/pdf', 'file_size': 6563163, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5FPS (log scale)\\n256 5121024Resolution128015362048\\n0.1\\n0.50.6\\n0.0\\n0.2\\n0.40.3Latency (Seconds)128 256 512TokenLength102412801536\\nFPS Comparison Across Different ResolutionsLatency Comparison Across Different Token Length\\nFaster Faster\\nFigure 5. Comparison between Mamba Baseline, MonoFormer, and ours MDM on inference speed test. The left shows the inference speed\\nof the model FPS at different resolutions. The right shows the inference speed of the model latency at different token lengths.\\nModel Image/Text\\nScan Switch\\nFPS w log\\nscale↑\\nFID↓\\nModel w Mamba ①②③④/①② 1.357 2.49\\nModel w Mamba ①②/① 1.405 3.96\\nModel w Transformer - 1.914 6.72\\nTable 3. Ablation on ImageNet 256×256 image generation.\\ncaption data. For the image generation results, we evalu-\\nated the FID and Gen Eval performance indicators of the\\nmodel-generated images. MDM still achieved the top three\\nperformance levels and achieved SOTA on Gen Eval.\\nText Generation.In the image-to-text task image cap-\\ntioning, according to the settings on image generation on\\nthe COCO dataset, we tested the caption data of the model\\nbased on the model outputting both text and image data us-\\ning the CIDEr indicator. The results showed that MDM\\nranked second among all models, as shown in Tab. 2. While\\nin task VQA, MDM achieves competitive performance, sur-\\npassing several traditional models including InstructBLIP,\\nmPLUG-Owl, DREAMLLM, and Emu, although it still\\ntrails behind top-performing models in the field as shown\\nin Tab. 2. In the text-to-text generation task, as shown\\nin Tab. 2, MDM and the other end-to-end multi-modal mod-\\nels perform worse than well-known traditional models. This\\ndiscrepancy may be attributed to the fact that these end-to-\\nend models have some deviations in multimodal fusion and\\nlearning because they abandon multiple language encoders,\\nvisual encoders, and multimodal fusion encoders. How-\\never, when compared with the other two end-to-end models,\\nMDM excels, outperforming MonoFormer and surpassing\\nChameleon on seven out of ten datasets.\\n5.3. Discussion\\n5.3.1. Performance Analysis\\nAs demonstrated in Fig. 3, MDM shows the ability to gen-\\nerate image and text simultaneously in multiple rounds of\\ndialogue and perform well in QA&VQA. Some results even\\nexceed those of GPT-4V , particularly evident in the second\\nand third rows of Fig. 4 which is a hybrid output process for\\nthe MDM model. Due to this, we set the model to generate\\ncorresponding images for the description text while simul-\\ntaneously generating image captioning.\\nThis enhanced performance stems from MDM’s multi-\\nstep selection diffusion decoder, which leverages Mamba’s\\nintegrated selection and denoising capabilities to maintain\\nfocused attention on both textual and visual details. Validat-\\ning our complexity analysis in Appendix E, MDM demon-\\nstrates superior efficiency compared to end-to-end Trans-\\nformer models when processing long sequences, as shown\\nin Fig. 5, particularly outperforming other end-to-end multi-\\nmodal models for sequences exceeding 1280 tokens.\\n5.3.2. Ablations\\nOur ablation studies examine the impact of both the se-\\nlection process and Mamba block components. Reducing\\nthe number of image/text scan switch sequences from 6\\n(’①②③④/①②’) to 3 (’①②/①’), as shown in Tab. 3, im-\\nproves inference speed but degrades image quality, as fewer\\nscan switch sequences limit the model’s ability to capture\\naccurate information in complex sequences. Additionally,\\nreplacing the Mamba block with the Transformer further\\ndeteriorates output image quality, suggesting Mamba’s tem-\\nporal network architecture is better suited for representing\\ndiffusion relationships during the denoising process.\\n6. Conclusion\\nThis paper introduces MDM (Multi-Modal Diffusion\\nMamba), a novel end-to-end architecture that significantly\\nenhances multi-modal processing through two key innova-\\ntions: a unified diffusion objective and an efficient selection\\nmechanism leveraging Mamba’s state-space structure.\\nBy integrating variational autoencoder with multi-step\\nselection diffusion, MDM achieves SOTA overall perfor-\\nmance in image generation and demonstrates remarkable\\nversatility across various tasks, including image-to-text,\\ntext-to-text and text-image-to-text-image. Our compre-\\nhensive experiments illustrate that MDM consistently\\nsurpasses traditional end-to-end multi-modal models,\\nparticularly in processing high-resolution images and long-\\nsequence text, while maintaining computational efficiency.\\nThe model’s ability to unify different modalities under a\\nsingle objective, coupled with its superior management of\\ntemporal relationships in the diffusion process, establishes\\na promising direction for future multi-modal architecture.\\n8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7bb173a0-130b-4ed1-a02a-0ff5cbdbd1ad', embedding=None, metadata={'page_label': '9', 'file_name': '2510.13253v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.13253v1.pdf', 'file_type': 'application/pdf', 'file_size': 6563163, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References\\n[1] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury\\nZemlyanskiy, Federico Lebr ´on, and Sumit Sanghai. Gqa:\\nTraining generalized multi-query transformer models from\\nmulti-head checkpoints.arXiv preprint arXiv:2305.13245,\\n2023. 1, 7\\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\\nMensch, Katherine Millican, Malcolm Reynolds, et al.\\nFlamingo: a visual language model for few-shot learning.\\nAdvances in neural information processing systems, 35:\\n23716–23736, 2022. 1, 3, 7\\n[3] Silas Alberti, Niclas Dern, Laura Thesing, and Gitta Ku-\\ntyniok. Sumformer: Universal approximation for efficient\\ntransformers. InTopological, Algebraic and Geometric\\nLearning Workshops 2023, pages 72–86. PMLR, 2023. 1\\n[4] Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu,\\nOwais Khan Mohammed, Kriti Aggarwal, Subhojit Som,\\nSonghao Piao, and Furu Wei. Vlmo: Unified vision-\\nlanguage pre-training with mixture-of-modality-experts.\\nAdvances in Neural Information Processing Systems, 35:\\n32897–32912, 2022. 1\\n[5] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell\\nNye, Augustus Odena, Arushi Somani, and Sa˘gnak Tas ¸ırlar.\\nIntroducing our multimodal models, 2023. 1, 3\\n[6] Stella Biderman, Hailey Schoelkopf, Quentin Gregory An-\\nthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-\\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai\\nPrashanth, Edward Raff, et al. Pythia: A suite for ana-\\nlyzing large language models across training and scaling.\\nInInternational Conference on Machine Learning, pages\\n2397–2430. PMLR, 2023. 7\\n[7] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\\net al. Piqa: Reasoning about physical commonsense in nat-\\nural language. InProceedings of the AAAI conference on\\nartificial intelligence, pages 7432–7439, 2020. 2, 6, 7\\n[8] GPTV System Card. Openai, 2023. 7\\n[9] Rui Chen, Lei Sun, Jing Tang, Geng Li, and Xiangxi-\\nang Chu. Finger: Content aware fine-grained evaluation\\nwith reasoning for ai-generated videos.arXiv preprint\\narXiv:2504.10358, 2025. 3\\n[10] Xiangxiang Chu, Renda Li, and Yong Wang. Usp: Unified\\nself-supervised pretraining for image generation and under-\\nstanding.arXiv preprint arXiv:2503.06132, 2025. 1\\n[11] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom\\nKwiatkowski, Michael Collins, and Kristina Toutanova.\\nBoolq: Exploring the surprising difficulty of natural yes/no\\nquestions.arXiv preprint arXiv:1905.10044, 2019. 2, 7\\n[12] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\\nAshish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.\\nThink you have solved question answering? try arc, the\\nai2 reasoning challenge.arXiv preprint arXiv:1803.05457,\\n2018. 2, 7\\n[13] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark\\nChen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry\\nTworek, Jacob Hilton, Reiichiro Nakano, et al. Train-\\ning verifiers to solve math word problems.arXiv preprint\\narXiv:2110.14168, 2021. 2, 7\\n[14] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christo-\\npher R ´e. Flashattention: Fast and memory-efficient exact\\nattention with io-awareness.Advances in Neural Informa-\\ntion Processing Systems, 35:16344–16359, 2022. 1\\n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\\nand Li Fei-Fei. Imagenet: A large-scale hierarchical im-\\nage database. In2009 IEEE conference on computer vision\\nand pattern recognition, pages 248–255. Ieee, 2009. 2, 6, 7\\n[16] Jacob Devlin. Bert: Pre-training of deep bidirectional\\ntransformers for language understanding.arXiv preprint\\narXiv:1810.04805, 2018. 3\\n[17] Prafulla Dhariwal and Alexander Nichol. Diffusion models\\nbeat gans on image synthesis.Advances in neural informa-\\ntion processing systems, 34:8780–8794, 2021. 6, 7\\n[18] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng,\\nShengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen\\nZhou. Enhancing chat language models by scaling\\nhigh-quality instructional conversations.arXiv preprint\\narXiv:2305.14233, 2023. 6\\n[19] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng\\nGe, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou,\\nHaoran Wei, et al. Dreamllm: Synergistic multimodal com-\\nprehension and creation.arXiv preprint arXiv:2309.11499,\\n2023. 3, 7\\n[20] Wenhao Dong, Haodong Zhu, Shaohui Lin, Xiaoyan Luo,\\nYunhang Shen, Xuhui Liu, Juan Zhang, Guodong Guo, and\\nBaochang Zhang. Fusion-mamba for cross-modality object\\ndetection.arXiv preprint arXiv:2404.09146, 2024. 1, 3\\n[21] Alexey Dosovitskiy. An image is worth 16x16 words:\\nTransformers for image recognition at scale.arXiv preprint\\narXiv:2010.11929, 2020. 1, 4\\n[22] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\\ntransformers for high-resolution image synthesis. InPro-\\nceedings of the IEEE/CVF conference on computer vision\\nand pattern recognition, pages 12873–12883, 2021. 6, 7\\n[23] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\\nEntezari, Jonas M ¨uller, Harry Saini, Yam Levi, Dominik\\nLorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified\\nflow transformers for high-resolution image synthesis. In\\nForty-first International Conference on Machine Learning,\\n2024. 3, 6, 7\\n[24] Zhengcong Fei, Mingyuan Fan, Changqian Yu, De-\\nbang Li, Youqiang Zhang, and Junshi Huang. Dimba:\\nTransformer-mamba diffusion models.arXiv preprint\\narXiv:2406.01159, 2024. 1\\n[25] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi,\\nLin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x:\\nMultimodal models with unified multi-granularity compre-\\nhension and generation.arXiv preprint arXiv:2404.14396,\\n2024. 1, 3\\n[26] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt.\\nGeneval: An object-focused framework for evaluating text-\\nto-image alignment.Advances in Neural Information Pro-\\ncessing Systems, 36:52132–52152, 2023. 6\\n9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2c0afb53-f41a-4674-9ef8-da4f2e3bb2ed', embedding=None, metadata={'page_label': '10', 'file_name': '2510.13253v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.13253v1.pdf', 'file_type': 'application/pdf', 'file_size': 6563163, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[27] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\\nBatra, and Devi Parikh. Making the v in vqa matter: El-\\nevating the role of image understanding in visual ques-\\ntion answering. InProceedings of the IEEE conference on\\ncomputer vision and pattern recognition, pages 6904–6913,\\n2017. 2, 6\\n[28] Albert Gu and Tri Dao. Mamba: Linear-time sequence\\nmodeling with selective state spaces.arXiv preprint\\narXiv:2312.00752, 2023. 1, 2, 5, 3\\n[29] Ahan Gupta, Yueming Yuan, Yanqi Zhou, and Charith\\nMendis. Flurka: Fast fused low-rank & kernel attention.\\narXiv preprint arXiv:2306.15799, 2023. 1\\n[30] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi\\nLin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.\\nVizwiz grand challenge: Answering visual questions from\\nblind people. InProceedings of the IEEE conference on\\ncomputer vision and pattern recognition, pages 3608–3617,\\n2018. 2, 6\\n[31] Insu Han, Rajesh Jayaram, Amin Karbasi, Vahab Mirrokni,\\nDavid P Woodruff, and Amir Zandieh. Hyperattention:\\nLong-context attention in near-linear time.arXiv preprint\\narXiv:2310.05869, 2023. 1\\n[32] Ali Hatamizadeh and Jan Kautz. Mambavision: A hy-\\nbrid mamba-transformer vision backbone.arXiv preprint\\narXiv:2407.08083, 2024. 1\\n[33] Wanggui He, Siming Fu, Mushui Liu, Xierui Wang, Wenyi\\nXiao, Fangxun Shu, Yi Wang, Lei Zhang, Zhelun Yu,\\nHaoyuan Li, et al. Mars: Mixture of auto-regressive mod-\\nels for fine-grained text-to-image synthesis.arXiv preprint\\narXiv:2407.07614, 2024. 1, 3\\n[34] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\\nMantas Mazeika, Dawn Song, and Jacob Steinhardt. Mea-\\nsuring massive multitask language understanding.arXiv\\npreprint arXiv:2009.03300, 2020. 2, 6, 7\\n[35] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul\\nArora, Steven Basart, Eric Tang, Dawn Song, and Jacob\\nSteinhardt. Measuring mathematical problem solving with\\nthe math dataset.arXiv preprint arXiv:2103.03874, 2021.\\n2, 7\\n[36] Jonathan Ho and Tim Salimans. Classifier-free diffusion\\nguidance.arXiv preprint arXiv:2207.12598, 2022. 6\\n[37] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\\nfusion probabilistic models.Advances in neural informa-\\ntion processing systems, 33:6840–6851, 2020. 2, 3, 6\\n[38] Jonathan Ho, Chitwan Saharia, William Chan, David J\\nFleet, Mohammad Norouzi, and Tim Salimans. Cascaded\\ndiffusion models for high fidelity image generation.Jour-\\nnal of Machine Learning Research, 23(47):1–33, 2022. 6,\\n7\\n[39] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga\\nGrebenkova, Pingchuan Ma, Johannes S Fischer, and Bj¨orn\\nOmmer. Zigma: A dit-style zigzag mamba diffusion model.\\narXiv preprint arXiv:2403.13802, 2024. 1\\n[40] Drew A Hudson and Christopher D Manning. Gqa: A new\\ndataset for real-world visual reasoning and compositional\\nquestion answering. InProceedings of the IEEE/CVF con-\\nference on computer vision and pattern recognition, pages\\n6700–6709, 2019. 6\\n[41] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch,\\nChris Bamford, Devendra Singh Chaplot, Diego de las\\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lam-\\nple, Lucile Saulnier, et al. Mistral 7b.arXiv preprint\\narXiv:2310.06825, 2023. 7\\n[42] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic\\nalignments for generating image descriptions. InProceed-\\nings of the IEEE conference on computer vision and pattern\\nrecognition, pages 3128–3137, 2015. 2, 6, 7\\n[43] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-\\nand-language transformer without convolution or region su-\\npervision. InInternational conference on machine learning,\\npages 5583–5594. PMLR, 2021. 1\\n[44] Diederik P Kingma. Auto-encoding variational bayes.\\narXiv preprint arXiv:1312.6114, 2013. 1, 3, 4, 5, 6\\n[45] Jing Yu Koh, Daniel Fried, and Russ R Salakhutdinov. Gen-\\nerating images with multimodal language models.Ad-\\nvances in Neural Information Processing Systems, 36,\\n2024. 3\\n[46] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\\ntidis, Li-Jia Li, David A Shamma, et al. Visual genome:\\nConnecting language and vision using crowdsourced dense\\nimage annotations.International journal of computer vi-\\nsion, 123:32–73, 2017. 6\\n[47] Taku Kudo and John Richardson. Sentencepiece: A\\nsimple and language independent subword tokenizer and\\ndetokenizer for neural text processing.arXiv preprint\\narXiv:1808.06226, 2018. 4, 6\\n[48] Rui Lan, Yancheng Bai, Xu Duan, Mingxing Li, Dongyang\\nJin, Ryan Xu, Lei Sun, and Xiangxiang Chu. Flux-text:\\nA simple and advanced diffusion transformer baseline for\\nscene text editing.arXiv preprint arXiv:2505.03329, 2025.\\n3\\n[49] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\\nBlip: Bootstrapping language-image pre-training for uni-\\nfied vision-language understanding and generation. InIn-\\nternational conference on machine learning, pages 12888–\\n12900. PMLR, 2022. 1\\n[50] Zijie Li, Henry Li, Yichun Shi, Amir Barati Farimani, Yuval\\nKluger, Linjie Yang, and Peng Wang. Dual diffusion for\\nunified image generation and understanding.arXiv preprint\\narXiv:2501.00289, 2024. 6, 7\\n[51] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\\nLee. Visual instruction tuning.Advances in neural infor-\\nmation processing systems, 36, 2024. 3, 6, 7\\n[52] Jiaming Liu, Mengzhen Liu, Zhenyu Wang, Lily Lee,\\nKaichen Zhou, Pengju An, Senqiao Yang, Renrui Zhang,\\nYandong Guo, and Shanghang Zhang. Robomamba: Mul-\\ntimodal state space model for efficient robot reasoning and\\nmanipulation.arXiv preprint arXiv:2406.04339, 2024. 1, 3\\n[53] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete\\ndiffusion modeling by estimating the ratios of the data dis-\\ntribution. InForty-first International Conference on Ma-\\nchine Learning, 2024. 2, 3, 4, 1, 5\\n[54] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongx-\\nuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for\\n10', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='478bdd9e-5179-4c6c-9850-7d4c24969e81', embedding=None, metadata={'page_label': '11', 'file_name': '2510.13253v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.13253v1.pdf', 'file_type': 'application/pdf', 'file_size': 6563163, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='diffusion probabilistic model sampling in around 10 steps.\\nAdvances in Neural Information Processing Systems, 35:\\n5775–5787, 2022. 5, 6\\n[55] Chunhao Lu, Qiang Lu, and Jake Luo. An explainable vi-\\nsion question answer model via diffusion chain-of-thought.\\nInEuropean Conference on Computer Vision, pages 146–\\n162. Springer, 2024. 1\\n[56] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu,\\nChengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie,\\nHaowei Zhang, Liang Zhao, et al. Janusflow: Harmo-\\nnizing autoregression and rectified flow for unified mul-\\ntimodal understanding and generation.arXiv preprint\\narXiv:2411.07975, 2024. 6, 7\\n[57] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\\nbenchmark requiring external knowledge. InProceedings\\nof the IEEE/cvf conference on computer vision and pattern\\nrecognition, pages 3195–3204, 2019. 2, 6\\n[58] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sab-\\nharwal. Can a suit of armor conduct electricity? a new\\ndataset for open book question answering.arXiv preprint\\narXiv:1809.02789, 2018. 2, 6\\n[59] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and\\nAnirban Chakraborty. Ocr-vqa: Visual question answering\\nby reading text in images. InICDAR, 2019. 6\\n[60] Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, and\\nFranc ¸ois Fleuret. Fast attention over long sequences with\\ndynamic sparse flash attention.Advances in Neural Infor-\\nmation Processing Systems, 36:59808–59831, 2023. 1\\n[61] William Peebles and Saining Xie. Scalable diffusion mod-\\nels with transformers. InProceedings of the IEEE/CVF In-\\nternational Conference on Computer Vision, pages 4195–\\n4205, 2023. 6, 7\\n[62] Dustin Podell, Zion English, Kyle Lacey, Andreas\\nBlattmann, Tim Dockhorn, Jonas M ¨uller, Joe Penna, and\\nRobin Rombach. Sdxl: Improving latent diffusion mod-\\nels for high-resolution image synthesis.arXiv preprint\\narXiv:2307.01952, 2023. 3, 6, 7\\n[63] Ramya Prabhu, Ajay Nayak, Jayashree Mohan, Ramachan-\\ndran Ramjee, and Ashish Panwar. vattention: Dynamic\\nmemory management for serving llms without pagedatten-\\ntion.arXiv preprint arXiv:2405.04437, 2024. 1\\n[64] Chengxuan Qian, Kai Han, Jingchao Wang, Zhenlong\\nYuan, Chongwen Lyu, Jun Chen, and Zhe Liu. Dyncim:\\nDynamic curriculum for imbalanced multimodal learning.\\narXiv preprint arXiv:2503.06456, 2025. 1\\n[65] Yanyuan Qiao, Zheng Yu, Longteng Guo, Sihan Chen, Zijia\\nZhao, Mingzhen Sun, Qi Wu, and Jing Liu. Vl-mamba: Ex-\\nploring state space models for multimodal learning.arXiv\\npreprint arXiv:2403.13600, 2024. 1, 3\\n[66] Haohao Qu, Liangbo Ning, Rui An, Wenqi Fan, Tyler Derr,\\nXin Xu, and Qing Li. A survey of mamba.arXiv preprint\\narXiv:2408.01129, 2024. 1, 7\\n[67] Xiangyan Qu, Jing Yu, Keke Gai, Jiamin Zhuang, Yuan-\\nmin Tang, Gang Xiong, Gaopeng Gou, and Qi Wu.\\nVisual-semantic decomposition and partial alignment for\\ndocument-based zero-shot learning. InProceedings of the\\n32nd ACM International Conference on Multimedia, MM\\n2024, Melbourne, VIC, Australia, 28 October 2024 - 1\\nNovember 2024, pages 4581–4590. ACM, 2024. 1\\n[68] Xiangyan Qu, Gaopeng Gou, Jiamin Zhuang, Jing Yu, Kun\\nSong, Qihao Wang, Yili Li, and Gang Xiong. Proapo: Pro-\\ngressively automatic prompt optimization for visual classi-\\nfication. InIEEE/CVF Conference on Computer Vision and\\nPattern Recognition, CVPR 2025, Nashville, TN, USA, June\\n11-15, 2025, pages 25145–25155, 2025.\\n[69] Xiangyan Qu, Jing Yu, Jiamin Zhuang, Gaopeng Gou,\\nGang Xiong, and Qi Wu. MADS: multi-attribute docu-\\nment supervision for zero-shot image classification.CoRR,\\nabs/2503.06847, 2025.\\n[70] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\\ning transferable visual models from natural language super-\\nvision. InInternational conference on machine learning,\\npages 8748–8763. PMLR, 2021. 1\\n[71] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\\nPatrick Esser, and Bj ¨orn Ommer. High-resolution image\\nsynthesis with latent diffusion models. InProceedings of\\nthe IEEE/CVF conference on computer vision and pattern\\nrecognition, pages 10684–10695, 2022. 6, 7\\n[72] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Sali-\\nmans, et al. Photorealistic text-to-image diffusion models\\nwith deep language understanding.Advances in neural in-\\nformation processing systems, 35:36479–36494, 2022. 3,\\n6, 7\\n[73] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,\\nand Yejin Choi. Winogrande: An adversarial winograd\\nschema challenge at scale.Communications of the ACM,\\n64(9):99–106, 2021. 2, 6\\n[74] Noam Shazeer. Fast transformer decoding: One write-head\\nis all you need.arXiv preprint arXiv:1911.02150, 2019. 1\\n[75] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\\nRohrbach. Towards vqa models that can read. InProceed-\\nings of the IEEE/CVF conference on computer vision and\\npattern recognition, pages 8317–8326, 2019. 6\\n[76] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong\\nDuan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng\\nQin, Yi Wang, et al. Journeydb: A benchmark for genera-\\ntive image understanding.Advances in Neural Information\\nProcessing Systems, 36, 2024. 6\\n[77] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue\\nPeng, Ping Luo, and Zehuan Yuan. Autoregressive model\\nbeats diffusion: Llama for scalable image generation.arXiv\\npreprint arXiv:2406.06525, 2024. 1, 3, 6, 7\\n[78] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong\\nZhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\\nHuang, and Xinlong Wang. Generative pretraining in mul-\\ntimodality.arXiv preprint arXiv:2307.05222, 2023. 7\\n[79] Chameleon Team. Chameleon: Mixed-modal early-fusion\\nfoundation models.arXiv preprint arXiv:2405.09818,\\n2024. 1, 3, 6, 7\\n11', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d8a36f82-a241-46f4-9b43-7dc5e3b1dce6', embedding=None, metadata={'page_label': '12', 'file_name': '2510.13253v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.13253v1.pdf', 'file_type': 'application/pdf', 'file_size': 6563163, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[80] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-\\nBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalk-\\nwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al.\\nGemini: a family of highly capable multimodal models.\\narXiv preprint arXiv:2312.11805, 2023. 7\\n[81] Yao Teng, Yue Wu, Han Shi, Xuefei Ning, Guohao Dai, Yu\\nWang, Zhenguo Li, and Xihui Liu. Dim: Diffusion mamba\\nfor efficient high-resolution image synthesis.arXiv preprint\\narXiv:2405.14224, 2024. 1, 2, 5, 6\\n[82] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\\nLlama 2: Open foundation and fine-tuned chat models.\\narXiv preprint arXiv:2307.09288, 2023. 1, 7\\n[83] Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu\\nZheng, Jiachen Liu, Zhongnan Qu, Shen Yan, Yi Zhu,\\nQuanlu Zhang, et al. Efficient large language models: A\\nsurvey.arXiv preprint arXiv:2312.03863, 2023. 1\\n[84] Zifu Wan, Pingping Zhang, Yuhao Wang, Silong Yong,\\nSimon Stepputtis, Katia Sycara, and Yaqi Xie. Sigma:\\nSiamese mamba network for multi-modal semantic seg-\\nmentation.arXiv preprint arXiv:2404.04256, 2024. 1, 3\\n[85] Jiyuan Wang, Chunyu Lin, Cheng Guan, Lang Nie, Jing\\nHe, Haodong Li, Kang Liao, and Yao Zhao. Jasmine: Har-\\nnessing diffusion prior for self-supervised depth estimation.\\narXiv preprint arXiv:2503.15905, 2025. 3\\n[86] JiYuan Wang, Chunyu Lin, Lei Sun, Rongying Liu, Lang\\nNie, Mingxing Li, Kang Liao, Xiangxiang Chu, and Yao\\nZhao. From editor to dense geometry estimator.arXiv\\npreprint arXiv:2509.04338, 2025. 1\\n[87] Xinghan Wang, Zixi Kang, and Yadong Mu. Text-\\ncontrolled motion mamba: Text-instructed tempo-\\nral grounding of human motion.arXiv preprint\\narXiv:2404.11375, 2024. 3\\n[88] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-\\nSeng Chua. Next-gpt: Any-to-any multimodal llm.arXiv\\npreprint arXiv:2309.05519, 2023. 1, 3, 6, 7\\n[89] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao\\nZhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu,\\nZhijie Chen, Zhenheng Yang, and Mike Zheng Shou.\\nShow-o: One single transformer to unify multimodal under-\\nstanding and generation.arXiv preprint arXiv:2408.12528,\\n2024. 6, 7\\n[90] Jing Nathan Yan, Jiatao Gu, and Alexander M Rush. Dif-\\nfusion models without attention. InProceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 8239–8249, 2024. 1\\n[91] Xu Yang, Hanwang Zhang, Guojun Qi, and Jianfei Cai.\\nCausal attention for vision-language tasks. InProceedings\\nof the IEEE/CVF conference on computer vision and pat-\\ntern recognition, pages 9847–9857, 2021. 3\\n[92] Zhe Yang, Wenrui Li, and Guanghui Cheng. Shmamba:\\nStructured hyperbolic state space model for audio-visual\\nquestion answering.arXiv preprint arXiv:2406.09833,\\n2024. 1, 3\\n[93] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\\nYaya Shi, et al. mplug-owl: Modularization empowers\\nlarge language models with multimodality.arXiv preprint\\narXiv:2304.14178, 2023. 7\\n[94] Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-\\nenmaier. From image descriptions to visual denotations:\\nNew similarity metrics for semantic inference over event\\ndescriptions.Transactions of the Association for Computa-\\ntional Linguistics, 2:67–78, 2014. 2, 6, 7\\n[95] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang,\\nJames Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge,\\nand Yonghui Wu. Vector-quantized image modeling with\\nimproved vqgan.arXiv preprint arXiv:2110.04627, 2021.\\n6, 7\\n[96] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung,\\nMojtaba Seyedhosseini, and Yonghui Wu. Coca: Con-\\ntrastive captioners are image-text foundation models.arXiv\\npreprint arXiv:2205.01917, 2022. 1\\n[97] Zhenlong Yuan, Xiangyan Qu, Chengxuan Qian, Rui Chen,\\nJing Tang, Lei Sun, Xiangxiang Chu, Dapeng Zhang, Yi-\\nwei Wang, Yujun Cai, et al. Video-star: Reinforcing open-\\nvocabulary action recognition with tools.arXiv preprint\\narXiv:2510.08480, 2025.\\n[98] Zhenlong Yuan, Jing Tang, Jinguo Luo, Rui Chen, Chengx-\\nuan Qian, Lei Sun, Xiangxiang Chu, Yujun Cai, Dapeng\\nZhang, and Shuo Li. Autodrive-r2: Incentivizing reasoning\\nand self-reflection capacity for vla model in autonomous\\ndriving.arXiv preprint arXiv:2509.01944, 2025. 1\\n[99] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,\\nand Yejin Choi. Hellaswag: Can a machine really finish\\nyour sentence?arXiv preprint arXiv:1905.07830, 2019. 2,\\n6\\n[100] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei\\nLu. Tinyllama: An open-source small language model.\\narXiv preprint arXiv:2401.02385, 2024. 7\\n[101] Chuyang Zhao, Yuxing Song, Wenhao Wang, Haocheng\\nFeng, Errui Ding, Yifan Sun, Xinyan Xiao, and Jingdong\\nWang. Monoformer: One transformer for both diffu-\\nsion and autoregression.arXiv preprint arXiv:2409.16280,\\n2024. 1, 2, 3, 6, 7\\n[102] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala,\\nMichihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe\\nMa, Luke Zettlemoyer, and Omer Levy. Transfusion: Pre-\\ndict the next token and diffuse images with one multi-modal\\nmodel.arXiv preprint arXiv:2408.11039, 2024. 1, 3, 6, 7\\n12', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a25a5a5a-ffdf-4fc0-a30a-32319f42502a', embedding=None, metadata={'page_label': '1', 'file_name': '2510.13253v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.13253v1.pdf', 'file_type': 'application/pdf', 'file_size': 6563163, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='End-to-End Multi-Modal Diffusion Mamba\\nSupplementary Material\\n7. Appendix A\\n7.1. Theorem 1\\nTheorem 1.According to Bayes’ theorem and the Gaus-\\nsian distribution density formula, the following calculation\\nrelationship of pdata(y)\\npdata(zg\\nn,t) is obtained:\\npdata(y)\\npdata(zg\\nn,t) = exp\\n \\n∥zg\\nn,t∥2\\n2 − ∥zg\\nn,t −\\np\\n¯αg\\nt zg\\nn,0∥2\\n2(1−¯αg\\nt )\\n!\\n(1)\\nProof.According to [53], from Bayes’ theorem, we express\\nthe posterior probability as:\\npdata(zg\\nn,0|zg\\nn,t) = p(zg\\nn,t|zg\\nn,0)pdata(zg\\nn,0)\\np(zg\\nn,t) . (2)\\nRearranging, we obtain:\\npdata(zg\\nn,0)\\npdata(zg\\nn,t) = p(zg\\nn,t|zg\\nn,0)\\np(zg\\nn,t) . (3)\\nGiven the real datazg\\nn,0, the probability of the diffused noise\\nstate isp(z g\\nn,t|zg\\nn,0).p(z g\\nn,t)is the marginal distribution of\\nall possiblez g\\nn,0 after diffusion.\\nThe forward noise addition process in the diffusion\\nmodel is defined as follows:\\nzg\\nn,t =\\nq\\n¯αg\\nt zg\\nn,0 +\\nq\\n1−¯αg\\nt ϵg\\nn,t, ϵg\\nn,t ∼ N(0, I), (4)\\nand it can be seen that givenz g\\nn,0,z g\\nn,t obeys the Gaussian\\ndistribution:\\np(zg\\nn,t|zg\\nn,0) =N(z g\\nn,t;\\nq\\n¯αg\\nt zg\\nn,0,(1−¯αg\\nt )I), (5)\\nwhere this conditional probability indicates thatz g\\nn,t is a\\nGaussian distribution with\\np\\n¯αg\\nt zg\\nn,0 as mean and(1−¯αg\\nt )I\\nas variance.\\nThen, for the marginal distributionp(z g\\nn,t)can be calcu-\\nlated by integration:\\np(zg\\nn,t) =\\nZ\\np(zg\\nn,t|zg\\nn,0)pdata(zg\\nn,0)dzg\\nn,0. (6)\\nTypically, we assume that the underlying distribution of the\\ndata follows a standard Gaussian:\\npdata(zg\\nn,0) =N(z g\\nn,0; 0, I), (7)\\nsince the convolution of two Gaussian distributions is a\\nGaussian distribution,p(z g\\nn,t)is still a Gaussian distribu-\\ntion:\\np(zg\\nn,t) =N(z g\\nn,t; 0, I). (8)\\nCombining the above derivation, we get:\\npdata(zg\\nn,0)\\npdata(zg\\nn,t) = p(zg\\nn,t|zg\\nn,0)\\np(zg\\nn,t) . (9)\\nThen, substitute into the Gaussian distribution density for-\\nmula:\\np(zg\\nn,t|zg\\nn,0)\\np(zg\\nn,t) =\\nexp (−\\n∥zg\\nn,t−\\n√\\n¯αg\\nt zg\\nn,0∥2\\n2(1−¯αg\\nt ) )\\nexp (−\\n∥zg\\nn,t∥2\\n2 )\\n. (10)\\nFurther sorting, thus, we derive Eq. (1), completing the\\nproof.\\n7.2. Theorem 2\\nTheorem 2.Given the denoising process modeled by a\\nscore-based probability ratio functions θ(zg\\nn,t), defined as\\nsθ =\\npdata(zg\\nn,0)\\npdata(zg\\nn,t) , this paper defines a learnable approxima-\\ntion using a parameterized score functionf θ, such that the\\nprobability ratio can be estimated as:\\nsθ(zg\\nn,t) = exp (fθ(zg\\nn,t, zg\\nn,0))P\\ny∈zg\\nn,0:t−1\\nexp (fθ(zg\\nn,t, y)), (11)\\nProof.To derive Eq. (11), we start from the definition of\\nthe score-based probability ratio:\\nsθ(zg\\nn,t) = pθ(zg\\nn,0)\\npθ(zg\\nn,t) . (12)\\nUsing Bayes’ theorem, we can express the conditional\\nprobability as:\\npθ(zg\\nn,0|zg\\nn,t) = p(zg\\nn,t|zg\\nn,0)pθ(zg\\nn,0)\\np(zg\\nn,t) . (13)\\nTaking the logarithm on both sides, we define a learnable\\nfunctionf θ(zg\\nn,t, zg\\nn,0)that approximates:\\nfθ(zg\\nn,t, zg\\nn,0)≈logp θ(zg\\nn,0|zg\\nn,t). (14)\\nGiven the forward diffusion process follows:\\np(zg\\nn,t|zg\\nn,0) =N(z g\\nn,t;\\nq\\n¯αg\\nt zg\\nn,0,(1−¯αg\\nt )I), (15)\\nand the marginal distribution:\\nq(zg\\nn,t)≈ N(zg\\nn,t; 0, I), (16)\\n1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4ffcedb8-511d-42f8-91de-acc217b5decc', embedding=None, metadata={'page_label': '2', 'file_name': '2510.13253v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.13253v1.pdf', 'file_type': 'application/pdf', 'file_size': 6563163, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='we obtain:\\nfθ(zg\\nn,t, zg\\nn,0) =− ∥zg\\nn,t −\\np\\n¯αg\\nt zg\\nn,0∥2\\n2(1−¯αg\\nt ) + ∥zg\\nn,t∥2\\n2 . (17)\\nTo ensure numerical stability and gradient optimization,\\nwe normalizesθ(zg\\nn,t)using softmax over the set of possible\\ndenoising states:\\nsθ(zg\\nn,t) = exp (fθ(zg\\nn,t, zg\\nn,0))P\\ny∈zg\\nn,0:t−1\\nexp (fθ(zg\\nn,t, y)). (18)\\nThus, we have derived Eq. (11), which provides a param-\\neterized score function for probability ratio estimation.\\n7.3. Theorem 3\\nTheorem 3.To achieve the optimalscoreentropy [53]\\nwhich is demonstrated on Eq.(21), the selection step choose\\njitems where eachz g\\nn,t satisfiesse= 0, i.e.,\\nsθ(zg\\nn,t)≈ pdata(y)\\npdata(zg\\nn,t) (19)\\nProof.To prove the Theorem 3, we divide this proof into\\nthree parts: The first is to determinethe optimization tar-\\nget of the model approximation. The second is to deter-\\nminethe iterative process of the model optimization tar-\\nget. The third is to provethe convergence validity of the\\niterative process.\\n1)The optimization target of the model approxima-\\ntion\\nAccording to the denoising score entropy proposed by\\nLouet al. [53], the Mamba block loss function can be de-\\nfined as follows:\\nLse =E zg\\nn,0∼p0,zg\\nn∼p(·|zg\\nn,0)se (20)\\nTo minimize the loss function, theseshould be closed\\nto value 0. And based on the score entropy loss [53], these\\ncan be described as:\\nse=\\nX\\ny∈zg\\nn,0:t−1\\nωg\\nzg\\nn,t\\n\\x12\\nsθ(zg\\nn,t)− pdata(y)\\npdata(zg\\nn,t) logs θ(zg\\nn,t)\\n+K\\n\\x12 pdata(y)\\npdata(zg\\nn,t)\\n\\x13\\x13\\n,\\n(21)\\nwhereK(a) =a(loga−1)a normalization term that en-\\nsures the loss is non-negative. And weightsω g\\nzg\\nn,t\\n∈(0,1)\\ncan adjust the weights assigned to different noise latent\\nrepresentations. This can improve optimization efficiency\\nby explicitly selecting important point pairs. For exam-\\nple, higher weights can be assigned to noise latent repre-\\nsentations that may introduce larger errors within a specific\\nrange, thereby guiding the update of the model. And ul-\\ntimately control the final totalseto be close to 0. And\\nsθ(zg\\nn,t)isn-th noise latent representation of the model pre-\\ndicted score ratio att-th denoising step.\\nTo determine the necessary conditions for minimiz-\\ning se, we compute the partial derivative with respect to\\nsθ(zg\\nn,t):\\n∂se\\n∂sθ(zg\\nn,t) =\\nX\\ny∈zg\\nn,0:t−1\\nωg\\nzg\\nn,t\\n\\x12\\n1− pdata(y)\\npdata(zg\\nn,t)\\n1\\nsθ(zg\\nn,t)\\n\\x13\\n.\\n(22)\\nSetting the gradient to zero for optimization,\\n1− pdata(y)\\npdata(zg\\nn,t)\\n1\\nsθ(zg\\nn,t) = 0. (23)\\nRearranging the terms, we obtain:\\nsθ(zg\\nn,t) = pdata(y)\\npdata(zg\\nn,t). (24)\\nThus, at the optimal solution, the predicted score func-\\ntion must exactly match the empirical probability ratio.\\nFor model parameters θ, we analyze the gradient:\\n∂se\\n∂θ =\\nX\\ny∈zg\\nn,0:t−1\\nωg\\nzg\\nn,t\\n\\x12∂sθ(zg\\nn,t)\\n∂θ −\\npdata(y)\\npdata(zg\\nn,t)\\n1\\nsθ(zg\\nn,t)\\n∂sθ(zg\\nn,t)\\n∂θ\\n\\x13\\n.\\n(25)\\nFor gradient convergence, we set the derivative to zero:\\n∂sθ(zg\\nn,t)\\n∂θ\\n\\x12\\n1− pdata(y)\\npdata(zg\\nn,t)\\n1\\nsθ(zg\\nn,t)\\n\\x13\\n= 0. (26)\\nSince the gradient term\\n∂sθ(zg\\nn,t)\\n∂θ is nonzero for model\\nupdates, the following condition must hold:\\n1− pdata(y)\\npdata(zg\\nn,t)\\n1\\nsθ(zg\\nn,t) = 0, (27)\\nwhich again yields the optimal condition:\\nsθ(zg\\nn,t) = pdata(y)\\npdata(zg\\nn,t). (28)\\nIn summary, the necessary conditions for minimizing the\\nScore Entropy Loss and ensuring the optimal score function\\nare:\\n• The predicted score function must satisfy:\\nsθ(zg\\nn,t) = pdata(y)\\npdata(zg\\nn,t). (29)\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='20dde44b-5b89-4ac2-807d-e7cf8899e522', embedding=None, metadata={'page_label': '3', 'file_name': '2510.13253v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.13253v1.pdf', 'file_type': 'application/pdf', 'file_size': 6563163, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='• The gradient with respect to the model parameters must\\nsatisfy:\\n∂sθ(zg\\nn,t)\\n∂θ\\n\\x12\\n1− pdata(y)\\npdata(zg\\nn,t)\\n1\\nsθ(zg\\nn,t)\\n\\x13\\n= 0. (30)\\nThese conditions imply that when the model learns the\\ncorrect probability ratio, the gradient becomes zero, leading\\nto optimal convergence of the Score Entropy Loss. There-\\nfore, optimizing sθ(zg\\nn,t) to match pdata(y)\\npdata(zg\\nn,t) is both a neces-\\nsary and sufficient condition for achieving the lowest possi-\\nble loss.\\nBased on Eq. (26),θ={H g\\nn,t, A, B, C, D,∆}represent\\nthe state space in the block. We can obtain the selected noise\\nlatent representationz g\\nn,t by updating the computation in the\\nstate space architecture from Mamba-2 [28], which can be\\ndefined as follows:\\nHg\\nn,t = ¯AHg\\nn,t−1 + ¯Bzg\\nn,t (31)\\nzg\\nn−1,t =CH g\\nn,t +Dz g\\nn,t (32)\\n¯A= exp (∆A) (33)\\n¯B= (∆A) −1 ·(exp (∆A)−I)·∆B (34)\\nwhereH g\\nn,t represents the hidden state representation,A\\nandBcontrol the evolution of hidden states and latent space\\nnoise vector inputs, respectively,Cgoverns the hidden state\\nrepresentation of the target output andDmanages the non-\\nlinear skip connection for latent space noise vector inputs.\\n∆denotes the learnable time parameter.\\n2)The iterative process of the model optimization tar-\\ngetConsidering the parameters inθ, they are updated by the\\nfollowing steps. First,the update ofAand ¯A. Given that ¯A\\ncontrols the recursive evolution of hidden stateH g\\nn,t based\\nonAand∆, we can gain the relationship in Eq. (33). So,\\nthe gradient can be described as follows:\\n∂L\\n∂A = ∂L\\n∂ ¯A · ∂ ¯A\\n∂A (35)\\nwhere\\n∂ ¯A\\n∂A = ∆·exp (∆A) (36)\\nthen through backpropagation to calculate the gradient ofL\\nto ¯Aand combined with the chain rule to updateA.\\nSecond,the update ofBand ¯B. Given that the def-\\ninition of ¯Bin Eq. (34), the gradient can be described as\\nfollows (familiar with the update rule ofA):\\n∂L\\n∂B = ∂L\\n∂ ¯B · ∂ ¯B\\n∂B (37)\\nwhere gradient transfer involves matrix derivation, which\\nrequires considering the derivative rule of matrix multipli-\\ncation. Finally, the chain rule depends on the gradients of\\n∆Aand∆B.\\nThird,the update ofC. Given thatCcontrols the hid-\\nden state and its direct contribution to the outputz g\\nn−1,t is\\nas Eq. (32) defined, the gradient can be described as fol-\\nlows:\\n∂L\\n∂C = ∂L\\n∂zg\\nn−1,t\\n· ∂zg\\nn−1,t\\n∂C (38)\\nwhere\\n∂zg\\nn−1,t\\n∂C =H g\\nn,t (39)\\nSo the update rule can be described as follows:\\nC←C−η ∂L\\n∂C (40)\\nwhereηis the learning rate.\\nFourth,the update ofD. Given thatDgoverns the skip\\nconnection and directly act onz g\\nn,t, the gradient can be de-\\nfined as follows:\\n∂L\\n∂D = ∂L\\n∂zg\\nn−1,t\\n· ∂zg\\nn−1,t\\n∂D (41)\\nwhere\\n∂zg\\nn−1,t\\n∂D =z g\\nn,t (42)\\nFifth,the update of∆.∆denotes the learnable time\\nparameter and affects the dynamic behavior of ¯Aand ¯B. So\\nthe gradient can be defined as follows:\\n∂L\\n∂∆ = ∂L\\n∂ ¯A · ∂ ¯A\\n∂∆ + ∂L\\n∂ ¯B · ∂ ¯B\\n∂∆ (43)\\nwhere\\n∂ ¯A\\n∂∆ =A·exp (∆A) (44)\\nf(A, B,∆) =−(∆A) −1A(∆A)−1(exp(∆A)−I)∆B\\n+ (∆A)−1(Aexp(∆A))∆B\\n+ (∆A)−1(exp(∆A)−I)B (45)\\nIn this problem, the structure of the state space model\\nand the diffusion model provide theoretical support for the\\nstrong convexity of the loss function and the Lipschitz prop-\\nerty of the gradient. First, the stability of the state space\\nmodel leads to the hidden state update equation:\\nHg\\nn,t = ¯AHg\\nn,t−1 + ¯Bzg\\nn,t (46)\\nwhere ¯A= exp(∆A), ¯B= (∆A) −1(exp(∆A)−I)∆B\\nis generated via matrix exponential. It has the following\\ncharacteristics:\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f406c624-6ace-4236-a47a-145085954c8c', embedding=None, metadata={'page_label': '4', 'file_name': '2510.13253v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.13253v1.pdf', 'file_type': 'application/pdf', 'file_size': 6563163, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='• IfAis a stable matrix (all eigenvalues have negative real\\nparts), then the modulus of the eigenvalues of ¯Ais less\\nthan1, which ensures that the hidden state does not di-\\nverge.\\n• The state update equation is linear, so the gradient of the\\nparametersA, B, C, Dis linearly solvable, making it easy\\nto optimize.\\nSecondly, given the characteristics of the diffusion\\nmodel, there is a score ratio prediction loss function:\\nL=E zg\\nn,t,p\\n\\x14\\n∥ pdata(y)\\npdata(zg\\nn,t) −s θ(zg\\nn,t)∥2\\n2\\n\\x15\\n(47)\\nwhereLis in squared error form and is therefore a con-\\nvex function (subconvexity). Then the gradient can be ex-\\npressed as follows:\\n∇θL= 2E zg\\nn,t,p\\n\\x14\\n∥( pdata(y)\\npdata(zg\\nn,t) −s θ(zg\\nn,t))∇θsθ(zg\\nn,t)∥2\\n2\\n\\x15\\n(48)\\nwhere the gradient is a linear combination ofθand satisfies\\nthe Lipschitz continuity condition.\\nTo sum up, combined with the model parametersθ=\\nA, B, C, D,∆, there is the following convergence of the\\nspecific parameter updating process.\\nFirst for the hidden state update:\\nHg\\nn,t = ¯AHg\\nn,t−1 + ¯Bzg\\nn,t (49)\\nwhereAis a stable matrix, ¯Ais stable, ensuring that the\\nhidden state does not diverge.\\nSecond for output calculation:\\nzg\\nn−1,t =CH g\\nn,t +Dz g\\nn,t (50)\\nand it is a linear transformation, which ensures the stability\\nof the gradient solution forCandD.\\nThird for time step parameters∆, it is a learnable pa-\\nrameter of the time scale, which is directly related to the\\ndiscretization in the state space model. It is updated by the\\nchain rule as follows:\\n∂L\\n∂∆ = ∂L\\n∂ ¯A · ∂ ¯A\\n∂∆ + ∂L\\n∂ ¯B · ∂ ¯B\\n∂∆ (51)\\namong this, in the discretization formula, ¯Aand ¯Bare ex-\\nponential functions with continuous and differentiable gra-\\ndients, which are easy to converge.\\n3)The convergence validity of the iterative process\\nIn order to ensure the convergence of the above iterative\\nprocess, the following conditions usually need to be met:\\n• The convergent objective functionLis a continuously dif-\\nferentiable function with respect to parameterθand it is\\nstrongly convex or subconvex (at least a convex function).\\n• Make sure the learning rate satisfies0< η <2/Lwhere\\nLis the Lipschitz constant for the gradient∇ θLof the\\nconvergent objective function (the upper bound on the\\nrate of change of the gradient).\\n• The matrix ¯A(generated by discretization) is stable, that\\nis, the magnitude of its eigenvalues is less than 1.\\nWhen the above convergence conditions are met, assum-\\ning that the convergence targetLfunction is aµ-strongly\\nconvex function (strong convexity is a stricter form of con-\\nvex function), the convergence of gradient descent can be\\nproved by the following formula. First, the updated formula\\nfor gradient descent is given:\\nθk+1 =θ k −η∇ θL(θk) (52)\\nwhereθ k is the parameter vector at the k-th iteration.\\nSecondly, the properties of strongly convex functions are\\ngiven, that is, if the convergent objective functionLisµ-\\nstrongly convex and the Lipschitz constant of the gradient\\nisL, then the error of the gradient descent method will con-\\nverge at an exponential rate:\\nL(θk)− L(θ∗)≤ρ k\\x00\\nL(θ0)− L(θ∗)\\n\\x01\\n(53)\\nwhereρ= 1−2ηµis the convergence rate (0< ρ <1),\\nandθ ∗ is the global optimum.\\nThird, if the Lipschitz gradient condition is satisfied, that\\nis,∇ θLisL-Lipschitz continuous:\\n∥∇θL(θ1)− ∇θL(θ2)∥ ≤L∥θ1 −θ 2∥ (54)\\nthen selecting a learning rate0< η < 2\\nL ensures conver-\\ngence.\\nAlgorithm 1Gradient Descent Algorithm\\nInput:Initialize parametersA,B,C,D, and∆.\\nrepeat\\nCalculate the lossL.\\nCompute the gradient ofLwith respect toA,B,C,\\nD, and∆using the chain rule.\\nUpdate each parameter using the gradient descent\\nrule.\\nPerform backpropagation to compute:\\n∇θ∥ pdata(y)\\npdata(zg\\nn,t) −s θ(zg\\nn,t)∥2\\n2.\\nuntilconvergence\\nIn general, the process of update and convergence can\\nbe summarized in Algorithm 1. Through repeated itera-\\ntions, the model parameterθwill be gradually optimized,\\nso that the convergence objective functionLwill be con-\\nverged andsegradually approaches 0, that is,s θ approaches\\npdata(y)\\npdata(zg\\nn,t) . Thenjitems of noise latent representationz g\\nn,t\\nthat satisfy all the above conditions will be selected, and\\n4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f1daf2ea-63be-4d0d-b7c9-c8abe71a334a', embedding=None, metadata={'page_label': '5', 'file_name': '2510.13253v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.13253v1.pdf', 'file_type': 'application/pdf', 'file_size': 6563163, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='the model will proceed to the next step of denoising in the\\ndirection of thesejitems.\\nAbove all, in the inference stage, the model will choose\\nthe best noise latent representation of image patch or text\\nembedding, includingjitems to restore the image or text.\\nDue to this, the model has already learned from the datasets\\nthat should be focused on and ignored. Compared with\\nthe Transformer models, which need to calculate all im-\\nage patches or text embeddings, it will shorten the infer-\\nence time when generating high-resolution images or long-\\nsequence text. The results are shown in the main paper Sec-\\ntion 5.3.1 Performance Analysis.\\n8. Appendix B\\n8.1. Denoising process based on DPM-Solver\\nBased on the diffusion denoising model trained by Score\\nEntropy Loss, we hope to combine DPM-Solver (Diffusion\\nProbabilistic Model Solver)[54] in the inference stage to re-\\nduce sampling steps and improve inference efficiency.\\nDPM-Solver is a high-order ODE-solving method for\\ndiffusion models. It constructs partial differential equations\\n(ODEs) and uses numerical solution techniques to accel-\\nerate the diffusion denoising process. It can restore high-\\nquality data from Gaussian noise in a minimal number of\\nsteps (such as 10 steps) without sacrificing model perfor-\\nmance.\\nThe core idea of DPM-Solver is to reformulate the in-\\nverse diffusion process of the diffusion model as an ordi-\\nnary differential equation (ODE) and solve it efficiently us-\\ning numerical methods. For the standard diffusion model,\\nwe have:\\ndzg\\nn,t\\ndt =− 1\\n2βtzg\\nn,t +\\np\\nβtϵg\\nn,t, ϵ g\\nn,t ∼ N(0, I).(55)\\nDPM-Solver estimatesϵ θ(zg\\nn,t, t)by denoising the score\\nmatching, which can be rewritten as:\\ndzg\\nn,t\\ndt =f θ(zg\\nn,t, t), (56)\\nwhere the formula describes the rate of change of the la-\\ntent variablez g\\nn,t in the timetdimension, and its evolution\\nprocess can be accelerated by numerical solution methods.\\nIn the Mamba decoder trained with Score Entropy Loss,\\nwe learn:\\nsθ(zg\\nn,t) = exp (fθ(zg\\nn,t, zg\\nn,0))P\\ny∈zg\\nn,0:t−1\\nexp (fθ(zg\\nn,t, y)). (57)\\nTherefore, in the DPM-Solver framework, we hope to\\nuse this ratio’s gradient information to directly construct the\\nODE and reduce the number of sampling steps during infer-\\nence.\\nFirst, we need to compute denoised ODE. DPM-Solver\\nuses Score Matching technology [54] to predict the noise\\nϵθ(zg\\nn,t, t)through a neural network, and then calculates it\\naccording to the denoising ODE:\\ndzg\\nn,t\\ndt =− 1\\n2βt\\n \\nzg\\nn,t −\\np\\n¯αg\\nt zg\\nn,0\\n1−¯αg\\nt\\n!\\n, (58)\\nfurthermore, we can calculate based on Score Entropy [53]:\\ndzg\\nn,t\\ndt =− 1\\n2βtsθ(zg\\nn,t)∇z logp θ(zg\\nn,0|zg\\nn,t), (59)\\nwhere∇ z logp θ(zg\\nn,0|zg\\nn,t)is calculated byse,s θ(zg\\nn,t is\\npredicted probability ratios through neural networks. This\\nformula describes the ODE trajectory from the noisy state\\nzg\\nn,t to the denoised statez g\\nn,0.\\nWe then use DPM-Solver to perform inference. For the\\nfirst-order approximation method, the basic form of DPM-\\nSolver is the first-order ODE approximation:\\nzg\\nn,t ≈z g\\nn,t−∆t − 1\\n2βt\\n \\nzg\\nn,t −\\np\\n¯αg\\nt zg\\nn,0\\n1−¯αg\\nt\\n!\\n∆t, (60)\\nby usings θ(zg\\nn,t)calculated by Score Entropy Loss, we can\\nfurther rewrite the formula:\\nzg\\nn,t ≈z g\\nn,t−∆t − 1\\n2βtsθ(zg\\nn,t)∇z logp θ(zg\\nn,0|zg\\nn,t)∆t.\\n(61)\\nThe formula can be directly used to update the denoising\\nprocess to achieve efficient sampling iteratively.\\nFurthermore, DPM-Solver uses second-order numerical\\nmethods [54] to improve accuracy:\\nzg\\nn,t =z g\\nn,t−∆t + ∆t\\n2\\nh\\nfθ(zg\\nn,t, t) +fθ(zg\\nn,t−∆t, t−∆t)\\ni\\n(62)\\nwhich allows us to complete denoising inference in a very\\nsmall number of iterations (e.g., 10-20 steps), significantly\\nspeeding up the computation compared to normal diffusion\\nsampling (e.g., 1000 steps).\\nAlgorithm 2Mamba-Based Inference with DPM-Solver\\nInput:Noisy latent state zg\\nn,t.\\nrepeat\\nPredict the score function sθ(zg\\nn,t) for computing\\nthe denoising ODE.\\nApply DPM-Solver update rule:z g\\nn,t ←z g\\nn,t−∆t +\\n∆t\\n2\\nh\\nfθ(zg\\nn,t, t) +fθ(zg\\nn,t−∆t, t−∆t)\\ni\\n.\\nuntilgain thez g\\nn,t\\n5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e91da44f-9187-412d-9565-31e80e9d3146', embedding=None, metadata={'page_label': '6', 'file_name': '2510.13253v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.13253v1.pdf', 'file_type': 'application/pdf', 'file_size': 6563163, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9. Appendix C\\n9.1. Model Configuration\\nConfiguration Value\\nSize 7B\\nMamba block 49\\nHidden Dimension 2048\\nGFlops 424\\nOptimizer AdamW\\nLearning Rate 0.0001\\nWeight Decay -\\nTraining Epochs 1\\nSampling step 500000\\nEMA 0.9999\\nPatch size 2×2\\nMaximum Token Length 512\\nTable 1. Parameter settings for MDM.\\n10. Appendix D\\n10.1. SentencePiece (Unigram BPE)\\nSentencePiece (Unigram BPE) [47] provides an optimal\\nsubword-based tokenization approach that enables im-\\nproved generalization and adaptability for handling both\\ntextual and multimodal data.\\n10.1.1. Theoretical Background\\nSentencePiece employs a probabilistic model based on a\\nUnigram Language Model (ULM), where each sentence x\\nis decomposed into a sequence of subwords si with a like-\\nlihood function:\\np(x) =\\nY\\ni\\np(si), (63)\\nwhere each subword unit si is assigned a probability es-\\ntimated from training data. Unlike traditional Byte-Pair\\nEncoding (BPE), which deterministically merges frequent\\nsubword pairs, the Unigram BPE method probabilistically\\nlearns an optimal vocabulary while gradually discarding\\nsubwords with lower contributions.\\nTo train SentencePiece, an initial vocabulary is con-\\nstructed using all possible subword combinations, after\\nwhich an iterative Expectation-Maximization (EM) opti-\\nmization is performed. At each iteration, subwords con-\\ntributing the least to sequence likelihoods are removed,\\nleading to an optimal vocabulary.\\n10.1.2. Training Procedure\\nThe training of the SentencePiece model is conducted on\\na large-scale dataset containing both pure-text corpora and\\nmultimodal text-image descriptions. Given the multimodal\\nnature of our dataset, we mix textual data from Ultrachat\\nand text descriptions from JourneyDB and ImageNet to en-\\nsure cross-modal adaptability.\\nDataset Preprocessing:To prepare the dataset, raw text\\nis extracted, normalized, and formatted as a line-separated\\ncorpus file. The dataset mixing strategy follows:\\n• Extract textual information from Ultrachat.\\n• Concatenate textual descriptions from JourneyDB and\\nImageNet.\\n• Remove redundant, low-quality, or excessively short text\\nsamples.\\n• Shuffle the corpus to prevent dataset bias.\\nSentencePiece Model Training:The SentencePiece Un-\\nigram BPE model is trained using the following configura-\\ntion:\\nimport sentencepiece as spm\\nspm.SentencePieceTrainer.train(\\ninput=\"text_data.txt\",\\n# Training corpus\\nmodel_prefix=\"unigram_bpe\",\\n# Output model prefix\\nvocab_size=32000,\\n# Vocabulary size\\nmodel_type=\"unigram\",\\n# Unigram-based BPE\\ncharacter_coverage=0.9995,\\n# Coverage for rare characters\\nnum_threads=8,\\n# Parallel training\\ninput_sentence_size=1000000,\\n# Sample size\\nshuffle_input_sentence=True\\n# Shuffle corpus\\n)\\nThis results in two key output files:\\nunigram bpe.model(binary model for tokeniza-\\ntion) andunigram bpe.vocab(vocabulary list with\\nprobabilities).\\n10.1.3. Evaluation and Optimization Strategies\\nThe effectiveness of the trained tokenization model is eval-\\nuated based on tokenization efficiency and generalization\\ncapability. The following criteria are considered:\\n•Subword Granularity: The trade-off between word and\\ncharacter-level tokenization.\\n•Out-of-Vocabulary (OOV) Rate: The ability to handle\\nunseen words.\\n•Multimodal Alignment: The compatibility of subword\\nembeddings with image features in the latent space.\\nGiven the computational constraints of multimodal dif-\\nfusion models, we optimize the SentencePiece model with:\\n• Selecting an optimalvocab size( 16K-32K) to bal-\\nance representation and sequence length.\\n• Applying dataset mixture strategies to enhance general-\\nization across different data distributions.\\n• Ensuring tokenization stability by enforcing\\ncharacter coverage 0.9995 to capture rare\\ntextual variations.\\n6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b1f54fb7-12c2-4fc0-b8ca-27cf91d7bf7c', embedding=None, metadata={'page_label': '7', 'file_name': '2510.13253v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.13253v1.pdf', 'file_type': 'application/pdf', 'file_size': 6563163, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11. Appendix E\\n11.1. Complexity\\nSince the size of the noisy latent encoder (V AE) is signifi-\\ncantly smaller than that of the diffusion decoder (Mamba),\\nwe will focus our analysis on the computational complexity\\nof the diffusion decoder. According to [66], the complexity\\nof each Mamba block isO(LN 2), whereLis the length of\\nthe input data andNrefers to the size of each parameter\\n({Hg\\nn,t, A, B, C, D,∆}) in the state space. The diffusion\\ndecoder is composed ofMMamba blocks, resulting in an\\noverall computational complexity ofO(MLN 2).\\nFor comparison, consider an equivalent end-to-end trans-\\nformer model optimized with GQA [1, 79, 101]. This model\\nmaintains the same input lengthLand GQA module dimen-\\nsionN. WithMlayers and a grouping parameterG, its\\ncomputational complexity isO(ML 2N/G).\\nDetermining which complexity is superior between\\nO(MLN 2)andO(ML 2N/G)can be challenging. How-\\never, it is important to note thatNcan be significantly\\nsmaller thanL/GwhenLis very large. As a result, the pro-\\nposed MDM can achieve greater computational efficiency\\nthan end-to-end transformer models when processing high-\\nresolution images and long-sequence texts.\\n12. Appendix F\\n12.1. Image generation\\nFigure 1. Image generation with CFG on ImageNet [15] 256×\\n256.\\n12.2. Image generation on COCO and Flickr\\nA black dog running on the grass.Two children playing on the beach.A red car parked by the street.A chef preparing food in a kitchen.\\nA cat sitting on the windowsilllooking outside.A group of people having a picnic in the park.\\n An elderly man reading a newspaper on a bench.\\nA photographer taking pictures of the sunset.\\nA hiker enjoying the view from the mountain top.\\nA scientist conducting research in a laboratory.A fox walking in the snow.\\nA kangaroo hopping on the grassland.\\nA butterfly resting on a flower.\\nA monkey picking bananas in a tree.A firefighter working at a fire scene.\\n A whale swimming in the ocean.\\nFigure 2. Image generation on COCO [42] caption text.\\nA fashion model walking on a runway surrounded by photographers.\\nA person rock climbing on a steep cliff without any safety ropes.\\nA group of friends enjoying a bonfire at a campsite under the stars.\\n A kayaker navigating through a narrow canyon with towering rock walls.\\nA street performer balancing on a unicycle while juggling in a crowded plaza.\\n A couple posing for wedding photos in a picturesque garden at sunset.\\nA person sitting on a rooftop terrace overlooking a bustling cityscape.\\nA group of students in graduation gowns tossing their caps into the air.\\nA surfer catching a large wave at a remote beach with rocky cliffs.\\nA traveller pulling a suitcase through a crowded airport terminal.\\nA flock of flamingos wading in a shallow lagoon at sunrise.\\nAn abandoned fishing boat resting on a sandy shore with peeling paint.\\nA family of otters swimming together in a crystal-clear river surrounded by mossy rocks.\\nA herd of wild horses galloping through a snow-covered meadow at dusk.\\nA peacock displaying its vibrant feathers in the middle of a serene botanical garden.\\nA waterfall cascading into a turquoise pool, surrounded by dense tropical rainforest.\\nFigure 3. Image generation on Flickr 30K [94] caption text.\\n7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2a0eb7f1-fd59-413f-a3ea-5573f2451644', embedding=None, metadata={'page_label': '8', 'file_name': '2510.13253v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.13253v1.pdf', 'file_type': 'application/pdf', 'file_size': 6563163, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='13. Appendix G\\nA young girl in a pink t-shirt is laughing as she swings on a playground swing, surrounded by green trees and a bright blue sky.\\nTwo elderly men, one wearing a blue cap and the other a grey sweater, are playing chess in a sunny park with people walking in the background.\\nA professional chef in a white uniform and hat is meticulously decorating a chocolate cake in a well-equipped kitchen.\\nA group of teenagers, three boys and two girls, are taking a selfie on a rocky beach at sunset, all smiling and making peace signs.\\nA small dog with fluffy white fur is jumping to catch a yellow frisbee on a grassy field, with no other people visible in the scene.\\nA street performer dressed in a colorfulcostume and mask dances in front of a crowd in an urban square, with old buildings in the background.\\nFigure 4. Drawbacks in image generation.\\n13.1. Drawbacks\\nWhile MDM demonstrates strong performance across\\nvarious tasks and enhanced processing speed for high-\\nresolution images and long text sequences (as shown in\\nthe main paper Section 5.3.1 Performance Analysis), it\\nfaces several limitations. The model shows reduced effi-\\nciency when handling low-resolution images or short text\\nsequences, and its overall performance still trails behind tra-\\nditional multi-modal pre-trained models. Furthermore, the\\nmodel exhibits hallucination issues. These limitations rep-\\nresent key areas for future improvement.\\nIt can be observed from Fig. 4 that MDM still generates\\na small number of defective images, such as image deforma-\\ntion, collapse, distortion, and blurring. This may be due to\\nthe model’s scale being insufficient and limitations in how\\neach modality’s data is represented in the decoder. Addi-\\ntionally, the diffusion reduction process might experience\\nsome instability, which could lead to subpar sampling re-\\nsults. Therefore, there is still potential for further improve-\\nments to the model to address these issues.\\nThe partial performance results of the model on the\\nFlickr 30K dataset reveal significant challenges, particularly\\nwhen dealing with complex text data that requires generat-\\ning intricate images, especially those involving people and\\nanimals. The model often loses important details, such as\\nfacial features and the depiction of limbs. Additionally, it\\nexhibits a tendency to be inefficient and make errors, such\\nas repetitively copying and pasting certain objects, resulting\\nin a dilution of detail for those entities and the generation of\\ninstances that do not accurately match the accompanying\\ndescriptive language (as shown in Figs. 3 and 5). The main\\nreason for the above problems is that the Flickr 30K dataset\\nemphasizes the correlation between different modal seman-\\ntics rather than focusing solely on classification or recog-\\nnition tasks like the COCO dataset. This means that the\\nmodel needs stronger capabilities for multi-modal semantic\\nunderstanding. The MDM model employs a unified modal\\nfusion decoder under a constrained scale, which may limit\\nits ability to enhance semantic understanding compared to\\ntraditional models. Therefore, the MDM model needs con-\\ntinuous optimization.\\nA young girl in a pink t-shirt is laughing as she swings on a playground swing, surrounded by green trees and a bright blue sky.\\nTwo elderly men, one wearing a blue cap and the other a grey sweater, are playing chess in a sunny park with people walking in the background.\\nA professional chef in a white uniform and hat is meticulously decorating a chocolate cake in a well-equipped kitchen.\\nA group of teenagers, three boys and two girls, are taking a selfie on a rocky beach at sunset, all smiling and making peace signs.\\nA small dog with fluffy white fur is jumping to catch a yellow frisbee on a grassy field, with no other people visible in the scene.\\nA street performer dressed in a colorfulcostume and mask dances in front of a crowd in an urban square, with old buildings in the background.\\nFigure 5. Drawbacks in generating complex captions images.\\n8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='870db060-8d9d-4b19-b7cd-a4a58a64bff2', embedding=None, metadata={'page_label': '1', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CONTEXT-SELECTIVESTATESPACEMODELS:\\nFEEDBACK ISALLYOUNEED\\nRiccardo Zattra Giacomo Baggio Umberto Casti Augusto Ferrante Francesco Ticozzi\\nDepartment of Information Engineering\\nUniversity of Padova\\nPadova, 35131, Italy\\n{riccardo.zattra, baggio, castiumber, augusto, ticozzi}@dei.unipd.it\\nABSTRACT\\nTransformers, powered by the attention mechanism, are the backbone of most foundation models,\\nyet they suffer from quadratic complexity and difficulties in dealing with long-range dependencies\\nin the input sequence. Recent work has shown thatstate space models(SSMs) provide an efficient\\nalternative, with the S6 module at the core of the Mamba architecture achieving state-of-the-art results\\non long-sequence benchmarks. In this paper, we introduce theCOFFEE(COntext From FEEdback)\\nmodel, a novel time-varying SSM that incorporatesstate feedbackto enable context-dependent\\nselectivity, while still allowing for parallel implementation. Whereas the selectivity mechanism of S6\\nonly depends on the current input, COFFEE computes it from the internal state, which serves as a\\ncompact representation of the sequence history. This shift allows the model to regulate its dynamics\\nbased on accumulated context, improving its ability to capture long-range dependencies. In addition\\nto state feedback, we employ an efficient model parametrization that removes redundancies present in\\nS6 and leads to a more compact and trainable formulation. On the induction head task, COFFEE\\nachieves near-perfect accuracy with two orders of magnitude fewer parameters and training sequences\\ncompared to S6. On MNIST, COFFEE largely outperforms S6 within the same architecture, reaching\\n97% accuracy with only 3585 parameters. These results showcase the role of state feedback as a key\\nmechanism for building scalable and efficient sequence models.\\n1 Introduction\\nRecurrent Neural NetworksandLong Short-Term Memory Networkshave been the dominant architectures for sequence\\nmodeling up to the introduction ofTransformers[ 1]. The latter are the backbone of modernfoundation models,\\nnamely large-scale models that are pre-trained on massive amounts of data (text, audio, video, etc.) and subsequently\\nfine-tuned, using task-specific data, for specific downstream tasks. By leveraging the celebrated attention mechanism,\\ntransformers achieved a groundbreaking improvement in generative Artificial Intelligence (AI), [ 2, 3, 4, 5, 6]. The\\nattention mechanism, though revolutionary, suffers from some limitations. In particular, a quadratic time complexity\\nwith respect to the length of the input sequence and severe difficulties in capturing long-range dependencies call for\\nfurther research and improvements. To this end, many variations of the attention mechanism have been proposed; see\\n[7, 8, 9, 10, 11]. A completely different and extremely promising approach is the use of linear State-Space Models\\n(SSMs): introduced in the 1960’s, they quickly revolutionized control and automation engineering [ 12, 13], and\\ntheir use spread to neighboring fields, from biology [ 14] to economics [15] and statistical modeling [16]. In fact, a\\nsignificant stream of valuable contributions have recently appeared that replace the attention mechanism with SSMs\\n[17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]. Other interesting works, such as [ 30] and [31] employ nonlinear\\nSSMs to address questions about asymptotic behavior of Transformers as the number of encoders layers increases.\\nThe state of the art of SSM in sequence modeling is calledS6and it lies at the core of the so-calledMambaarchitecture\\n[20]. The latter exhibits great capabilities and is able to outperform the state-of-the-art transformer in challenging tasks,\\nincluding the Long Range Arena (LRA), see [20] and [32]. The reason for this success lies in the fact that thestateis,\\nby definition, the mathematical object that encodes all the relevant information on the past evolution of a system needed\\narXiv:2510.14027v1  [cs.LG]  15 Oct 2025', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ab2e2e6c-f6fd-4c3d-b7c9-6148c7795d3c', embedding=None, metadata={'page_label': '2', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='to predict its future behavior1. Thisseparation propertyallows states to store elusive long-range dependencies. Of\\ncourse, the main challenge is to derive a universal class of state-space models whose parameters can be learned for each\\nspecific task. The S6 module captures, or rather approximates, this universality by resorting to (linear) time-varying\\nmodels.\\nIn this paper, we propose a further dramatic improvement: a novel class of SSMs that we call COFFEE (COntext From\\nFEEdback) which is endowed with feedback capabilities. Dynamical systems endowed with feedback loops are at the\\nbasis for the large majority of complex natural and artificial phenomena, as they provide a system with the possibility of\\nchanging its behavior on the basis of its current state, which, as previously recalled, contains all the relevant information\\nabout the past of the sequence under analysis. In this way, we are able to inducecontext-selectivityin the behavior of\\nCOFFEE: a property of paramount importance, which appears to give a momentous boost to the system’s performance.\\nWith respect to S6, COFFEE has another advantage (which could also be implemented in the S6 model): by exploiting a\\nchange of basis in the state-space representation of the model, we are able to reduce appreciably the number of learnable\\nparameters, offering an obvious edge in terms of both numerical implementation and mechanistic interpretability.\\nWe tested COFFEE in several versions of the induction head benchmark and in the MNIST benchmark. In these\\ntasks, our model appears to perform significantly better than the state-of-the-art S6 in more than one way. In fact,\\nnot only does COFFEE give much better results in terms of the accuracy of its output, but it also requires a smaller\\nnumber of learnable parameters and a much smaller training set to converge. The closed-loop COFFEE model is\\nobtained by a nonlinear state-feedback action on a time-varying linear model. In general, this kind of structure leads to\\ncomputational drawbacks because it is not amenable to parallelization and hence cannot take advantage of the highly\\nparallel architecture of modern GPU’s. In COFFEE, however, the feedback structure is constrained in such a way\\nthat, on the one hand, the performance degradation (with respect to the case of a more general feedback structure) is\\nnegligible and, on the other, the Jacobian of the state transition function is guaranteed to be diagonal. As a consequence,\\nwe can leverage recent results in [33] to obtain scalable parallel training that is computationally highly efficient.\\nThe remainder of this paper is organized as follows. In Section 2 we give a brief background on state-space models,\\nwith further details and useful proofs provided in A. Based on the review of state space models, in Section 3, we present\\nour model along with its mathematical motivations. In Section 4, we prove the effectiveness of our model by comparing\\nits performance with the state-of-the-art S6 model [20], in both the induction head and MNIST tasks. In Section 5, we\\nprovide some theoretical insights, based on an analysis made on a simplified version of the induction head task, whose\\ndetails are presented in Appendix D. We summarize our findings in Section 6, where we also outline the next steps in\\nthe development and testing of the model.\\n2 Background on SSMs\\n2.1 Linear, discrete SSM\\nState space models are powerful tools to model, analyze and control dynamical systems [ 13]. These are models in\\nwhich the relation between the input or control sequence u(k)∈R m and the sequence y(k)∈R p associated to the\\nquantities of interest for the system at hand and called theoutput, is mediated by a signal x(k)∈R n, which is called\\nthestateof the system. For more details, see Appendix A. In this paper, we leverage the class of linear, time-varying,\\ndiscrete-time systems with finite-dimensional state space. The models of this class are described by a couple of vector\\nequations of the form\\nx(k) =A(k)x(k−1) +B(k)u(k)\\ny(k) =C(k)x(k) (1)\\nwhere the matrix A(k)∈R n×n is calledstate transition matrix, B(k)∈R n×m is calledinput matrix, C(k)∈R p×n\\nis calledoutput matrix. From (1) it follows that, given the state x(k) at the present “time” k, the past of the input is\\nirrelevant for the future of the output sequence. This key feature of the state is known as theseparation property, and is\\nthe fundamental reason why the state functions as memory. The state can then be considered as a natural representative\\nof thecontextset by the past data, and as such will be exploited in our setting in selecting the dynamics conveniently.\\n2.2 Relevance of SSMs to Sequence Modeling\\nIn machine learning, especially in sequence modeling tasks (e.g. time series forecasting, language modeling), SSMs\\noffer several advantages:\\nEfficiency: The convolutional formulation allows for fast evaluation via the Fast Fourier Transform (FFT), reducing the\\ncomplexity fromO(L 2)(as in the attention mechanism) toO(LlogL),Lbeing the length of the input signalu(k);\\n1See Section 2 and Appendix A for more details on this.\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9255d13a-8103-479e-9e6a-62e29cfc8164', embedding=None, metadata={'page_label': '3', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Long-range Dependencies: By construction, the state of the system contains a compressed representation of the\\nsystem’s past history. In the context of sequence modeling, the input u(k) corresponds to the tokens provided to the\\nmodel, meaning that the state is capable of “remembering” all previously seen tokens in a compact form;\\nInterpretability and Structure: The matrices A(k), B(k), C(k)explicitly define the system dynamics, providing both\\nstructure and, ideally, interpretability.\\nRecent neural architectures such as S4 [19] and Mamba [20] leverage these ideas, combining them with modern training\\ntechniques and nonlinearities to build powerful sequence models. In particular, S4 uses a time-invariant model, namely\\na model of the form (1) where, however, the matrices A, B, Care independent of k, while the main novelty of S6, the\\ncore module of Mamba, is that it uses time-varying matricesA(k), B(k), C(k).\\nIn the context of sequence modeling, a state-space model as in Equation (1) is used as black box in foundation models.\\nThe exogenous input u(k) represents the k-th input token fed into the model, the state x(k) stores relevant information\\nabout the tokens up the k-th one, while y(k) is the k-th output: for generative or prediction models y(k) is used for the\\nprediction ofu(k+ 1); more generally,y(k)is the signal used to solve the task at hand.\\nIn general, the matrices A, B, Care learned using a stochastic gradient descent. Computational efficiency and\\ninitialization are crucial in this context, therefore, the matrices A, B, Coften have a specific structure, giving rise to the\\nconcept of structured state space models. For example, the matrixAis typically chosen to be diagonal.\\n3 COFFEE: Towards Context-Selective SSMs\\nIn this section, we introduce a novel state space model which we call COFFEE (COntext From FEEdback). The\\nstate-of-the-art S6 SSM, proposed in [20], enables selectivity conditioned on the current token by adapting the system\\ndynamics to the current input. In contrast, the main novelty of COFFEE is to replace thistoken-based selectivitywith\\ncontext-based selectivity, achieved through a feedback mechanism that modulates the dynamics based on thestateof\\nthe model.\\nBoth COFFEE and S6 are implemented within the core module architecture described in Appendix E, which maps\\nsequences over a finite vocabulary V into embeddings in RD, and processes each embedding feature with a dedicated\\nsingle-input single-output (SISO) state space model. To motivate our modifications, we now briefly review the structure\\nof the S6 SISO component (a more detailed analysis is available in Appendix B).\\n3.1 Background: The S6 Model\\nAs mentioned above, in S6 each feature u(k)i of the embedding u(k)∈R D is handled independently by a dedicated\\nSISO state-space model Σ(i). This design allows the architecture to scale across the embedding dimension by running\\nDparallel SISO models.\\nThe discrete-time update for each feature is:\\n(\\nx(k)(i) =e A(i)∆i(k)x(k−1) (i) +\\n\\x00\\nA(i)\\x01−1\\x00\\neA(i)∆i(k) −I\\n\\x01\\nB(k)u(k) i,\\ny(k)(i) =C(k)x(k) (i), (2)\\nwhere x(k)(i) ∈R n is the hidden state of the i-th feature, A(i) = diag(λ(i)\\n0 , . . . , λ(i)\\nn−1)∈R n×n is a diagonal transition\\nmatrix, andu(k) i denotes thei-th feature of the embedding vectoru(k).\\nThe input-to-state and state-to-output mappings are defined as\\nB(k) =W Bu(k), C(k) = (W Cu(k))⊤,\\nwith WB, WC ∈R n×D shared across all features. Similarly, the step sizes ∆i(k) are determined from the current input\\nas the components of the vector\\n∆(k) =ζ(W Du(k)),(3)\\nwhereW D ∈R D×D andζ(·)denotes the softplus function.\\nA distinctive aspect of S6 is its token-based selectivity: the parameters B(k), C(k), and ∆(k) depend directly on\\nthe current token u(k). This enables the model to dynamically amplify or suppress contributions from different\\ninputs, effectively filtering out irrelevant or noisy symbols while focusing on semantically meaningful ones. Such\\ninput-conditioned adaptation is particularly advantageous in sequential domains (e.g., speech, where disfluencies such\\nas “uh” carry little meaning). This token-dependent, time-varying structure is widely regarded as the core innovation\\nthat underpins the strong empirical performance of Mamba and its descendants.\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3161df0f-ccb5-4708-8642-8cfa502cc28b', embedding=None, metadata={'page_label': '4', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.2 The COFFEE Model\\nThe central novelty of COFFEE is the use ofstate feedbackto achieve context-based selectivity. Unlike S6, where the\\nupdate gates ∆i(k) depend only on the current input token, COFFEE computes them as functions of the internal state,\\nallowing the model to adapt its dynamics based on the accumulated context. It is well known and very intuitive that\\nopen-loop systems are fragile and sensitive to model uncertainties and disturbances, whereas closed-loop feedback\\nensures robustness; COFFEE exploits this principle to achieve context-based selectivity, improving robustness and\\ncoherence in generative modeling.\\nBeyond feedback, we further refine the S6 formulation through two steps: (i) linearizing the exponential dynamics to\\nmake the role of∆(k)more interpretable, and (ii) eliminating parameter redundancy via changes of basis in the input\\nand state spaces. A complete derivation of COFFEE is presented in Appendix C; below we summarize the key steps\\nleading to the proposed model.\\nStep 1. Simplified Dynamics via Linearization.In S6, the step size ∆i(k) controls the tradeoff between retaining\\npast state and incorporating new input. However, its action is embedded within matrix exponentials, which complicates\\ndirect analysis. To make this dependence more explicit and obtain a more interpretable formulation, we approximate\\neA(i)∆i(k) with its first-order Taylor expansion around∆ i(k) = 0. This yields the dynamics:\\n\\x1ax(k)(i) =\\n\\x00\\nI+A (i)∆i(k)\\n\\x01\\nx(k−1) (i) + ∆i(k)B(i)u(k)i,\\ny(k)(i) =C (i)x(k)(i), (4)\\nwhere B(i) ∈R n×1 and C(i) ∈R 1×n are time-invariant, feature-specific parameters. The latter choice on the input\\nand output matrices is motivated by the fact that, due to discretization, the input-to-state mapping in (2) is already\\nfeature-specific, while we additionally assign feature-specific C(i) to maintain symmetry and let selectivity be governed\\nsolely by the step size∆ i(k).\\nBy replacing exponentials with affine dependence on ∆i(k), the simplified model in (4) preserves the essential gating\\nmechanism of S6 while rendering the dynamics simpler and more amenable to mechanistic interpretation.\\nStep 2. Feedback for Context-Based Selectivity.A central strength of S6 lies in its token-based selectivity: the\\nupdate gates ∆i(k) are computed as functions of the current input token u(k), so that the model can suppress irrelevant\\ninputs and amplify informative ones. However, token-based selectivity is limited because identical tokens may play\\nvery different roles depending on context. For example, if we ask for an approximation of the number “111.11”, where\\neach digit “1” is represented by the same token, the model must treat different occurrences of “1” differently depending\\non their position.\\nTo address this limitation, we observe that the hidden state x(k−1) (i) of each subsystem Σ(i) already encodes a\\ncompressed representation of the full history up to step k−1 , and therefore provides a natural source of contextual\\ninformation. We therefore replace the token-based gating (3) with astate-dependent gating rule:\\n∆i(k) =σ\\n\\x00\\nw(i)\\nD ⊙x(k−1) (i)\\x01\\n,(5)\\nwhere w(i)\\nD ∈R n is a learnable parameter vector, ⊙ denotes the Hadamard product, and σ(·) is the sigmoid function\\napplied componentwise to vectors.2 By doing so, each subsystem adapts its update gate from its past history rather than\\nthe raw input token, enabling context-based selectivity: identical tokens can be filtered differently depending on their\\nrole in the evolving sequence. Concretely, the gating vector ∆i(k)∈R n enters the dynamics in (4) as a diagonal state\\nfeedback matrix, which modulates the contribution of the past state through\\n(I+A (i)diag(∆i(k)))x(k−1) (i),\\nand the input through\\ndiag(∆i(k))B(i)ui(k).\\nThe resulting subsystem dynamics is therefore context-adaptive, taking the form\\nΣ(i) =\\n\\x1ax(k)(i) = (I+A (i)diag(∆i(k)))x(k−1) (i) + diag(∆i(k))B(i) u(k)i,\\ny(k)(i) =C (i)x(k)(i). (6)\\n2We use the sigmoid rather than the softplus, since its bounded range keeps ∆i(k) limited, helping maintain the validity of the\\nTaylor approximation in (4).\\n4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6572269b-efac-4a24-b002-7b9031ba7a62', embedding=None, metadata={'page_label': '5', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Remark1 (Parameter efficiency).Compared to S6, the proposed feedback formulation is significantly more parameter-\\nefficient: whereas S6 requires a dense WD ∈R D×D, our version replaces it with D vectors w(i)\\nD ∈R n, i.e., Dn\\nparameters in total. Since typically n≪D (e.g., n= 16 , D= 2048 in Mamba3), this corresponds to a reduction from\\n4.2M to just32K gating parameters.\\nStep 3. Eliminating Redundancy via Change of Basis.As a final step, we note that the parameterization of\\n(B(i), C(i)) in (6) exhibits parameter redundancy: through suitable changes of basis (corresponding to simple rescalings)\\nthe same input-output behavior can be preserved while reducing the number of parameters. This fact is formalized in\\nthe following proposition, whose proof is deferred to the Appendix (see Section C.3 where a few comments are also\\nprovided on the proposition assumptions).\\nProposition 1(Redundancy Elimination by Change of Basis).Assume that at least one embedding vector has all the\\nentries that are non-zero and that all the entries of the B(i) are non-zero. For the family of subsystems {Σ(i)}D−1\\ni=0\\ndefined in Eq. (6), there exist changes of basis in the input and state spaces such that the structure of Eq. (6) is preserved\\nand:\\n(i) all input-to-state matrices B(i) can be fixed to the constant vector1= [1, . . . ,1]⊤, with their scaling absorbed\\ninto the correspondingC (i), and\\n(ii) one embedding vector can be normalized to1.\\nThe transformations in Proposition 1 preserve both the input-output map and the diagonal structure ofA(i). Consequently,\\nthe input-to-state matrices B(i) are redundant, as their effect can always be absorbed into C(i), and one embedding\\nvector can be fixed to 1. This reduces the parameter count by nD+D without loss of expressivity, yielding a leaner\\narchitecture that is easier to train.\\nFinal COFFEE Model.Combining Steps 1-3, the resulting COFFEE subsystem for featureitakes the form\\nΣ(i) =\\n\\x1ax(k)(i) = (I+A (i)diag(∆i(k)))x(k−1) (i) + ∆i(k)u(k) i,\\ny(k)(i) =C (i)x(k)(i), (7)\\nwhere the state-dependent gates ∆i(k) are given in Eq. (5). Here, the learnable parameters are the diagonal elements\\n{λ(i)\\nk }n−1\\nk=0 of the matrix A(i), the output matrix C(i) ∈R 1×n, and the feedback vector w(i)\\nD ∈R n. To ensure numerical\\nstability during training and inference, the choice of the parameters λ(i)\\nk is constrained to the interval [−2,0] , which\\nguarantees that the matrixI+A (i) is stable. A schematic representation of the resulting model is shown in Fig. 1.\\nRemark2 (Scalable parallel training).By construction, A(i) is diagonal, and each component of ∆i(k) depends only\\non the corresponding component of x(k−1) (i). As a result, the state dynamics in (7) decomposes into n independent\\nscalar recursions. This in turn implies that the state-transition Jacobian of (7)\\nJ(k)(i) := ∂x(k)(i)\\n∂x(k−1) (i)\\nis diagonal at every time step. The diagonal structure of the Jacobian drastically reduces both memory and computational\\nrequirements. In particular, as shown in [33], trajectory computation can be cast as an optimization problem, where\\ndiagonal Jacobians allow efficient parallel solvers with total work and memory scaling that are linear in the sequence\\nlength T and the state dimension n, i.e., O(T n), while the parallel depth in T is only O(logT) . The overall run-time\\ndepends on the number of solver iterations required for convergence, which is typically small in practice. This enables\\nefficient training on parallel hardware, in contrast with cubic and quadratic costs in the state dimension that arise with\\ndense Jacobians.\\nRemark3.(Model with output filtering) Notice that the model in (7), can be modified by adding other filtering\\ncapabilities on the output without compromising the possibility to parallelize the architecture. In particular, we can\\ntransform the output equation in:\\ny(k)(i) = (C(i)γ(k)(i))x(k)(i)\\nwhereγ(k) (i) ∈Ris evaluated as:\\nγ(k)(i) =σ((w (i)\\nγ )⊤x(k)(i))\\nwith w(i)\\nγ ∈R n learnable parameters. As a consequence, the model would have other nD learnable parameters.\\nMore generally, the output equation, being static, does not represent a problem for parallelization. Thus, arbitrary\\nnon-linearities (if beneficial) can be added.\\n3The reported numbers are taken from the minimal PyTorch implementation of Mamba (https://github.com/johnma2006/\\nmamba-minimal), numerically equivalent to the official release (https://github.com/state-spaces/mamba/tree/main).\\n5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='98f19c72-a3d5-4e88-bcf5-2549e1f1d447', embedding=None, metadata={'page_label': '6', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='context-based selectivity\\nCOFFEE Context-selective State Space Model\\n... ...... ...\\n... ...\\n· · ·· · ·\\n...\\n...\\n+\\n+\\n+\\nI + A(i)diag(∆i(k))\\n∆i(k)\\nw(i)\\nD\\n∆i(k) C(i)\\nu(k)i y(k)(i)\\nx(k − 1)(i) x(k)(i)\\nFigure 1:Block diagram of the i-th feature subsytem of COFFEE.Orange denotes the model signals, blue the\\nlearnable parameters, and green the selectivity or gating parameter, which is obtained from the past state through (5). By\\nfeeding this parameter back into the dynamics, the model forms a closed-loop mechanism that provides context-based\\nselectivity, enabling identical tokens to be processed differently depending on their role in the sequence.\\n4 Experiments\\nIn this section, we present numerical experiments on two primary tasks, the Induction Head (including variants) and\\nMNIST. Implementation details are provided in Appendix G and Appendix E for the Induction Head, and in Appendix H\\nfor MNIST.\\n4.1 Results on the Induction Head Task\\nIn the following, we present results from a series of experiments designed to solve the Induction Head Task, originally\\nintroduced in [34] and adopted as one of the motivating tasks in Mamba [ 20]. This task was proposed to evaluate\\nin-context learning capabilities of modern deep learning architectures. An explanation of the induction head task is\\nreported in Appendix D. In what follows, the adopted vocabulary isV={1,2,3,4,5,6,7}.\\nLet us start by comparing the performance on the induction head task between the COFFEE model and S6.\\nWe test the two models on the IH task with Lseq = 16, Ltri = 1 and Ltar = 1, where Lseq, Ltri and Ltar are the lengths of\\nthe whole sequence, the trigger and the target, respectively. The results are reported in Table 1.\\nTable 1: Comparison between S6 and COFFEE on the IH task. Different learning rate were adopted for both S6 and\\nCOFFEE, the best ones are reported.\\nModel n\\n(state dimension)\\nη\\n(learning rate)\\nD\\n(embedding dimension) Loss Accuracy\\nS6 8 0.003 16 0.852 0.68\\nCOFFEE 8 0.01 16 0.000 0.99+\\nAt this point, the two models have exactly the same number of parameters (i.e. #parameters=784). Our model is able\\nto reach 0.99+ accuracy in just one epoch, namely with only 5’120’000 training sequences. The S6 performance is\\nobtained by running the training algorithm for 100 epochs, namely 512’000’000 training sequences.\\n6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ddb88f4e-329d-4a58-9531-7e580228fea8', embedding=None, metadata={'page_label': '7', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='To test the limits of COFFEE we add an extra difficulty by reducing the state and embedding dimensions. Given\\nthe performance of S6 in the previous simplified scenario, we do not provide a comparison with COFFEE in more\\ncomplicated versions of the IH task reported in Table 2.\\nTable 2: COFFEE model performance with reduced state and embedding dimensions\\nModel n\\n(state dimension)\\nη\\n(learning rate)\\nD\\n(embedding dimension) Loss Accuracy\\nCOFFEE 1 0.01 9 0.000 0.99+\\nOur model has only 99 learnable parameters. The results are obtained within seven epochs, leading to 35’840’000\\ntraining sequences.\\nTo further assess the capabilities of our model, we test it on a variation of the induction head task. The standard\\nimplementation of the induction head task, as explained in Appendix D, assumes a sequence with the following\\nstructure:\\nnoise∥trigger∥target∥noise∥trigger\\nwhere∥denotes sequence concatenation. We modify the above structure in:\\nnoise∥trigger∥noise∥target∥noise∥trigger\\nWe evaluate our model in this scenario withLseq = 16,L tri = 1andL tar = 1and we obtain the results in Table 3.\\nTable 3: COFFEE model performance on this modified version of the IH task\\nModel n\\n(state dimension)\\nη\\n(learning rate)\\nD\\n(embedding dimension)\\nLnoise\\n(noise length) Loss Accuracy\\nCOFFEE 8 0.003 16 1 0.000 0.99+\\nCOFFEE 8 0.003 16 2 0.002 0.99+\\nIn the above table, “noise length\" denotes the length of the noise subsequence between the trigger and target subse-\\nquences.\\nWe now test our model on the induction head task withLseq = 16, Ltri = 1 and Ltar = 2. Results are reported in Table 4.\\nTable 4: COFFEE model performance withL tri = 1andL tar = 2\\nModel n\\n(state dimension)\\nη\\n(learning rate)\\nD\\n(embedding dimension) Loss Accuracy\\nCOFFEE 8 0.01 16 0.004 0.99\\nSimilarly, withL seq = 16,L tri = 2andL tar = 1we obtain the results in Table 5.\\nTable 5: COFFEE model performance withL tri = 2andL tar = 1\\nModel n\\n(state dimension)\\nη\\n(learning rate)\\nD\\n(embedding dimension) Loss Accuracy\\nCOFFEE 8 0.003 16 0.660 0.78\\nTo conclude, we test our model on the induction head task with Lseq = 32,64,128,256 , Ltri = 1 and Ltar = 1. The\\nresults and settings are reported in Table 6.\\n7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='542fd52f-9b8f-4382-8845-5643b9c42aa0', embedding=None, metadata={'page_label': '8', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 6: COFFEE model performance with increased sequence length\\nModel Lseq\\n(sequence length)\\nn\\n(state dimension)\\nη\\n(learning rate)\\nD\\n(embedding dimension) Loss Accuracy\\nCOFFEE 32 8 0.01 16 0.000 0.99+\\nCOFFEE 64 8 0.01 16 0.001 0.99+\\nCOFFEE 128 8 0.01 16 0.000 0.99+\\nCOFFEE 256 8 0.01 16 0.001 0.99+\\nConsidering that S6 is the first SSM to introduce selectivity and is regarded as the state of the art, we conclude that our\\nmodel deserves attention and further investigation. Based on the results above, COFFEE appears to be significantly\\nmore effective compared to the S6 model.\\n4.2 Results on the MNIST task\\nTo further assess our model ability to capture dependencies between tokens, we tested our SSM on the MNIST dataset.\\nThe adopted architecture and the hyperparameters setting are fully explained in Appendix H. In Table 7 we report\\na summary of the results obtained: in the table, “COFFEE with output filtering” refers to the usual COFFEE model\\nimproved with filtering capabilities on the output (see Remark 3).\\nTable 7: Performance comparison between COFFEE and S6 using the MNIST dataset\\nModel Epochs n\\n(state dimension) # parameters Loss Accuracy\\nS6 100 2 5585 1.891 28.2%\\nS6 100 16 10085 1.876 28.6%\\nCOFFEE 100 2 3385 0.107 96.6%\\nCOFFEE with output filtering 100 2 3585 0.100 97.0%\\nThis experiment demonstrates the practical applicability of COFFEE in a scenario that is very different from those\\nconsidered in the previous section. As discussed in [20], selectivity is a fundamental property of powerful architectures\\nfor sequential modeling. In the previous section we established the relevance and efficiency of the selection mechanism\\nintroduced in COFFEE for tasks in the induction head family; with the results summarized in Table 7, we show how\\nthis embedded mechanism transfers to a completely different and more complex task, without incurring a prohibitive\\nincrease in model size.\\nIn summary, we demonstrate that our model is flexible and able to outperform the current S6 model in significantly\\ndifferent tasks: for these reasons, we believe it deserves consideration as a building block in more complex architectures,\\nsuch as Mamba [20].\\n5 Insights on the functioning principles of COFFEE\\nOne of the major challenges in deep learning is related to the mechanistic interpretability of the learned models, the latter\\nbeing essential towards the safe and responsible application of these tools in critical domains. In order to understand\\nhow these models encode and process information, the first crucial step is to gain awareness of the internal mechanisms\\nand limitations of trained mathematical models.\\nOur COFFEE model has been designed with a few basic, interpretable functioning principles in mind:\\nP1:The state represents the memory of the system, which is propagated to the next iteration by the state matrix.\\nP2: The update gates ∆i(k) select if and how much of the input u(k)i is transferred to the state dynamics and\\nhence to memory.\\nP3: Different areas of the state space allow the next encoded symbol to affect its future evolution differently\\n(context selectivity).\\nNext, we investigate whether the proposed model is indeed capable of operating by exploiting the previous rules. An\\nin-depth analysis, aimed at identifying the key principle at play, is possible only by considering small models with\\n8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='63613d2d-08b7-4913-9478-c14e7fa9d922', embedding=None, metadata={'page_label': '9', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='limited degrees of freedom and simple tasks. Even the simple architecture that solved the inductive head task, with only\\nninety-nineparameters (see Section 4), turned out to be impractical to study due to the large dimensionality of the space\\nand the lack of convenient visualization of the state and model parameter dynamics. A further recurring difficulty is\\npresented by the fact that a model that is general enough to solve a wide spectrum of tasks can, in general, solve a given\\nproblem in many different ways, using inequivalent emerging “strategies”.\\nWe therefore decided to reduce the “complexity” of the induction head task as much as possible, so that we could\\ngeometrically represent the evolution of the state and more easily identify the choices of parameters that can implement\\nthe principles described above. In this way, we can verify that the learned solution is indeed behaving as expected. The\\nsimplified inductive head task (IH0, described in detail in Section F.1) considers sequences of four symbols chosen in\\nV={1,2,3}, where 1 represents the trigger. The trigger appears two times in each sequence, one of which as the last\\nsymbol. To solve this problem, we chose embeddings in R2, and a COFFEE model composed of two one-dimensional\\nSSM, so that the evolution can be depicted inR 2 as well.\\nIn order to highlight the role of the functioning principlesP1-3above, we introduce further model simplifications:\\n• PrincipleP1is implemented by choosing the state matrix to be the identity (i.e. all λ(i) = 0 in Equation 7.),\\nthus implementing a perfect memory. Furthermore, we also assume C(i) = 1 for i= 1,2 , so the state values\\ncorrespond to the outputs, and simplify interpretation.\\n• To implementP2, the update gates ∆i(k) are chosen as functions of the corresponding state features: Given\\ntheir definition in (5), the sigmoid functions play a key role. By choosing w(i)\\nD = 1 in our reduced model, the\\nstate value remains the only relevant parameter. High positive values of the state components correspond to\\nupdate gates close to one, and negative values bring the update gates close to zero.\\n• RegardingP3, in the simplified model the state belongs to R2, and its components directly activate the\\ncorresponding sigmoids. Loosely speaking, a state in the upper half-plane will cause the next input to “save”\\nits second component (feature) in memory, and the right half-plane will memorize the first one. The first\\nquadrant will save both, and the third quadrant neither.\\nWith these simplifications in place, the only parameters left to learn are the embeddings. Taking into account the\\nprevious observations and the fact that the inductive head task consists of memorizing the symbol following the\\nfirst trigger, we can intuitively guess the general form of a successful embedding. In particular, thetrigger symbol\\nembedding shall move the state in a region of activation of the sigmoid that is high and neutral with respect to the\\nembedding of the target symbols. Natural candidates are vectors on the bisector of the first quadrant. The embeddings\\nof thetarget symbolsshall (i) be well distanced, in order to ensure distinguishability of the output; (ii) move the state\\nfrom the triggered position to an area that is closer to their position than the others, so that their symbol is chosen as the\\nfinal output; (iii) lower the activation of the sigmoid, and hence the update gates, so that further inputs will not cause\\nthe state to move too much towards the embedding of different symbols. Natural candidates for the simplified problems\\nare large vectors in the third quadrant.\\nWe would like to verify that the learned solutions for the IH task respect these principles. Once all the proposed model\\nsimplifications are implemented, the training phase does not always end successfully due to the extremely limited\\nnumber of parameters to be optimized (6, corresponding to the embedding), and a suitable initialization must be\\nconsidered. With this in mind, we trained the simplified model starting from a tentative embedding that satisfies the\\ndesign principles sketched above, where:\\n1→[6,6] ⊤\\n2→[−10,−1] ⊤\\n3→[−1,−10] ⊤.\\nFrom this initial condition, the learning phase converges extremely quickly, and the learned model solves the task with\\n100%accuracy. The learned solution corresponds to the embedding:\\n1→[5.394,5.3433] ⊤\\n2→[−10.2643,−1.5753] ⊤\\n3→[−1.5395,−10.3404] ⊤.\\nThe full trajectories of the state variables, as they react to input and context, are depicted in Figure 2 in the Appendix.\\nTheir behavior is as expected, in accordance with principlesP1-3.\\nIn light of this, the question “Is the automatically learned solution exploiting the same mechanism and degrees of\\nfreedom we describe inP1-2-3?\" can be answered positively, at least in the case of the simplified model composed of\\n9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='08354a7a-c059-40f6-813d-deea8c987820', embedding=None, metadata={'page_label': '10', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='perfect memories (integrators). In the general case, the introduction of nontrivial state dynamics induces a drift field\\nwhich further complicates the analysis, and feedback matrices wD influence nontrivially the geometry of the activation\\nareas, and yet a general agreement with the proposed functioning principles can be seen, albeit not as clearly.\\n6 Conclusion\\nIn this work we propose a new context-selective state-space model (COFFEE) in which a state-feedback mechanism is\\ndesigned to obtain a significant advantage in the model expressivity with a low number of trainable parameters while\\nremaining suitable for parallel implementation. In the tests conducted, the COFFEE model largely outperforms S6 in\\nboth the induction head and the MNIST when the two are used within the same simple architecture. So far, COFFEE\\nis still a proof of principle, since its performance in more complex scenarios (as, for example, the long-range arena)\\nhas not yet been tested. Of course, these scenarios require a Mamba-like architecture with several COFFEE modules.\\nNonetheless, the accuracy that is achieved with a single COFFEE module and a limited set of parameters is remarkable.\\nWe are currently working on incorporating the COFFEE module in a suitable architecture, and compare it with Mamba\\nand other, non state-space models on more challenging and diverse benchmarks. The numerical feasibility and efficiency\\nof such implementation is granted by the chosen structure of COFFEE: since ensures a diagonal Jacobian, it remains\\namenable to parallelization (using the techniques recently proposed in [33]).\\nReferences\\n[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and\\nIllia Polosukhin. Attention is all you need.Advances in neural information processing systems, 30, 2017.\\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\\ntransformers for language understanding. InProceedings of the 2019 conference of the North American chapter of\\nthe association for computational linguistics: human language technologies, volume 1 (long and short papers),\\npages 4171–4186, 2019.\\n[3] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.arXiv preprint\\narXiv:2303.08774, 2023.\\n[4] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi\\nWang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.arXiv\\npreprint arXiv:2501.12948, 2025.\\n[5] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,\\nAndrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly capable multimodal models.arXiv\\npreprint arXiv:2312.11805, 2023.\\n[6] Tao Ge, Si-Qing Chen, and Furu Wei. Edgeformer: a parameter-efficient transformer for on-device seq2seq\\ngeneration.arXiv preprint arXiv:2202.07959, 2022.\\n[7] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.arXiv preprint\\narXiv:2004.05150, 2020.\\n[8] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip\\nPham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences.Advances in\\nneural information processing systems, 33:17283–17297, 2020.\\n[9] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer.arXiv preprint\\narXiv:2001.04451, 2020.\\n[10] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter\\nHawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers.arXiv\\npreprint arXiv:2009.14794, 2020.\\n[11] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidimensional\\ntransformers.arXiv preprint arXiv:1912.12180, 2019.\\n[12] Rudolf E Kalman. A new approach to linear filtering and prediction problems.Journal of Basic Engineering,\\n82(1):35–45, 1960.\\n[13] Thomas Kailath.Linear Systems. Prentice-Hall, 1980.\\n10', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b39244f3-8158-4737-86ed-96b5b7f30a6d', embedding=None, metadata={'page_label': '11', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"[14] Mihajlo D. Mesarovi´c. Systems theory and biology—view of a theoretician. In M. D. Mesarovi ´c, editor,Systems\\nTheory and Biology, pages 59–87, Berlin, Heidelberg, 1968. Springer Berlin Heidelberg.\\n[15] Geoffrey M. Hodgson. Economics and systems theory.Journal of Economic Studies, 14(4):65–86, 04 1987.\\n[16] Anders Lindquist and Giorgio Picci.Linear Stochastic Systems: A Geometric Approach to Modeling, Estimation\\nand Identification. Springer Berlin, Heidelberg, 2015.\\n[17] Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. Simplified state space layers for sequence\\nmodeling.arXiv preprint arXiv:2208.04933, 2022.\\n[18] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces.\\nAdvances in neural information processing systems, 35:22982–22994, 2022.\\n[19] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state spaces.\\narXiv preprint arXiv:2111.00396, 2021.\\n[20] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.arXiv preprint\\narXiv:2312.00752, 2023.\\n[21] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham\\nDe. Resurrecting recurrent neural networks for long sequences. InInternational Conference on Machine Learning,\\npages 26670–26698. PMLR, 2023.\\n[22] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus.\\nLiquid structural state-space models. InInternational Conference on Learning Representations, 2023.\\n[23] Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms through structured\\nstate space duality. InInternational Conference on Machine Learning, 2024.\\n[24] Luca Zancato, Arjun Seshadri, Yonatan Dukler, Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew\\nTrager, Alessandro Achille, and Stefano Soatto. B'MOJO: Hybrid state space realizations of foundation models\\nwith eidetic and fading memory. InAdvances in Neural Information Processing Systems, 2024.\\n[25] Jindong Jiang, Fei Deng, Gautam Singh, Minseung Lee, and Sungjin Ahn. Slot state space models.Advances in\\nNeural Information Processing Systems, 37:11602–11633, 2024.\\n[26] Nicola Muca Cirone, Antonio Orvieto, Benjamin Walker, Cristopher Salvi, and Terry Lyons. Theoretical\\nfoundations of deep selective state-space models.Advances in Neural Information Processing Systems, 37:127226–\\n127272, 2024.\\n[27] Rom Parnichkun, Stefano Massaroli, Alessandro Moro, Jimmy T.H. Smith, Ramin Hasani, Mathias Lechner,\\nQi An, Christopher Re, Hajime Asama, Stefano Ermon, Taiji Suzuki, Michael Poli, and Atsushi Yamashita.\\nState-free inference of state-space models: The *Transfer function* approach. InInternational Conference on\\nMachine Learning, 2024.\\n[28] T. Konstantin Rusch and Daniela Rus. Oscillatory state-space models. InInternational Conference on Learning\\nRepresentations, 2025.\\n[29] Naoki Nishikawa and Taiji Suzuki. State space models are provably comparable to transformers in dynamic token\\nselection. InInternational Conference on Learning Representations, 2025.\\n[30] Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, and Philippe Rigollet. A mathematical perspective on\\ntransformers.Bulletin of the American Mathematical Society, 62(3):427–479, 2025.\\n[31] Álvaro Rodríguez Abella, João Pedro Silvestre, and Paulo Tabuada. The asymptotic behavior of attention in\\ntransformers.arXiv preprint arXiv:2412.02682, 2024.\\n[32] Carmen Amo Alonso, Jerome Sieber, and Melanie N Zeilinger. State space models as foundation models: A\\ncontrol theoretic overview. In2025 American Control Conference (ACC), pages 146–153, 2025.\\n[33] Xavier Gonzalez, Andrew Warrington, Jimmy T Smith, and Scott W Linderman. Towards scalable and stable\\nparallelization of nonlinear rnns.Advances in Neural Information Processing Systems, 37:5817–5849, 2024.\\n[34] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,\\nAmanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads.arXiv preprint\\narXiv:2209.11895, 2022.\\n[35] Diederik P Kingma and Jimmy Ba. A method for stochastic optimization.arXiv preprint arXiv:1412.6980,\\n1412(6), 2014.\\n[36] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. Hippo: Recurrent memory with optimal\\npolynomial projections.Advances in neural information processing systems, 33:1474–1487, 2020.\\n11\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1d2f85e6-9b55-4221-9d54-e082d66f81f0', embedding=None, metadata={'page_label': '12', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[37] Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. Mnist handwritten digit database. http://yann.\\nlecun.com/exdb/mnist/, 1998. Accessed: 2025-08-12.\\n[38] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus).arXiv preprint arXiv:1606.08415, 2016.\\n[39] Faris Badawi, Anders Lindquist, and Michele Pavon. A stochastic realization approach to the smoothing problem.\\nIEEE Transactions on Automatic Control, 24(6):878–888, 1979.\\n[40] Augusto Ferrante and Giorgio Picci. Minimal realization and dynamic properties of optimal smoothers.IEEE\\nTransactions on Automatic Control, 45(11):2028–2046, 2000.\\n[41] Chai Wah Wu. Prodsumnet: reducing model parameters in deep neural networks via product-of-sums matrix\\ndecompositions.arXiv preprint arXiv:1809.02209, 2018.\\n12', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='50739713-8f12-4cc8-85aa-65aad6185bb1', embedding=None, metadata={'page_label': '13', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Useful Concepts of State Space Models\\nIn the following, a brief introduction tostate space modelsis provided. The specific outline is given below:\\n1. Modeling approaches for dynamical systems\\n2. Formal definitions and change of bases on linear time invariant state space models.\\n3. Discretization.\\nA.1 Modeling Approaches\\nMathematical models for dynamical systems have been studied for centuries. Two common approaches to modeling the\\nrelation between input and output of such systems are:\\n1. Input-output view: In this approach, the system is directly characterized by its input-output relation. The\\nlatter is typically described by means of a (high order) differential equation.\\n2. State-space view: This approach introduces an auxiliary variable to model the system: thestate. The latter\\nprovide a complete description of the configuration of the system at each time instant. As a consequence, the\\nstate at present time contains all the information about the past of the system that is relevant for its future\\nevolution. The input-output view is still present, but now the state acts as a “mediator” between the input and\\nthe output.\\nState space models are widely used in numerous fields, including mathematics, engineering, economics, and biology,\\nand have recently been employed in the deep learning field to maintain a compressed representation of the context\\nprovided by previously seen tokens, where the tokens take on the role of the inputs.\\nFrom now on, we focus only on state space models in which the state is defined over a finite-dimensional vector space.\\nA.2 Definitions and changes of basis\\nIn this section, we aim to provide a brief introduction to linear state-space models. Both continuous- and discrete-time\\nstate-space models are defined along with a relation between the two. We also define the important property of algebraic\\nequivalence.\\nDefinition1.Alinear, continuous-time state space modelis a representation of a system in which the input u(t) and the\\noutputy(t),t∈R, are continuous-time signals and their relation is given by:\\n\\x1a˙x(t) =A(t)x(t) +B(t)u(t)\\ny(t) =C(t)x(t) (8)\\nwhere the continuous-time, vector-valued signal x(t)∈R n is calledstateof the model. The matrix A(t)∈R n×n is\\ncalledstate transition matrix, B(t)∈R n×m is calledinputmatrix (or input to state matrix) and C(t)∈R p×n is called\\noutputmatrix (or state to output matrix).\\nAlinear, discrete-time state space modelis a representation of a system in which the input u(k) and the output y(k),\\nk∈Z, are discrete-time signals and their relation is given by:\\n\\x1ax(k) =A(k)x(k−1) +B(k)u(k)\\ny(k) =C(k)x(k) (9)\\nAgain the discrete-time, vector-valued signal x(k)∈R n is calledstateof the model. Matrix dimensions and names are\\nthe same as those used for the model in (8).\\nWe refer to n as the state space dimension, while m and p are referred to as the input and output space dimensions,\\nrespectively. The first equation in (8) is a first order differential equation (or a set of first order differential equations for\\nn >1) describing how the state evolves over time. Once the state evolution is determined, the second equation in (8)\\nyields the evolution of the variables of interest modeled by y(t). Linear state-space models are often represented as\\ntriples in the formΣ = (A(k), B(k), C(k))(Σ = (A(t), B(t), C(t))for continuous-time models).\\nThe interpretation of the equations in (9) is with the difference that, given the discrete-time nature, the state evolution\\nmust obey a difference equation (or a set of difference equations).\\nIf the matricesA, B, Care independent oft(or ofkin discrete-time), the model is said to betime-invariant.\\nThere is an inherent redundancy in the state space description because we can arbitrarily select the basis in the state space\\nRn and this choice does not affect the input-output relation of the model. As a consequence, there are infinitely many\\n13', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='686d17bb-87bc-41e0-91ce-d524ac645187', embedding=None, metadata={'page_label': '14', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='state space models corresponding to the same input/output dynamics. Two linear models4 Σ1 = (A1(k), B1(k), C1(k))\\nand Σ2 = (A2(k), B2(k), C2(k)) differing only for the choice of basis in the state space are said to bealgebraically\\nequivalent. A simple and direct computation shows that, in this case, there exists a non-singular matrix T∈R n×n such\\nthat A2(k) =T −1A1(k)T , B2(k) =T −1B1(k) and C2(k) =C 1(k)T . The matrix T induces the change of basis and\\nis such that the statesx 1(t)ofΣ 1 andx 2(t)ofΣ 2 are related byx 1(t) =T x2(t).\\nWe can also perform a change of basis in the input or in the output space. Of course, these changes affect the input-output\\nrelationship. However, in the setting of this work, this is not a problem. In fact, the input sequence is made up of tokens\\nwhose embedding in Rm is learned so that it can absorb the selection of the basis. The latter transforms the model\\nby only changing B(k) to B(k)V where V∈R m×m is the (non-singular) matrix inducing the change of basis in the\\ninput space. A similar argument holds for the change of basis in the output space which transforms the model by only\\nchanging C(k) to L−1C(k) where L∈R p×p is the (non-singular) matrix inducing the change of basis in the output\\nspace.\\nA.3 Discretization\\nConsider a continuous-time state space model Σc = (A(t), B(t), C(t))whose input u(t) is piece-wise constant on\\nintervals of the same length T, so that for all t1, t2 ∈[kT,(k+ 1)T) , with k∈Z , u(t1) =u(t 2). In this case, we\\ncan concentrate the input information into a new discrete-time signal ud(k) :=u(kT) and by integrating the first of\\nEquations (8) for each interval [kT,(k+ 1)T) , we obtain a discrete-time model where the discrete state and output are\\nobtained by sampling the original state and output at the instantskT:x d(k) :=x(kT),y d(k) :=y(kT).\\nIn the case of a time-invariant continuous-time models, we can easily derive closed-form expressions for the discrete-time\\nmodelΣ d = (Ad, Bd, Cd)which is time-invariant as well:\\n\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\nAd =e AT\\nBd =\\nZ T\\n0\\neAτ Bdτ=A −1 \\x00\\neAT −I\\n\\x01\\nB\\nCd =C\\n(10)\\nwhere the last equality in the expression ofB d holds in the case of non-singular state matrixA.\\nB The S6 state space model\\nSSMs have recently been explored as efficient sequence models, motivated by their ability to capture long-range\\ndependencies with favorable scaling, and as a structured alternative to attention [32, 19, 18, 17, 21]. Early work focused\\non linear time-invariant (LTI) models, which are computationally efficient but limited in expressivity: their fixed\\nrecurrence forces every token to influence the state, preventing the model from ignoring irrelevant inputs.\\nThe S6 model [ 20] addresses this by introducing linear time-varying (LTV) dynamics. The state matrices\\n(A(k), B(k), C(k))depend on the input embedding, enabling token-dependent gating of information. This design, mo-\\ntivated by tasks such asinduction headandselective copy, allows the model to update memory selectively. Unlike LTI\\nSSMs, the time-varying recurrence cannot be expressed as a convolution, precluding fast FFT-based implementations.\\nTo remain efficient, [20] proposed a hardware-aware parallel algorithm that minimizes GPU memory traffic, enabling\\nscalable training and inference.\\nTo describe the model more concretely, consider its input representation. S6 operates on a multi-dimensional array\\n[B, L, D], with batch size B, sequence length L, and embedding dimension D. Each L×D matrix is an embedded\\nsequence, where columns correspond to embedding features. The model processes these features independently by\\ninstantiating D parallel SISO SSMs: the i-th SSM receives thei-th feature across all tokens. Each feature thus maintains\\nits own state, while the recurrences are coupled through shared token-dependent matrices B(t), C(t)and adaptive step\\nsizes∆ i(t), enabling selective memory updates.\\nFormally, for each featurei= 1, . . . , D, the model defines a SISO state space system:\\nΣ(i) =\\n\\x1a\\n˙x(t)(i) =A (i)x(t)(i) +B(t)u(t) i\\ny(t)(i) =C(t)x(t) (i)\\nwhere\\n4Here we refer to discrete-time models but everything can be repeatedverbatimfor the continuous-time case.\\n14', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='11b1e97c-8279-4938-b542-7f501673e55a', embedding=None, metadata={'page_label': '15', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='•x(t) (i) ∈R n is the hidden state associated with feature i. The dimension n is shared across features and\\ntreated as a tunable hyperparameter.\\n•u(t) i denotes the i-th coordinate of the token embedding u(t)∈R D, which acts as the scalar input for the i-th\\nSSM.\\n•A (i) ∈R n×n is the transition matrix, chosen diagonal with learnable entries, and independently parameterized\\nfor each feature.\\n•B(t), C(t)∈R n are input- and output-mapping vectors, shared across features but dependent on the entire\\ntoken embedding:\\nB(t) =W Bu(t)⊤, C(t) = (W Cu(t)⊤)⊤,\\nwith WB, WC ∈R n×D learnable. This design makes the recurrence token-dependent, allowing the model to\\ngate information based on content.\\n•y(t) (i) represents the feature-level output at timet.\\nNote that this is a continuous-time formulation. In practice, the typical sequence inputs, such as text, are inherently\\ndiscrete. A standard way to bridge this gap is to view u(t) as a piecewise constant signal derived from the discrete\\nembeddings, and then apply a discretization procedure. In S6, the chosen approach is zero-order hold (ZOH), which\\nadmits closed-form discrete-time matrices (see Sec. A.3).\\nIn the S6 model, discretization is applied independently to each feature-specific system Σ(i). Importantly, the sampling\\ninterval is not fixed butinput-dependent: for each tokenu(t), the model defines a vector of adaptive step sizes\\n∆(t) =ζ(W Du(t)⊤)∈R D,(11)\\nwhereW D ∈R D×D is learnable and the softplusζ(·)ensures positivity. Each∆ i(t)controls the discretization of the\\ni-th feature’s SSM. Rather than being literally interpreted as different sampling times, these values can be understood as\\nfeature-specific gates that balance the influence of the past state and the current input when forming the update.\\nThe resulting discrete-time system for featureiis\\nΣ(i) =\\n\\x1a\\nx(k)(i) =e A(i)∆i(k)x(k−1) (i) +\\n\\x00\\nA(i)\\x01−1\\x00\\neA(i)∆i(k) −I\\n\\x01\\nB(k)u(k)i\\ny(k)(i) =C(k)x(k) (i) (12)\\nIn S6,A (i) is chosen diagonal with entriesλ (i)\\nj and the previous expression simplifies to\\nΣ(i) =\\n\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\nx(k)(i) =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\neλ(i)\\n0 ∆i(k)\\n...\\neλ(i)\\nn−1∆i(k)\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fbx(k−1) (i) +\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n(eλ(i)\\n0 ∆i(k)−1)B0(k)\\nλ(i)\\n0\\n...\\n(e\\nλ(i)\\nn−1∆i(k)\\n−1)Bn−1(k)\\nλ(i)\\nn−1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nu(k)i\\ny(k)(i) = [C0(k)···C n−1(k)]x(k) (i)\\n(13)\\nThe S6 parameterization is designed to enable input-dependent selectivity. The input and output mappings are token-\\ndependent,\\nB(k) =W Bu(k)⊤, C(k) = (W Cu(k)⊤)⊤,\\nwhich allows the model to decide at each step whether to admit information into the state or propagate it to the output\\n[20].\\nSelectivity is further controlled by the adaptive step sizes :\\n∆(k) =ζ(W Du(k)⊤), k∈Z.\\nIn the discrete update (Eq. (13)) if the eigenvalues λ(i)\\nj are in the left half of the complex plane (that is, they correspond\\nto stable modes in continuous time),∆ i(k)interpolates between two extremes:\\n•∆ i(k)→ ∞: past context is forgotten, the state depends only onu(k)i;\\n•∆ i(k)→0: the input is ignored, the state is preserved.\\nThis mechanism provides fine-grained control over memory and update.\\nFinally, stability is ensured by constraining the eigenvalues ofA(i) to be negative via\\nλ(i)\\nj =−e µ(i)\\nj ,\\nwithµ (i)\\nj unconstrained.\\n15', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c0170ab7-172d-42b0-9575-556a4e4ee299', embedding=None, metadata={'page_label': '16', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='C Extended Derivations for COFFEE\\nOur formulation builds on the S6 model (Appendix B). The derivation proceeds in three steps:\\n1. Linearize the exponential in Equation (12) via a first-order Taylor expansion to clarify the role of ∆(t) and to\\nenhance the mechanistic interpretability.\\n2. Incorporate state feedback to enable context-dependent selectivity.\\n3. Remove parameter redundancy through changes of basis in the state and input spaces.\\nC.1 Simplified Dynamics via Linearization\\nExpanding the exponential in Equation (12) around∆ i(k) = 0gives\\nΣ(i) =\\n(\\nx(k)(i) =\\n\\x00\\nI+A (i)∆i(k)\\n\\x01\\nx(k−1) (i) +\\n\\x00\\nA(i)\\x01−1\\x00\\nI+A (i)∆i(k)−I\\n\\x01\\nB(k)u(k)i,\\ny(k)(i) =C(k)x(k) (i),\\nwhich simplifies to\\nΣ(i) =\\n(\\nx(k)(i) =\\n\\x00\\nI+A (i)∆i(k)\\n\\x01\\nx(k−1) (i) + ∆i(k)B(k)u(k) i,\\ny(k)(i) =C(k)x(k) (i). (14)\\nIn S6, the matrices B(k), C(k)are shared across all subsystems and depend on the input embedding. However, after\\ndiscretization, the effective input-to-state mapping:\\n\\x00\\nA(i)\\x01−1\\x00\\neA(i)∆i(k) −I\\n\\x01\\nB(k),\\ninvolves A(i), making it inherently feature-specific. This motivates assigning each subsystem its own time-invariant\\nmatrices B(i) and C(i). We therefore equip each subsystem with feature-specific B(i) and C(i), both time-invariant.5\\nThis yields:\\nΣ(i) =\\n\\x1ax(k)(i) =\\n\\x00\\nI+A (i)∆i(k)\\n\\x01\\nx(k−1) (i) + ∆i(k)B(i)u(k)i\\ny(k)(i) =C (i)x(k)(i), (15)\\nwhere B(i) ∈R n is a column vector, C(i) ∈R n a row vector, and A(i) ∈R n×n diagonal. Since (15) is derived via\\nTaylor expansion, the model is accurate if the∆ i(k)remain small. Following S6, we set\\n∆(k) =σ(W Du(k)⊤), W D ∈R D×D.\\nParameter count.The resulting model has3nD+D 2 parameters:\\n•nDfrom{B (i)},\\n•nDfrom{C (i)},\\n•nDfrom the diagonals of{A (i)},\\n•D 2 fromW D.\\nThis matches S6, which has nD parameters in each of WB and WC, nD for {A(i)}, and D2 for WD, again totaling\\n3nD+D 2. We remark that these counts refer only to the SSM block. The full architecture (presented in Appendix E)\\nincludes an additional |M|D parameters from the embedding matrix, for a total of 3nD+D 2 +|M|D , where M is\\nthe vocabulary size.\\nC.2 Feedback for Context-Based Selectivity\\nOur model introduces context-based selectivity by letting the update gates depend on the hidden state, rather than the\\nraw input token. In S6, gating is computed as:\\n∆(k) =ζ(W Du(k)⊤), W D ∈R D×D.\\n5Time variation arises solely through∆ i(k).\\n16', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e3565399-388a-412d-9bdb-f1ccd9a2d9b8', embedding=None, metadata={'page_label': '17', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='where we recall that ζ(·) is the softplus function. In contrast, we replace this token-based rule with a state-feedback law:\\n∆i(k) =σ\\n\\x00\\nw(i)\\nD ⊙x(k−1) (i)\\x01\\n, w (i)\\nD ∈R n,(16)\\nwhere gating is conditioned on the compressed context encoded in the hidden state of each subsystem Σ(i). In addition\\nto introducing context-dependent selectivity, this change reduces the gating parameters fromD2 in S6 to Dn. Since\\ntypically n≪D , this corresponds to a considerable reduction in the number of gating parameters. The state-dependent\\ngating∆ i(k)is incorporated into the subsystem dynamics (15) as\\nΣ(i) =\\n\\x1ax(k)(i) = (I+A (i)diag(∆i(k)))x(k−1) (i) + diag(∆i(k))B(i) u(k)i,\\ny(k)(i) =C (i)x(k)(i). (17)\\nC.3 Eliminating Redundancy via Change of Basis\\nThe number of parameters in (17) can be reduced through a suitable change of basis. This is formalized in Proposition 1\\nin the main text; here we provide the proof.\\nProof of Proposition 1.Let j be the index of the embedding vectorv(j) whose entries v(j)\\ni are all nonzero, and consider\\nthe family of systems in Equation (17). We rewrite this family as\\n(\\nx(k)(i) =\\n\\x00\\nI+A (i)diag(∆i(k)))\\n\\x01\\nx(k−1) (i) + diag(∆i(k)))[B(i)v(j)\\ni ][u(k)i/v(j)\\ni ],\\ny(k)(i) =C (i)x(k)(i). (18)\\nIt is therefore apparent that the j-th embedding vector can be normalized to 1 a the price of rescaling each B(i) by\\nmultiplying each of its entries by v(j)\\ni . This is indeed the simplest example of change of basis in the input space which\\nis simplyR.\\nNow we are ready for a change of basis on the state space which does not affect the input/output behavior of the state\\nmodel. We select the change of basis induced by the matrix Ti := diag\\n\\x00\\nB(i)\\n0 v(j)\\ni , . . . , B(i)\\nn−1v(j)\\ni\\n\\x01\\n, with B(i)\\nk being the\\nk-th entry of the vector B(i). This transformation does not affect the state transition matrix but normalizes the input\\nmatrix to diag(∆i(k)))1= ∆ i(k). Finally, after the change of basis, the output matrix becomes (B(i))⊤v(j)\\ni ⊙C (i),\\nwhere⊙denotes the Hadamard product.\\nNotice that assuming that each entry of B(i) is non-zero is very reasonable as if one entry, say the k-th, is zero it means\\nthat the k-th component of the state is not affected by the input and hence it can be eliminated without affecting the\\ninput/output relationship of the model. The fact that at least one of the embeddings has all entries which are non-zero is\\na very reasonable assumption, as the embedding parameters are learned.\\nIn summary, the change-of-basis argument reveals two sources of redundancy:\\n(i) One embedding vector can be fixed arbitrarily (e.g., to 1) without altering the input-output behavior, which reduces\\nthe embedding parameters from|M|Dto(|M| −1)D;\\n(ii) B(i) and C(i) need not be learned independently: any rescaling of B(i) can be absorbed into C(i), so only their\\nproduct matters. One set can therefore be fixed, savingnDparameters.\\nAfter applying all the steps described above, the resulting COFFEE dynamics is given in Equation (7) of the main text.\\nThe overall parameter count decreases from3nD+D 2 +|M|Din S6 to3nD+ (|M| −1)Din our model.\\nD Induction Heads Task\\nThe term “induction heads” is used by [34] to denote a type of attention heads in the Transformers architecture. These\\nheads learn the ability to complete a sequence by memorizing the symbol (or symbols) following a certain subsequence\\ncalled “trigger” and by producing such a symbol whenever the trigger appears again. In [ 34], strong evidence is\\npresented that supports the assumption that this ability might be one of the fundamental mechanisms for \"in-context\\nlearning\". For this reason, such an ability is a benchmark to assess to what extent the models are capable of identifying\\npatterns within sequences.\\nD.1 Task description\\nConsider a sequence of symbols extracted from a certain finite vocabularyV. In our setting, we select\\nV={1,2,3,4,5,6,7}\\n17', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='90267b1a-7665-46be-89a6-0d760a107863', embedding=None, metadata={'page_label': '18', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='To test the model’s ability to capture patterns, the model is presented with a sequence of arbitrary length that follows\\nthe structure outlined below:\\nnoise 1∥trigger∥target∥noise 2∥trigger (19)\\nwhere ∥ denotes sequence concatenation, and noise 1, noise 2, trigger, and target indicate subsequences of symbols.\\nEach sequence is constructed in such a way that the same trigger subsequence appears exactly twice. The two noise\\nsubsequences may contain the target subsequence, but they must not include the trigger subsequence. The trigger\\nsubsequence serves to indicate the beginning of a pattern to be reproduced. Upon encountering the trigger subsequence,\\nthe model is expected to recognize that it must “pay attention\" to the subsequent elements, calledtargetsubsequence,\\nand retain this information. While the trigger subsequence is fixed, the target subsequence varies across sequences.\\nWe consider the task successfully completed when the model outputs the target subsequence after observing the final\\ntrigger subsequence in Equation (19). This outcome would demonstrate that the model has correctly learned the pattern\\n“trigger∥target\".\\nWhen the sequence length is fixed, the following steps are necessary to build an appropriate sequence:\\n1. Choose the length of the trigger subsequence\\n2. Choose a fixed trigger subsequence with symbols fromV\\n3. Choose and fix the length of the target subsequence\\nRemark4.Fixed trigger subsequence means that once it is selected (e.g. randomly), it must be used consistently to\\ngenerate all training, validation, and test sequences. In other words, in all training, validation and test sequences, the\\ntrigger subsequence is always the same in both of its occurrences in Equation (19).\\nLet us denote by Lseq, Ltri, and Ltar the lengths of the sequence, trigger, and target subsequences, respectively. These\\nlengths should be chosen such that:\\nLseq −2L tri −L tar ≥1\\nto ensure that there is at least one of the two “noise” subsequences, consisting of at least one element. In our experiments,\\nwe considered sequences constructed according to the following combinations:\\n1.Triggerandtargetsubsequences both of length one\\n2.Triggersubsequence of length two andtargetof length one\\n3.Targetsubsequence of length two andtriggerof length one\\nOnce the trigger and the target length are fixed, to generate admissible sequences we randomly select the position of\\nthe initial symbol of the first “trigger” subsequence, the target subsequence and the two noise subsequences. We use\\nsuitable uniform discrete independent random variables for all these selections with the caution of discarding sequences\\nin which at least one of the target and the noise subsequences contains the trigger.\\nRemark5.Whenever the length of the target subsequence is greater than 1, extra caution is required. In fact, in this\\ncase, when the model encounters the last trigger subsequence, it can output only one symbol and cannot complete the\\ntarget subsequence. To address this issue, we pad the input sequence by concatenating it with the padding sequence\\nmade of Ltar −1 copies of the special symbol “0”. For example, an admissible sequence with Ltri = 3 and Ltar = 3\\ncould be:\\n3,2,6,| {z }\\nnoise\\n5,6,7| {z }\\ntrigger\\n2,4,3,| {z }\\ntarget\\n1,2,2,6,| {z }\\nnoise\\n5,6,7,| {z }\\ntrigger\\n0,0|{z}\\npadding\\nIn this way, the model can observe the entire trigger subsequence and produce the complete target subsequence. Upon\\nencountering the last symbol, “7\", the correct first output would be “2\", and the subsequent zeros would allow the model\\nto also output the rest of the target subsequence, i.e.,“4\" and “3\".\\nAll the generated input sequences are transformed, by the usual embedding process, into sequences of vectors. The\\nlatter are used as inputs to a state-space model. Specifically, given a linear discrete-time state-space model\\n\\x1ax(k) =Ax(k−1) +Bu(k)\\ny(k) =Cx(k) +Du(k)\\nembedding vectors of symbols are the variousu(k)fork= 0, ..., Lseq −1. For example, given the sequence:\\n3,2,6,| {z }\\nnoise\\n5,6,7| {z }\\ntrigger\\n2,4,3,| {z }\\ntarget\\n1,2,2,6,| {z }\\nnoise\\n5,6,7,| {z }\\ntrigger\\n0,0|{z}\\npadding\\n18', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='955b0f2c-d6c3-45f2-8351-985aba9285f3', embedding=None, metadata={'page_label': '19', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='u(0)is the embedding of the symbol “3\",u(1)is the embedding of the symbol “2\", and so on.\\nWe finally emphasize that this task can be defined over any set V . In our case, we used V={1,2,3,4,5,6,7} , but one\\ncould choose a larger set or a vocabulary consisting of different symbols, such as V={“cat\",“dog\", ...} . The elements\\nof the vocabulary are symbols, and the relevant thing is to highlight their mutual relations.\\nE Core Module Architecture\\nWe now provide a description of the core module architecture employed to address the induction head task. A detailed\\naccount of all steps involved in data processing is presented. The operations performed by the core module in our setup\\nare as follows:\\n1. The core module receives a sequence or a batch of sequences as input. Each sequence is built according to the\\nrules presented in Appendix D. The input can be regarded as an array of dimension [B, L], where B denotes\\nthe batch size andLthe length of each sequence within the batch. 6\\n2. Each symbol is mapped to a real-valued vector, referred to as theembedding vector. The dimensionality of the\\nembedding space is a user-defined hyperparameter and can be adjusted according to the specific requirements\\nof the task. Formally, the embedding procedure consists in applying the followinglearnedmap:\\nemb() :M→R D\\nwhere M is the model’s vocabulary while D denotes the dimension of the embedding space and sometimes is\\nalso calledmodel dimension.\\nRemark6.Notice that, the domain of the emb() map is M. Since we assume that V⊆M (V is the vocabulary\\nused to build sequences), this transformation is well-defined: it guarantees that every symbol in any input\\nsequence has a corresponding embedding vector.\\nAfter applying this mapping, we obtain a three-dimensional array with shape [B, L, D], where D denotes the\\nembedding dimensionality.\\n3. The resulting sequence of embedding vectors is fed into a state space model, which contains additional\\nlearnable parameters.\\n4. For each input (i.e. the embedding vector of a symbol) the model outputs another vector of the same\\ndimension, specifically a vector in RD. After processing all input sequences, the state space model produces a\\nmultidimensional array of shape [B, L, D]. For each batch index b= 0, . . . , B−1, this corresponds to an\\nL×Dmatrix containing all output vectors associated with the input sequence.\\n5. In general, the outputs produced by the state space model do not correspond exactly to the embedding vectors\\nassociated with symbols in M. However, the ultimate objective is for the model to generate symbols from the\\nset M. To achieve this, the first step involves computing the distance between each output vector and every\\nembedding vector corresponding to symbols in M. This operation results in a multidimensional array of shape\\n[B,L,|M|].\\nAfter having evaluated the distances we have:\\nDistances =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nd(0)\\n0,0 ···d (0)\\n0,|M|−1\\n... ... ...\\nd(0)\\nL−1,0 ···d (0)\\nL−1,|M|−1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n...\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nd(B−1)\\n0,0 ···d (B−1)\\n0,|M|−1\\n... ... ...\\nd(B−1)\\nL−1,0 ···d (B−1)\\nL−1,|M|−1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nwhere d(b)\\nl,m denotes the distance between the output vector at time l of batch b and the embedding vector of the\\nsymbolm∈M, for everyl= 0, . . . , L−1,b= 0, . . . , B−1, andm∈M.\\n6More generally, we use the notation [d1,..., dl] to indicate an array having l dimensions, where the first dimension is d1 and the\\nl-th dimension isd l\\n19', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6dc4e749-4e36-472f-b067-c5133174472c', embedding=None, metadata={'page_label': '20', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6. We now apply the softmin 7 function along the rows of the matrices within the multidimensional array\\nDistances. This operation produces another multidimensional array, denoted as Softmin, which has the same\\ndimensions asDistances, namely [B,L,|M|].\\nBy applying the softmin function to the rows of Distances array, we obtain a probability distribution where\\nthe smallestd (b)\\nl,m for everyl= 0, ..., L−1,b= 0, ..., B−1andm∈Mhas the maximum probability.\\nThe resulting matrix after this step is:\\nSoftmin =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\ns(0)\\n0,0 ···s (0)\\n0,|M|−1\\n... ... ...\\ns(0)\\nL−1,0 ···s (0)\\nL−1,|M|−1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n...\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\ns(B−1)\\n0,0 ···s (B−1)\\n0,|M|−1\\n... ... ...\\ns(B−1)\\nL−1,0 ···s (B−1)\\nL−1,|M|−1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n7. After applying the softmin function, we then apply an element-wise transformation known as the logit 8\\nfunction. The logit function maps probabilities from the interval(0,1)to the entire real lineR.\\nIn this manner, we obtain another multidimensional array, which we denote as Logits. Each entry is given by\\nLogitsk,i,j = logit(Softmink,i,j). Naturally, the Logits array has the same dimensions as Softmin, namely\\n[B,L,|M|]. The resulting matrix after this step is:\\nLogits =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nlogit(0)\\n0,0 ···logit (0)\\n0,|M|−1\\n... ... ...\\nlogit(0)\\nL−1,0 ···logit (0)\\nL−1,|M|−1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n...\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nlogit(B−1)\\n0,0 ···logit (B−1)\\n0,|M|−1\\n... ... ...\\nlogit(B−1)\\nL−1,0 ···logit (B−1)\\nL−1,|M|−1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n8. To conclude, the core module architecture produces as output two multidimensional arrays. The first is exactly\\nLogits. The second is a multidimensional array of dimensions [B, L], obtained by applying the arg maxalong\\nthe last dimension of Logits. This means that we compute the arg max over all rows of every matrix within\\nthe multidimensional array Logits. The resulting array of shape [B, L] is denoted as Predictions. Formally,\\nthis matrix is constructed as follows:\\nPredictions =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\narg maxm∈M logit(0)\\n0,m ···arg max m∈M logit(0)\\nL−1,m\\n...\\narg maxm∈M logit(B−1)\\n0,m ···arg max m∈M logit(B−1)\\nL−1,m\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\nThis matrix, for each row (i.e. for each batch), contains the predicted symbols for all time stepsl= 0, . . . , L−1.\\nNotice that, as a consequence of the previous operations, taking the arg max corresponds to selecting, for\\n7Given a vector x= (x 1, x2, . . . , xn)∈R n, the softmin function produces another vector softmin(x)∈R n whose entries are\\ndefined by\\nsoftmin(x)i = e−xi\\nPn\\nj=1 e−xj\\n, i= 1, . . . , n.\\n8The logit function is defined as the inverse of the sigmoid function. For a real numberp∈(0,1), it is given by:\\nlogit(p) = log\\n\\x12 p\\n1−p\\n\\x13\\n20', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='20353428-6b7e-4ff9-b400-712fa9431b21', embedding=None, metadata={'page_label': '21', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='every batch and every time step, the symbol whose embedding vector is closest (with respect to the Euclidean\\nnorm) to the output produced by the state space model.\\nRemark7.The multidimensional array Predictions can be obtained in multiple ways. For eachl= 0, . . . , L−\\n1, the predicted symbol mpred ∈M is the one closest to the output vector of the state-space model. Therefore,\\ntaking the arg min over the rows of the matrices in the Distances array, or the arg max over the rows of the\\nmatrices in theSoftminorLogitsarrays, will yield the samePredictionsmatrix.\\nWe use cross-entropy loss as the loss function, which requires the logits as input. Therefore, the Logits array\\nis used for computing the loss, while thePredictionsarray is used to evaluate the accuracy.\\nThe core module architecture is summarized in the following block diagram:\\nInput sequence\\nEmbedding\\nState space model\\nDistances evaluation\\nsoftmin applied to rows\\nlogit applied entry-wise\\nLogits Predictions\\nFirst output Second output\\nF Towards a mechanistic interpretation of COFFEE: auxiliary material\\nF.1 Simplified Version of the Induction Head Task: IH0\\nTo take a first step toward a mechanistic interpretability we simplify the induction head task. The solution of the\\nproblem by hand presents several challenges related to the embedding and state dimension, the sequence length, and the\\nnumber of symbols in the vocabulary.\\nFor this reason, we focus on addressing a toy version of the induction head task. We set:\\n1. Embedding dimensionD= 2and state dimensionn= 1\\n2. Sequence lengthL seq = 4\\n3. V ocabulary made of three symbolsV={1,2,3}\\nAn embedding in R2 allows us to employ only two one-dimensional state-space models. It is hence possible to visualize\\nthe trajectories of both states on a Cartesian plane. The choice of setting Lseq = 4 and using a vocabulary consisting of\\nonly three symbols, significantly reduced the number of possible sequences that could be generated according to the\\nrules described in Appendix D.\\nBy specilizing COFFEE as in Equation (7) forD= 2andn= 1, we obtain the following setting:\\nΣ(i) =\\n\\x1a\\nx(k)(i) = (1 +λ (i)∆i(k))x(k−1) (i) + ∆i(k)u(k)i\\ny(k)(i) =C (i)x(k)(i) (20)\\n21', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8b4438eb-4a4d-4ff6-be43-be83ea2bb8bd', embedding=None, metadata={'page_label': '22', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='with:\\n∆i(k) =σ(W (i)\\nD x(k−1) (i))\\nwithi= 0,1;x(k) (i) ∈RandW (i)\\nD ∈R. The possible sequences are now of the form:\\ntrigger∥target∥noise∥trigger or noise∥trigger∥target∥trigger\\nwhere, the trigger, target and noise subsequences are all of length one, resulting in a total sequence length of Lseq = 4.\\nWith these choices, the number of possible input sequences is reduced to just 8, allowing us to check the validity of a\\nproposed model by inspection.\\nF.2 COFFEE simplified model and state evolution for IH0\\nWe illustrate how the reduced COFFEE model, introduced to illustrate the functioning principles of the method, is able\\nto solve IH0 by depicting the learned encoded symbols and the state evolution corresponding to a given input sequences.\\nBy setting λ(i) = 0 and C(i) = 1 for all i= 0,1 , eachstate x(k)(i) becomes equal to the corresponding output y(k)(i),\\nand the system behaves as a perfectintegrator. This is desirable to our aim since it implments a perfect dynamical\\nmemory in absence of inputs. The model thus becomes:\\nΣ(i) =\\n\\x1a\\nx(k)(i) =x(k−1) (i) + ∆i(k)u(k)i\\ny(k)(i) =x(k) (i) (21)\\nwith:\\n∆i(k) =σ(W (i)\\nD x(k−1) (i))\\nwhere x(k)(i) ∈R and W(i)\\nD ∈R for i= 0,1 . We set W(i)\\nD = 1 for i= 0,1 , in order to allow for more direct\\ninterpretability of the effect of state feedback in this setting.\\nThe only parameters to be learned at this point are embedding vectors for the symbols in the vocabulary V={1,2,3} .\\nAccording to the principlesP1-P3described in Section 5, we start with the embedding parameters initialization:\\n1→[6,6] T\\n2→[−10,−1] T\\n3→[−1,−10] T .\\nRecall that, without loss of generality, symbol “1” is assumed to be the trigger. The final, learned solution is:\\n1→[5.394,5.343] T\\n2→[−10.264,−1.575] T\\n3→[−1.539,−10.340] T .\\nSuch solution can provably solve correctly the problem with 100% accuracy. This can be verified by inspection, since\\nthe possible input sequences are just 8.\\nIn fact, with these parameter choices and the introduced simplifications, it is manageable to visualize how the possible\\ninput sequences are processed by the simplified model. All vectors are represented inR 2,and we usez 1, z2 to denote\\nthe abscissa and ordinate, respectively (we avoid using x, yto avoid confusion with the state x(k) and the output y(k)).\\nSpecifically, the outputs of Σ(0) generate the z1-coordinates of the output vectors, while the outputs of Σ(1) generate\\nthez 2-coordinates.\\nA picture illustrating how the input sequence “1∥2∥3∥1” is processed is provided next, which also illustrates the intuitive\\nmotivation for the particular choice of parameters. It is instructive to compare it with the trajectory associated to the\\nsequence “3∥1∥2∥1”, in order to see how the different positioning of the trigger with respect to the noise influences the\\ndynamics.\\n22', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a67d0db2-5539-427a-8b8b-80f0dc402188', embedding=None, metadata={'page_label': '23', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='z1\\nz2\\n−10 −5 5\\n−10\\n5Region “2” Region “1”\\nRegion “3”\\n“1”,\\n\\x125.39\\n5.34\\n\\x13\\n“2”,\\n\\x12−10.26\\n−1.57\\n\\x13\\n“3”,\\n\\x12−1.54\\n−10.34\\n\\x13\\nk= 0\\nk= 1\\nk= 2\\nk= 3,\\n\\x12−6.92\\n−6.73\\n\\x13\\nk= 0\\nk= 1k= 2\\nk= 3,\\n\\x12−6.41\\n−5.11\\n\\x13\\nFigure 2: Embeddings (in grey) and trajectories of the state for the reduced model solving the IH0 problem, with input\\n1∥2∥3∥1 and 3∥1∥2∥1 (in blue and red, respectively). The last trigger does not move significantly the state since the\\nsigmoids in the input gates are not activated. The colored regions indicate the sections of the plane corresponding to\\neach possible choice of the final output symbol (e.g final states in Region 2 lead to output “2”).\\nIn the above diagram thearrow headsof the light blue vectors point to the output y(k)∈R 2 (and also the state\\nx(k)∈R 2, because y(k)(i) =x(k) (i), i= 0,1) of the simplified SSM model at the time steps k= 0,1,2,3 ), whose\\ncoordinates\\n\\x12\\nz1\\nz2\\n\\x13\\n∈R 2 are also provided. Vectors in gray are the embedding vectors of the symbols in V with their\\ncoordinates.\\nThe qualitative behavior for the “1∥2∥3∥1” sequence is as expected: the trigger moves the state in an activation area; the\\ntarget symbol moves it towards its own representation, where the gate activation is also lower; the subsequent symbols\\ndo not move the state outside of the correct decision area. For the other sequence, after the first noise symbol moves the\\nstate in an area of low activation for the second state variable, the dynamics plays as expected but mostly along the first\\nvariable (horizontally). All other sequences have similar behavior to the ones depicted.\\nG Results on the Induction Head Task (IH)\\nIn the subsequent paragraphs, additional details on the training procedure for the induction head task are given.\\nMoreover, initialization details are provided for both the COFFEE model and S6.\\nG.1 Experiment settings\\nWe do not rely on a predefined dataset with a fixed set of sequences. For instance, by setting the sequence length to\\nLseq = 16 and defining the vocabulary as V={1,2,3,4,5,6,7} , the total number of possible sequences that can be\\ngenerated, according to the rules described in Appendix D, exceeds several hundred billion.\\nTo address this, we developed a software tool capable of generating such sequences randomly on demand. We created\\ntwo independent instances of this tool to generate training and validation sequences.\\nAlthough it is theoretically possible for the same sequence to appear in both the training and validation sets, this is\\nextremely unlikely given that the total number of training sequences used is capped at 512 million.\\n23', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='13fd42ac-f8a0-4078-ace1-49d42b4dd0da', embedding=None, metadata={'page_label': '24', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='As the loss function, we employedcross-entropy, and for optimization, we used theAdamoptimizer [ 35]. The learning\\nrate is specified for each experiment individually.\\nIn each training iteration, we randomly generate and extract 512 sequences. This process is repeated 10’000 times,\\nwhich we define as oneepoch. The maximum number of epochs is set to 100. However, if good performance is\\nachieved before reaching this limit, the training process is stopped early.\\nAt the end of each epoch, we evaluate the model’s generalization ability on a randomly generated set of 10’000\\nsequences, computing thelossand/oraccuracy. This evaluation also allows us to save the model that performs best\\nduring training.\\nG.2 Models Initialization\\nIn this section, we provide details about the initialization of our model and the S6 model used in our experiments.\\nOur model\\nThe parameters of our model are initialized as follows:\\n1. Transition matricesA (i) ∈R n×n. These matrices are diagonal, with entries initialized to0;\\n2. State-to-output row vectors C(i) ∈R n, whose entries are initialized according to the standard normal\\ndistributionN(0,1);\\n3. The vectors W(i)\\nD ∈R n, whose entries are also initialized according to the standard normal distribution\\nN(0,1).\\nRegarding the embedding matrix, we initialize it as follows:\\n1. We generate a random matrix L∈R D×|M| with entries sampled from a uniform distribution in the interval\\n[0,1);\\n2. The matrix L is then decomposed using QR decomposition as L=QR , where Q∈R D×|M| has all the\\ncolumnsorthogonalto each other and ofnormone;\\n3. Finally, we takeQT ∈R |M|×D and set it as the initial embedding matrix. This procedure yields an orthonormal\\ninitialization of the embedding vectors.\\nS6 model\\nThe parameters in S6 are initialized as follows:\\n1. Transition matrices A(i) ∈R n×n. These matrices are diagonal, with entries initialized according to HiPPo\\ntheory [36]. In particular, the i-th diagonal entry is initialized as −(i+ 1), i= 0, ..., n−1. For more details\\nsee [20], sec. 3.6 pag. 9;\\n2. The weight matrixW B ∈R n×D is initialized according to the standard normal distributionN(0,1);\\n3. The weight matrixW C ∈R n×D is initialized according to the standard normal distributionN(0,1);\\n4. The weight matrixW D ∈R D×D is initialized according to the standard normal distributionN(0,1);\\n5. The embedding matrix inR |M|×D , is initialized according to the standard normal distributionN(0,1).\\nH MNIST Architecture and Results\\nIn this section, we describe the experimental setup used to obtain the results in Section 4.2. Specifically, we (i) describe\\nthe MNIST dataset, (ii) a limitation of the sMNIST task in evaluating SSM performance, and (iii) present the employed\\narchitecture to address this classification task.\\nH.1 MNIST Dataset\\nThe MNIST dataset [ 37] is a widely adopted benchmark in machine learning, particularly for image classification\\ntasks. It comprises 70’000 grayscale images of handwritten digits from 0 to 9, each with a resolution of 28×28 pixels.\\n24', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='378e0309-d478-49ff-94b7-56a8391072fa', embedding=None, metadata={'page_label': '25', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The dataset is split into a training set of 60’000 images and a test set of 10’000 images. Thanks to its simplicity and\\nstandardized format, MNIST serves as a common starting point for evaluating new models and algorithms.\\nIn our experiments, we construct a validation set by randomly sampling 10’000 images from the original MNIST\\ntraining set. Consequently, the dataset is partitioned into 50’000 training images, 10’000 validation images, and 10’000\\ntest images. Moreover, we did not use the full 28×28 pixel images. Instead, we opted to crop them to 25×25 pixels.\\nH.2 Architecture Description\\nIn this section, we first discuss a limitation in assessing the true performance of an SSM layer in solving thesMNIST\\ntask, a variant commonly used to evaluate state-space models performance. We then present our architecture and its\\nsubsequent developments. Our approach allows for a more accurate evaluation of the effectiveness of state-space\\nmodels.\\nH.2.1 Limitations of the sMNIST Task\\nA review of the literature shows that, in general, to assess the ability of state space models to capture long-range\\ndependencies, they are tested on a modified version of the MNIST task calledsMNIST, which stands forsequential\\nMNIST. In this task, each 28×28 pixel image is vectorized. Specifically, the image is converted into a column vector\\nin R784 by vertically stacking all columns of size R28, a method known as column-major order. Alternatively, the same\\nresult can be achieved by horizontally stacking all rows of the image to form a vector in R784, known as row-major\\norder.\\nDrawing inspiration from the literature, we initially attempted to solve thesMNISTtask using the following architecture:\\nImage as a column vector inR784\\nSingle SISO SSM layer\\nGELU non linearity\\nLinear layer:R784→R10\\narg max\\nWith this architecture, we take as input the vectorized image in R784. The image, now represented as a sequence of\\npixels, is processed by an SSM layer parameterized with our model. A GELU [ 38] activation function is applied to\\neach element of the resulting output sequence.9 After the GELU, we still have a vector in R784, which is mapped to\\nR10 by a linear layer. This linear layer produces the logits for each of the ten classes. The predicted class is the one\\nwith the highest logit, obtained by taking thearg maxover the entries of the resulting vector inR 10. Specifically,\\nˆi= arg max\\ni∈{0,...,9}\\nvi\\nwhere with ˆiwe denote the predicted class and withv i we denote thei-th entry of the vectorv∈R 10.\\nEach symbol in the input sequence represents, by construction, a pixel value. Consequently, each symbol is embedded\\nas a scalar, resulting in a model dimension of D= 1 . By training the above architecture with a single state space model\\n(sinceD= 1) with:\\n1. State dimensionn= 8;\\n9The Gaussian Error Linear Unit (GELU) activation function is defined as GELU(x) =x·Φ(x) , where Φ(x) is the cumulative\\ndistribution function (CDF) of the standard normal distribution. For more details, see [38].\\n25', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a33b7a36-8359-4c92-a4f0-53be33fae1f2', embedding=None, metadata={'page_label': '26', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.100epochs;\\n3. Batch size512;\\n4. Using an initial learning rate of η= 0.01 , which was reduced to η= 0.005 once the training loss fell below\\n0.150;\\n5. Data augmentation, using aroto-translationwith a maximum rotation of 5 degrees and a maximum translation\\nof0.01along both thexandyaxes 10,\\nwe obtained alossof 0.226 and aaccuracyof 93.6%. The employed loss function is the cross-entropy loss. To evaluate\\nthe contributions of the SSM layer and the linear layer to the observed performance, we trained the same architecture\\nwithout the SSM layer, as illustrated in the following diagram:\\nImage as a column vector inR784\\nGELU non linearity\\nLinear layer:R784→R10\\narg max\\nwe trained the architecture, again, with:\\n1.100epochs;\\n2. Batch size512;\\n3. An initial learning rate ofη= 0.01, which was reduced toη= 0.005once the training loss fell below0.150;\\n4. Data augmentation implemented with aroto-translationwith a maximum rotation of 5 degrees and maximum\\ntranslation of0.01on bothxandyaxes;\\nas a result, we obtained alossof0.284and aaccuracyof92.0%.\\nThese results suggest that in this configuration the contribution of the SSM layer to the performance is negligible\\ncompared to that of the linear layer.\\nIn light of this experiment, it becomes evident that, in order to properly assess the SSM’s ability to solve the task, the\\nuse of linear layers should be minimized. Their strong effectiveness can obscure the contributions of state space models.\\nH.2.2 Our Architecture\\nOur model is designed as a general tool for sequence processing. In general, when dealing with sequences, we take\\na sequence of symbols from some vocabulary, associate an embedding vector to each symbol, and then process the\\nsequence of embedding vectors with the model. For instance, this is done for the induction head task presented in\\nAppendix D.\\nWith the MNIST dataset, we want to adopt the same approach. As previously mentioned, we use cropped images\\nof size 25×25 pixels. We treat an image as a sequence oftwenty-fivesymbols (i.e. one for each row). For each\\nsymbol si, i= 0, . . . ,24, its embedding vector in R25 is given by the i-th row of the image. Thus, processing an image\\nis equivalent to processing a sequence of twenty-five symbols embedded in R25. To clarify this idea, consider the\\nfollowing diagram.\\n10For instance, if we set translate=(a, b) (i.e. x, y translations respectively), then horizontal shift is randomly sampled in the range\\n[−imgwidth ·a,img width ·a]and vertical shift is randomly sampled in the range[−img height ·b,img height ·b].\\n26', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='61654d75-c50f-47ef-a2c7-88559cffe807', embedding=None, metadata={'page_label': '27', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Embedding of the first symbol inR25\\n...\\nEmbedding of the last symbol inR25\\nFirst row of the image\\nLast row of the image\\ns0\\ns24\\nOverall imageSymbols\\nDrawing inspiration from the celebrated two filter formula used for the smoothing problem, see [39] and [40], we can\\nconsider the image as a sequence of symbols s0∥. . .∥s24, with each si ∈R 25, and build two sequences: one using\\nthe rows as embedding vectors and the other using the columns. Let s0∥. . .∥s24 denote the sequence created from\\nthe rows and s\\n′\\n0∥. . .∥s\\n′\\n24 the one from the columns. The index i, i= 0, . . . ,24, can now be interpreted astime. By\\nthinking of a sequence as a discrete time signal of finite support, given that we have access to all the signals in advance,\\nwe process the same image in four ways:\\n1. By rows fromi= 0toi= 24;\\n2. By columns fromi= 0toi= 24;\\n3. By rows fromi= 24toi= 0;\\n4. By columns fromi= 24toi= 0.\\nThe architecture now becomes:\\nImage by rows Image by cols Image by rows reverseImage by cols reverse\\nSSM layer SSM layer SSM layer SSM layer\\nHorizontal stack of the last outputs\\nLinear layer:R100→R25\\nGELU\\nLinear layer:R25→R10\\narg max\\nFigure 3: Proposed architecture.\\nIn this setup, we process the images by rows, columns, and by rows and columns in reverse order. Each SSM layer\\nproduces an output matrix in R25×25, from which we consider only the last row. We then take the last rows of all four\\nmatrices and stack them horizontally to obtain a vector in R100. This vector is mapped to the corresponding logits using\\ntwo linear layers separated by a GELU activation function. The predicted class is again selected as:\\nˆi= arg max\\ni∈{0,...,9}\\nvi\\n27', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='912f3ede-47cd-4066-ad46-d5e9aafa0b4f', embedding=None, metadata={'page_label': '28', 'file_name': '2510.14027v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14027v1.pdf', 'file_type': 'application/pdf', 'file_size': 550174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='where with ˆiwe denote the predicted class and withv i we denote thei-th entry of the vectorv∈R 10.\\nLet us now analyze the number of learnable parameters in this architecture:\\n1. In Appendix C, we see that an SSM layer has 3nD parameters, with n being the state dimension and D the\\nembedding dimension. In our case, we use n= 2 and D= 25 , so a single SSM layer has 150 parameters.\\nSince we have four of them, the total number of parameters for the SSM layers is600;\\n2. The first linear layer, which maps vectors from R100 to R25, has 2500 + 25 = 2525parameters: 2500 for the\\nlinear transformation and25for the bias, resulting in an overall affine transformation;\\n3. The second linear layer, which maps vectors from R25 to R10, has 250 + 10 = 260parameters: 250 for the\\nlinear transformation and10for the bias, resulting in an overall affine transformation.\\nTaking all these contributions into account, the overall architecture has3385parameters.\\nWe trained this model with:\\n1. State dimensionn= 2;\\n2.100epochs;\\n3. Batch size512;\\n4. An initial learning rateη= 0.01that was reduced toη= 0.005when the training loss fell below0.450;\\n5. Data augmentation implemented with a roto-translation with a maximum angle of 5 degrees and maximum\\ntranslation of0.01on bothxandyaxes.\\nWith these parameters, we obtain alossof0.107and aaccuracyof96.6%.\\nFor comparison with the S6 model, we trained the same architecture using the S6 model within the SSM\\nlayers instead of our model. As mentioned in Appendix C, our model has fewer parameters than S6, so the total number\\nof parameters in this case is5885. Initially, we tried with the same settings, namely:\\n1. State dimensionn= 2;\\n2.100epochs;\\n3. Batch size512;\\n4. With an initial learning rateη= 0.01that was reduced toη= 0.005when the training loss fell below0.450;\\n5. Data augmentation implemented with a roto-translation with a maximum angle of 5 degrees and maximum\\ntranslation of0.01on bothxandyaxes,\\nand we obtained alossof1.891with aaccuracyof28.2%.\\nWe conducted a second experiment by increasing the state dimension for the S6 model. We trained the\\narchitecture with:\\n1. State dimensionn= 16;\\n2.100epochs;\\n3. Batch size512;\\n4. With an initial learning rateη= 0.01that was reduced toη= 0.005when the training loss fell below0.450;\\n5. Data augmentation implemented with a roto-translation with a maximum angle of 5 degrees and maximum\\ntranslation of0.01on bothxandyaxes.\\nIn this case, the architecture has10 ′085parameters, and we obtained alossof1.876with aaccuracyof28.6%.\\nBased on these results, we can conclude that our model achieves far superior performance compared to the\\nS6 model, and this performance is obtained with fewer parameters. Moreover, our model is also competitive with the\\nhighly optimized models presented in [41], even without specific optimization.\\n28', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='278ce52f-a256-4e44-b141-2aea88589c5b', embedding=None, metadata={'page_label': '1', 'file_name': '2510.14516v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14516v1.pdf', 'file_type': 'application/pdf', 'file_size': 4321174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Vision Mamba for Permeability Prediction of\\nPorous Media\\nALIKASHEFI 1,† , TAPANMUKERJI 2\\nStanford University, Stanford, CA 94305, USA\\n1kashefi@stanford.edu\\n2mukerji@stanford.edu\\n†The corresponding author\\nAbstract:VisionMambahasrecentlyreceivedattentionasanalternativetoVisionTransformers\\n(ViTs) for image classification. The network size of Vision Mamba scales linearly with input\\nimage resolution, whereas ViTs scale quadratically, a feature that improves computational and\\nmemoryefficiency. Moreover,VisionMambarequiresasignificantlysmallernumberoftrainable\\nparameters than traditional convolutional neural networks (CNNs), and thus, they can be more\\nmemory efficient. Because of these features, we introduce, for the first time, a neural network\\nthat uses Vision Mamba as its backbone for predicting the permeability of three-dimensional\\nporous media. Wecompare theperformance ofVisionMamba withViT andCNN modelsacross\\nmultiple aspects of permeability prediction and perform an ablation study to assess the effects\\nof its components on accuracy. We demonstrate in practice the aforementioned advantages of\\nVision Mamba over ViTs and CNNs in the permeability prediction of three-dimensional porous\\nmedia. We make the source code publicly available to facilitate reproducibility and to enable\\nother researchers to build on and extend this work. We believe the proposed framework has the\\npotential to be integrated into large vision models in which Vision Mamba is used instead of\\nViTs.\\nKeywords.VisionMamba;Visiontransformer;Convolutionalneuralnetwork;Porousmedia;Permeability\\n1. Introduction and motivation\\nPorousmediaplayacentralroleacrossdiversescientificandindustrialdomains,includingdigital\\nrockphysics[1 –3],membranesystems[4,5],geologicalcarbonstorage[6,7],andmedicine[8 –10].\\nConventional investigations use numerical simulations and laboratory experiments to analyze\\nporousmediaandtoobtaintheirphysicalandgeometriccharacteristics. Althoughbothapproaches\\nare valuable, they are resource-intensive, requiring substantial computation, specialized lab\\ninstrumentation, and considerable wall-clock time. To reduce this burden, deep learning within\\nthe broader machine-learning paradigm can accelerate tasks such as segmentation of porous\\nmedia [11–14] and the prediction of porous-medium properties, including permeability [15–23],\\nporosity [24], elasticity [25,26], and effective diffusivity [27]. Moreover, deep learning\\nconfigurations can predict pore-scale fields such as velocity and pressure [28–30]. Additionally,\\ngenerativedeeplearningmodelsareusedforporous-mediareconstruction[31 –34]. Inthepresent\\nwork, we focus on predicting the permeability of porous media from digital rock images using\\nsupervised deep-learning frameworks.\\nFrom a computer-science perspective, a variety of deep-learning frameworks have long been\\napplied to permeability prediction in porous media, each with its own advantages and limitations.\\nWe briefly review these approaches and then explain how our proposed deep-learning framework\\naddresses several of their challenges while introducing new capabilities. Convolutional neural\\nnetworks (CNNs) and CNN-based variants such as ResNet [35] have been widely used to predict\\npermeability from 2D and 3D representations of porous media [16,18,19,36]. These models\\noften achieve strong accuracy with relatively simple architectures (compared with other models\\nthat will be mentioned later in this paragraph). However, they typically require a large number of\\nlearnable parameters, often more than the alternatives we will discuss later. They also operate\\narXiv:2510.14516v1  [cs.CV]  16 Oct 2025', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a9f5864a-8b86-4ba8-8d05-95232d80c1b5', embedding=None, metadata={'page_label': '2', 'file_name': '2510.14516v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14516v1.pdf', 'file_type': 'application/pdf', 'file_size': 4321174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='on fixed input resolutions; a network trained on cubes of one size generally expects test data\\nof the same size. Point-cloud neural networks, such as PointNet [37,38] and PointNet++ [39],\\nare another family of deep-learning frameworks used for permeability prediction [17]. In this\\nsetup, the boundary between pore and grain phases of the porous medium is represented as a\\npoint cloud. The main advantage of this approach compared with CNNs is that it dramatically\\nreduces the dataset size, since the full volumetric cubes are no longer needed [17]. Additionally,\\nalthough models are usually trained with the same number of points per point cloud within a\\nbatch (each batch can still contain point clouds with different numbers of points), at test time, the\\nnumber of points can vary. However, preprocessing is required to convert volumetric images\\nof porous media into point-cloud data. Furthermore, if the number of boundary points varies\\ndrastically across the dataset, the training procedure may face additional challenges, both in\\nimplementationandinloss-functionconvergence. Fourierneuraloperators(FNOs)havealsobeen\\nused for permeability prediction [23]. FNOs are invariant to input image size [40]; leveraging\\nthis property, they can be trained on porous media of different sizes simultaneously and have\\nshown strong generalizability to unseen sizes. However, FNOs can be prone to overfitting on the\\ntraining data. Another limitation is their sensitivity to hyperparameters, especially the number of\\nFourier modes, which introduces additional challenges for training and fine-tuning [23]. Vision\\nTransformers (ViTs) [41], as another deep-learning architecture, have been applied to predicting\\nthe permeability of porous media [42–44]. In several settings, ViTs achieve competitive or\\nsuperior accuracy with comparable, or sometimes fewer, trainable parameters than CNNs [44].\\nMoreover, unlike standard CNNs, vanilla ViTs with full self-attention can look across the entire\\nimage from the very first layer, so they pick up long-range patterns early; CNNs usually need\\nmany layers or operations like pooling or dilated convolutions to see that much of the image.\\nMamba[45]wasintroducedasanalternativetoTransformers[46]. Buildingonthislineofwork,\\nVisionMamba[47]wasalsointroducedasanalternativetoVisionTransformers[41]. Oneofthe\\nmain advantages of Vision Mamba over ViTs is that it scales linearly (rather than quadratically)\\nwith the number of tokens. This motivates us to propose a deep learning framework based on\\nVision Mamba for predicting the permeability of porous media. Vision Mamba has been so far\\nused for several key applications in vision tasks [47,48] such as image detection [49], medical\\nimage classification [50], remote sensing [51], ocean engineering of underwater vehicles [52],\\nmedical image segmentation [53], and medical video segmentation [54]. It is important to note\\nthat Mamba and its vision counterpart are emerging as alternatives to Transformers and ViTs\\nand are increasingly used as building blocks in large language and large vision models [55–60].\\nDemonstrating that Vision Mamba can predict properties of three-dimensional porous media is\\ntherefore significant, as it indicates a pathway to incorporating this task into future large models\\nthat perform multiple functions, including permeability estimation from volumetric data.\\nThe key contributions of this study are as follows.\\n• We introduce a novel neural network, based on the Vision Mamba architecture, for\\npredicting the permeability of voxelized porous media.\\n• The proposed network leverages Vision Mamba to achieve linear scaling with token size\\n(or similarly patch size), whereas ViTs scale quadratically.\\n• Leveraging Vision Mamba significantly reduces trainable parameters compared to CNNs,\\nimproving memory efficiency.\\n• The code and documentation are released as open-source to support reproducibility,\\neducational purposes, and future extensions of this work.\\nWe now outline the structure of the remainder of this research paper. In Sect. 2, we describe\\nthe generation and collection of three-dimensional porous media and the computation of their', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='27e00bed-9174-4fa5-a0c5-cb7e0fcc032e', embedding=None, metadata={'page_label': '3', 'file_name': '2510.14516v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14516v1.pdf', 'file_type': 'application/pdf', 'file_size': 4321174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='permeability by numerically solving the Stokes equations for the proposed supervised deep-\\nlearning framework. In Sect. 3, we present the Vision Mamba architecture, adapted to predict\\na volumetric property, here, the permeability of 3D porous media. Training of Vision Mamba\\nand the hyperparameter settings are explained in Sect. 4. We then discuss the results in Sect.\\n5, including the performance of Vision Mamba, its comparison with CNNs and ViTs, and the\\nablation studies. Finally, Section 6 provides a summary and potential directions for future\\nresearch.\\n2. Data generation\\nTo test the deep-learning framework, we synthesize voxelized porous media using the truncated-\\nGaussian construction [61,62]. Each sample occupies a cube of side length𝐿discretized on an\\n𝑛×𝑛×𝑛 grid, with morphology characterized by a target porosity𝜙(i.e., pore-volume fraction)\\nand a spatial correlation lengthℓ𝑐. To construct each volumetric sample, we follow three stages.\\nFirst, we generate a64×64×64 (i.e., 𝑛=64 ) scalar field of white noise by drawing samples\\nfrom a standard normal distribution at every voxel. Second, we impose spatial correlation by\\nconvolving the field with a three-dimensional Gaussian kernel with a standard deviation of 5.0\\nand a spatial correlation lengthℓ𝑐 =17 voxels. Third, we rescale the smoothed field to the\\ninterval [0,1] and apply a global threshold of0.45 so that values less than or equal to0.45 are\\nlabeled as pore and values greater than0.45 are labeled as grain, yielding a binary pore–grain\\nmedium. The selected threshold constrains the porosity to𝜙∈[0.125,0.200] . In the present\\nstudy, the characteristic domain length𝑙is defined as𝑙=𝑛Δ𝑥 , whereΔ𝑥denotes the physical\\nlength associated with each side of a single pixel in the discretized porous medium. For all\\nsimulations, Δ𝑥is set as0.003m, thereby setting the spatial resolution of the computational grid.\\nWe generate 1692 samples and randomly partition them into three disjoint subsets: 1353 for\\ntraining, 169 for validation, and 170 for testing. A few examples of these cubic porous media are\\nshown in Fig. 1.\\nFlow through each synthesized porous medium is driven by imposing a uniform streamwise\\npressure gradientΔ𝑝/𝑙 in the𝑥-direction. The two bounding𝑦–𝑧 faces are assigned no-slip\\nconditions. Within the pore space, we compute the steady incompressible motion using a lattice\\nBoltzmann solver [63] that resolves the Stokes system,\\n∇·u=0,(1)\\n∇𝑝−𝜇∇ 2u=0,(2)\\nwhere 𝜇isthedynamicviscosityand u and 𝑝denotethevelocityandpressurefields,respectively.\\nFrom the converged solution, the intrinsic permeability (𝑘) in the𝑥-direction is obtained via\\nDarcy’s law [64],\\n𝑘=− 𝜇𝑈𝑙\\nΔ𝑝 ,(3)\\nwith𝑈the superficial (volume-averaged) velocity evaluated over the entire sample (assigning\\nzero velocity in solid voxels). Across the dataset, the resulting permeabilities lie within\\n[20 mD,200 mD].\\n3. Vision Mamba architecture\\nIn this section, we describe the architecture of the proposed neural network, whose core is Vision\\nMamba, a selective state-space model, adapted to predict permeability from voxelized porous\\nmedia. Figure 2 illustrates the schematic of Vision Mamba, which serves as the core of the\\nproposed neural network, with the 3D porous medium cube as the input. We split the cube into', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7910e34c-c48e-4197-97a2-fcae40186d11', embedding=None, metadata={'page_label': '4', 'file_name': '2510.14516v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14516v1.pdf', 'file_type': 'application/pdf', 'file_size': 4321174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Fig. 1. Three representative examples of the synthetically generated three-dimensional\\ndigital porous media (with𝑛=64 ) used to train Vision Mamba are shown; phases\\nare color-coded with blue indicating the solid grain matrix and red indicating the pore\\nspace.\\nnon-overlapping 3D patches, and each patch is embedded into a token. A stack of Vision Mamba\\nblocks scans the embedded tokens along the depth, height, and width axes. Each scan performs\\nbidirectional state updates through forward and backward recurrences. Finally, we take a global\\naverage over space and use a linear layer to output a single permeability value.\\n3.1. Input and patchification\\nThe input to the network is a batch of generated porous media, i.e., a batch of cubes, which\\nmathematicallycanbeshownby 𝑋∈R B×1×𝐷×𝐻×𝑊 ,where Bisthebatchsizeand 𝐷, 𝐻,and 𝑊\\nare spatial dimensions. Next, we apply a patchification operator. Patchification uses a single 3D\\nconvolution with kernel size and stride equal to the patch size. In this setup, the patchification\\nproduces an𝐷′×𝐻′×𝑊′grid of patch tokens, each withCchannels. Consequently, the output\\nof the patchification operator is the token grid𝑧tok ∈R B×C×𝐷 ′×𝐻′×𝑊′\\n.\\n3.2. Vision Mamba block\\nThe token grid𝑧tok (obtained from the previous step) serves as the input to Vision Mamba. To\\nelaborateontheprocesswithinVisionMamba,wedescribeitinthreestages: token-wiseparameter\\ngeneration, selective scanning along each axis, and axis fusion with residual connections.\\n3.2.1. Token-wise parameter generation\\nIn the next step, a1×1×1 convolution reads𝑧tok ∈R B×C×𝐷 ′×𝐻′×𝑊′\\nand produces five fields of\\nsize B×C×𝐷 ′×𝐻′×𝑊′: input gate (𝑔in), output gate (𝑔out), and two state-space coefficients\\n𝐵and𝐶, as well as positive step sizeΔ. Moreover, we create a learnable vector𝐴∈R Cand a\\nskip vector𝐷skip ∈R C. In addition, we define𝑢as\\n𝑢=𝑔 in ⊙𝑧tok,(4)\\nwhere ⊙denotes elementwise product and thus𝑢∈R B×C×𝐷 ′×𝐻′×𝑊′\\n. 𝐴+ is introduced and\\ncomputed by applying the elementwise softplus function to the vector𝐴. The softplus function is\\ndefined as\\n𝜎(𝜆)=ln \\x001+𝑒 𝜆\\x01.(5)\\nNote that although𝐴+ ∈R C, it is treated as𝐴+ ∈R 1×C×1×1×1 in practice from a software\\nengineering perspective. Next,𝛼is introduced and computed as\\n𝛼=exp \\x00 −𝐴 + ⊙Δ\\x01,(6)\\nwhere𝛼∈R B×C×𝐷 ′×𝐻′×𝑊′\\n.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='99bb225e-8a43-464f-9048-34caea5e895e', embedding=None, metadata={'page_label': '5', 'file_name': '2510.14516v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14516v1.pdf', 'file_type': 'application/pdf', 'file_size': 4321174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.2.2. Selective scan per axis\\nFor each spatial axis𝜂∈{𝐷 ′,𝐻′,𝑊′}, the token grid𝑧tok is viewed as a collection of length-𝐿𝜂\\nsequences by treating that axis as an ordered time dimension and flattening the remaining indices\\ninto independent sequences. The forward selective state-space scan along axis𝜂 updates a\\nchannelwise hidden state𝑠and produces an output𝑦fwd via\\n𝑠𝑡 =𝛼𝑡 ⊙𝑠𝑡−1 +𝐵𝑡 ⊙𝑢𝑡,(7)\\n𝑦fwd\\n𝑡 =𝐶𝑡 ⊙𝑠𝑡 +𝐷skip ⊙𝑢𝑡,(8)\\nfrom𝑡=0 to𝑡=𝐿 𝜂−1. Notethatthesubscript 𝑡addedtothecomponents 𝑠,𝛼, 𝐵,𝑢,𝐶,and 𝑦fwd\\n(i.e., 𝑠𝑡, 𝛼𝑡, 𝐵𝑡,𝑢𝑡,𝐶𝑡, and𝑦fwd\\n𝑡 ) indicates that these components are reshaped to𝑁seq ×𝐿×C ,\\nand therefore{𝑠𝑡,𝛼𝑡,𝐵𝑡,𝑢𝑡,𝐶𝑡,𝑦fwd\\n𝑡 }∈R 𝑁seq×C. The value of𝑁seq depends on the scanning\\naxis. For example, when scanning along the𝐷′axis, 𝑁seq =B×𝐻 ′×𝑊′. Similarly,𝑁seq is\\ncomputed when the other two axes are scanned. Finally, a corresponding backward scan runs\\nfrom𝑡=𝐿 𝜂 to𝑡=1, producing𝑦 bwd.\\n3.2.3. Axis fusion and residuals\\nThe two directions are fused to remove directional bias,\\nˆ𝑦=1\\n2\\n\\x00𝑦fwd +𝑦bwd\\x01,(9)\\nand the result is then gated at the output:\\n𝑦𝜂 =𝑔out ⊙ˆ𝑦.(10)\\nThe outputs along the three axes are subsequently averaged to obtain the final output:\\n𝑦= 𝑦𝐷′ +𝑦 𝐻′ +𝑦𝑊′\\n3 .(11)\\nThe Vision Mamba block employs a residual connection and a pointwise MLP to mix channels\\nafter the selective scan, formulated as\\n𝑧+=𝑧tok +𝑦,(12)\\n𝑧out =𝑧++M(𝑧 +),(13)\\nwhereMdenotes a pointwise MLP implemented using1×1×1 convolutions with the Gaussian\\nerror linear unit activation function.\\n3.3. Global pooling and head\\nAfter the Vision Mamba block, a global average pooling across spatial dimensions and a linear\\nprojection to a scalar is applied. Ifℎdenotes the globally pooled representation, the predicted\\npermeability ˆ𝑘is computed as\\nℎ=mean(𝑧 out),(14)\\nˆ𝑘=𝑤 ⊤ℎ+𝑏,(15)\\nwhere𝑤is the weight vector and𝑏is a scalar bias.\\nNote that we described the architecture of a single Vision Mamba block. Multiple blocks\\ncan be stacked sequentially to construct deeper networks. For further details on the underlying\\nmethodology, we refer readers to the original Mamba formulation [65] and its vision-oriented\\nadaptation in Vision Mamba [66]. Moreover, implementation-specific details are documented in\\nour openly available GitHub repository (see the Data Availability part at the end of the article),\\nwhich includes extensive inline comments.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2b744ebf-1a62-4eea-95e2-259b7e18115b', embedding=None, metadata={'page_label': '6', 'file_name': '2510.14516v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14516v1.pdf', 'file_type': 'application/pdf', 'file_size': 4321174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1 2 3 4\\n1 5 2 6\\n1 3 2 4\\n…\\n…\\n…\\nx-axis\\ny-axis\\nz-axis\\nVision Mamba \\nBlocks\\nPermeability\\nMLP1 2\\n3 4\\nFig. 2. Schematic architecture of the proposed network based on Vision Mamba for\\ndeep learning of the permeability of three-dimensional porous media. If the input is\\na 64×64×64 porous medium (i.e.,𝑛=64 ) with a patch size of 32, there are eight\\nsubcubes,labeled1through8. Byscanningalongthe 𝑦-axis,thesubcubesarearranged\\nin the order 1, 5, 2, 6, 3, 7, 4, 8. Scanning along the𝑥 and 𝑧 directions is defined\\nsimilarly.\\n4. Parameter setup and training\\nSince 𝑛=64 , the parameters𝐷,𝑊, and𝐻are set to64. We set a patch size of8 voxels in the\\npatchification operator on643 inputs, producing an8×8×8 grid of512 tokens with embedding\\nwidth C=64 . Since the input dimensions are64 and the patch size is8, it follows that𝐷′,𝑊′,\\nand 𝐻′are 8 (because 64/8=8 ). Moreover, because𝑛=64 and the patch size equals8, it is\\nconcluded that the sequence length along each axis is𝐿𝜂 =8 (the number of pixels divided by\\nthepatchsize). Additionally,wesettheblockdepth 𝑁block to3. Theblockdepthof3( 𝑁block =3 )\\nmeans that three Vision Mamba blocks are stacked sequentially after the patch-embedding stem,\\neach applying axiswise bidirectional selective scans and residual pointwise mixing to the output\\nof the preceding block before the global pooling and linear regression head. In Sect. 5.4, we\\nreport a series of ablation studies that systematically examine how these hyperparameters affect\\nthe predictive performance of the model.\\nTraining uses mean-squared error on a min–max normalized target. Let𝑘min and 𝑘max be the\\nminimum and maximum permeability values computed on the training split. The normalized\\ntarget is\\ne𝑘= 𝑘−𝑘 min\\n𝑘max −𝑘min\\n.(16)\\nThe loss over a batch is\\nL= 1\\nB\\nB∑︁\\n𝑖=1\\n\\x00ˆ𝑘𝑖 −e𝑘𝑖\\n\\x012.(17)\\nAt evaluation time, predictions are mapped back to physical units by the inverse of Eq. 16. This\\nnormalization stabilizes optimization without imposing a hard output range; the regression head\\nremains unconstrained and learns to match the normalized scale.\\nModel training proceeds via stochastic, mini-batch gradient optimization by adopting the\\nAdam optimizer [67] with a constant learning rate of 0.001, and using mini-batches of 128\\nsamples (i.e.,B=128 ) for each parameter update [68]. To avoid overfitting, model performance', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d24182c3-8887-470d-8070-9ecf12d75d6a', embedding=None, metadata={'page_label': '7', 'file_name': '2510.14516v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14516v1.pdf', 'file_type': 'application/pdf', 'file_size': 4321174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='is continuously monitored on a held-out validation set throughout training. Convergence is\\ntypically achieved within approximately 300 epochs, at which point the final optimized model is\\nselected. All experiments are executed on a single NVIDIA A100 (SXM4) GPU equipped with\\n80 GB of memory.\\nFig. 3. Comparison of Vision Mamba and CNN performance using the𝑅2 score (left:\\nVision Mamba; right: CNN)\\n5. Results and discussion\\n5.1. General analysis\\nTo assess predictive accuracy for permeability, we employ the coefficient of determination,𝑅2.\\nFor a test set comprising𝑃 samples with ground-truth permeabilities𝑘𝑖 and corresponding\\npredictions ˜𝑘𝑖, and denoting by¯𝑘= 1\\n𝑃\\nÍ𝑃\\n𝑖=1 𝑘𝑖 the empirical mean of the ground truth,𝑅2 is\\ndefined as\\n𝑅2 =1−\\nÍ𝑃\\n𝑖=1\\n\\x00𝑘𝑖 −˜𝑘𝑖\\n\\x012\\nÍ𝑃\\n𝑖=1\\n\\x00𝑘𝑖 −¯𝑘\\x012 .(18)\\nNote that negative values of𝑅2 imply performance inferior to the trivial predictor𝑘𝑖 = ¯𝑘. We\\nalso report the root-mean-square error (RMSE), defined as\\nRMSE=\\nvut\\n1\\n𝑃\\n𝑃∑︁\\n𝑖=1\\n\\x00𝑘𝑖 −˜𝑘𝑖\\n\\x012.(19)\\nWe further report the maximum relative error over the test set by computingmax\\nn\\n|𝑘𝑖 −˜𝑘𝑖 |\\n|𝑘𝑖 |\\no𝑃\\n𝑖=1\\n.\\nThe minimum relative error is similarly defined.\\nThe performance and error analysis of the Vision Mamba model in predicting the permeability\\nof the test set (170 porous media) are summarized in Table 1. As reported, the𝑅2 score is\\n0.9969 and the root mean square error is 2.6939 mD. The maximum and minimum relative\\nerrors are 0.2708 and 0.0003, respectively. The left panel of Fig. 3 further illustrates the\\npredicted versus ground-truth permeability for all samples in the test set, highlighting the results\\nfor individual porous media. These findings demonstrate the successful training and accurate\\npredictive capability of the proposed Vision Mamba–based neural network for applications to\\nthree-dimensional porous media.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dab25574-2a10-4f33-9c5e-a5ec1317249c', embedding=None, metadata={'page_label': '8', 'file_name': '2510.14516v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14516v1.pdf', 'file_type': 'application/pdf', 'file_size': 4321174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 1.𝑅2 score, root mean square error, and minimum/maximum relative errors of\\nthe test set (170 samples) for the comparison between Vision Mamba and CNN models.\\nThe batch size (B) for both models is set to 128. We set𝑁block =3 and as well as the\\npatch size of 8 in the Vision Mamba model.\\nVision Mamba CNN\\n𝑅2 score 0.9969 0.9762\\nRoot mean square error (mD) 2.6939 7.5054\\nMinimum relative error 0.0003 0.0004\\nMaximum relative error 0.2708 0.9312\\nTraining time per epoch (s) 3.0 1.6\\nNumber of trainable parameters 195841 2582369\\n5.2. Comparison between Vision Mamba and CNNs\\nThe next step is to compare the performance of Vision Mamba with that of a CNN. For\\ncompleteness, webrieflyoutlinetheCNNarchitectureusedinthisstudy. Specifically, weemploy\\nthe same CNN model that was adopted in our previous work published in 2021 [17]. In simple\\nterms, the CNN model consists of an encoder and a decoder. In the encoder, convolutional\\nchannels that start at 16 and double at each stage. Downsampling is done with stride-2\\nconvolutions without padding. There are no pooling layers in the encoder. We use2×2×2\\nkernels, except for the last layer of the encoder, which uses a1×1×1 kernel. This final layer\\nproduces a single global latent vector of length1024. This latent vector is then passed to a\\ndecoder,implementedasamultilayerperceptron(MLP)withthreelayersofsizes512,256,and1,\\nrespectively, whichisusedtopredictthepermeability. Inboththeencoderanddecoder, Rectified\\nLinearUnit(ReLU)activationfunction(seeEq. 7inRef.[17]forthemathematicalexpressionof\\nthisfunction)isappliedastheactivationfunctionaftereachlayerexceptthefinallayer,whichhas\\nno activation. Batch normalization [69] is applied after each layer. In the decoder, dropout [70]\\nwith a probability of 0.7 is used. Similar to the Vision Mamba model, the loss function is the\\nmean squared error (Eq. 17). For additional background and implementation details on CNNs\\nfor permeability prediction in porous media, see Ref. [17].\\nThe performance and error analysis of the predicted permeability values of the test set using\\nthe CNN model are listed in Table 1, with the corresponding results illustrated in the right panel\\nof Fig. 3 for the𝑅2 score. In comparison with Vision Mamba, the CNN yields a lower𝑅2\\nscore (0.9762 vs. 0.9969), a higher root mean square error (7.5054 mD vs. 2.6939 mD), a\\nhigher minimum relative error (0.0004 vs. 0.0003), and a higher maximum relative error (0.9312\\nvs. 0.2708). It is important to emphasize that our focus here is not solely on showing that\\nVision Mamba consistently outperforms CNN in terms of prediction accuracy of porous media\\npermeability. In fact, we ensured that the CNN model was optimized to achieve its best possible\\nperformance. Nevertheless, as discussed earlier, CNN still achieves lower𝑅2 scores compared to\\nVision Mamba. Instead, our comparison primarily concerns the training time and the number\\nof trainable parameters, as summarized in Table 1. The training time (per epoch) of Vision\\nMamba is approximately 1.875 times longer than that of CNN. This can be attributed to the\\nsequential nature of Vision Mamba, which converts each three-dimensional porous medium into\\na sequence of patches and processes them serially. In contrast, CNNs process three-dimensional\\nporous media through multiple channels in parallel, where the number of channels typically\\nincreases and the kernel size decreases at deeper layers, leading to faster training. However,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='43ae12e8-d6bf-4f34-9ee4-c7d6be66964d', embedding=None, metadata={'page_label': '9', 'file_name': '2510.14516v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14516v1.pdf', 'file_type': 'application/pdf', 'file_size': 4321174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='this parallelization comes at the cost of requiring more trainable parameters and higher GPU\\nmemory consumption. Based on Table 1, the number of trainable parameters in the CNN model\\nis 2582369, whereas Vision Mamba requires only 195841 parameters, approximately a 13.2-fold\\nreduction, which is a substantial difference.\\n5.3. Comparison between Vision Mamba and ViT\\nThissubsectioncomparestheproposedmodel,basedonVisionMamba,withViTforpermeability\\npredictionofporousmedia. AbriefsummaryoftheViTarchitectureisprovidedattheendofthis\\nsubsection; here, we focus on the results. In each machine learning experiment, the number of\\ntrainable parameters between the two models is matched as closely as possible. Once the initial\\nViT design is established, the only variable across experiments is the patch size (i.e., token size).\\nThe Vision Mamba configuration follows that described previously, except that the patch size is\\nvaried. For both models, the batch size is fixed at 128 (B=128 ). Other hyperparameters and\\ntraining procedures are selected to achieve the best performance, and early stopping is applied to\\nmitigate overfitting. Table 2 reports the results for patch sizes 4, 8, 16, and 32. In both models,\\nreducing the patch size decreases the number of trainable parameters but increases GPU memory\\nrequirements. ThecharacterofthisincreasedistinguishesVisionMambafromViT.Althoughthe\\nnumber of trainable parameters remains nearly constant in both models as patch size decreases,\\nGPU memory usage grows at different rates. For ViT, the increase is so pronounced that a model\\nwith patch size 4 cannot be executed on an 80-GB NVIDIA A100 (SXM4) GPU, resulting in job\\nfailure, as reported in Table 2. Figure 4 shows the GPU memory usage per epoch as a function\\nof patch size. In this plot, the patch size is normalized by the largest patch size (i.e., 32 in the\\ncurrent case). As shown in Fig. 4, the required GPU memory per epoch increases linearly with\\ndecreasing patch size in Vision Mamba, whereas it increases quadratically with decreasing patch\\nsize in ViT. Our experimental results on three-dimensional porous media (i.e., a specific 3D\\nimage) confirm the theoretical design of Vision Mamba and ViT in terms of linear and quadratic\\nscaling with token size. As illustrated in Fig. 4, we apply a least-squares fit to derive linear and\\nquadratic equations describing the experimental GPU memory requirements. Based on these\\nequations, it is predicted that at a patch size of 4, the required GPU memory per epoch for ViT\\nwould be approximately 451 GB, which explains why this machine learning experiment could\\nnot be executed on our 80-GB GPU.\\nFigure 5 presents the permeability predictions versus the ground truth for patch sizes 8, 16,\\nand 32 using the Vision Mamba and ViT models. Based on the results reported in Table 2 and\\nthe visualizations in Fig. 5, it can be concluded that very large patch sizes reduce the accuracy of\\npermeability prediction. This effect is more pronounced for ViT, which attains an𝑅2 score of\\n0.9491 at a patch size of 32, whereas Vision Mamba maintains an𝑅2 score of 0.9817 at the same\\npatch size. Overall, Vision Mamba achieves higher accuracy and, owing to its lower memory\\nfootprint, allows exploration of smaller patch sizes. This advantage is expected to become more\\nsignificant for larger porous media (i.e., higher values of𝑛) and for media with shorter spatial\\ncorrelation lengths. According to Table 2, the ViT model generally requires less time per epoch,\\nalthough the difference from the Vision Mamba model is not substantial.\\nAttheendofthissubsection,weprovideabriefexplanationoftheViTarchitectureimplemented\\nin this study. The network partitions each64×64×64 porous medium into non-overlapping\\npatches (e.g.,8×8×8 patches when the patch size is 8) and maps each patch to a token\\nvia a three-dimensional convolutional stem whose kernel and stride are equal to the patch\\nsize, producing embeddings of dimension 64. A learned absolute three-dimensional positional\\nembedding, defined on a base token grid (e.g.,8×8×8 grid for a patch size of 8), is trilinearly\\ninterpolated to the current token grid and added to the tokens. The encoder consists of three pre-\\nnormalized Transformer blocks; in each block, tokens are normalized, processed by multi-head\\nself-attention with 8 heads, and merged back through a residual connection. This is followed by', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f5f5579d-394f-4f4f-a228-442da7c85bec', embedding=None, metadata={'page_label': '10', 'file_name': '2510.14516v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14516v1.pdf', 'file_type': 'application/pdf', 'file_size': 4321174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='another normalization, a two-layer MLP with Gaussian error unit activation and dropout, and\\na second residual addition. After the block stack, tokens are layer-normalized and aggregated\\nby global average pooling, and a linear head maps the pooled representation to a single scalar\\npermeability prediction. Further details can be found in the original Transformer [46] and Vision\\nTransformer [41] articles, as well as in our open-source code, the link to which is provided at the\\nend of this article.\\nTable 2. Performance comparison between Vision Mamba (ViM) and Vision Trans-\\nformer (ViT). Reported metrics include the𝑅2 score, root mean square error (RMSE),\\nminimumrelativeerror(MiRE),andmaximumrelativeerror(MaRE)onthetestset(170\\nsamples) for different patch sizes. See text for details of the setup for each architecture.\\nThe symbol×indicates that the corresponding machine learning experiment could not\\nbe run due to GPU memory limitations.\\nPatch size 4 8 16 32\\nModel ViM ViT ViM ViT ViM ViT ViM ViT\\nGPU memory 19.967× 2.732 7.318 0.477 0.344 0.322 0.305\\nper epoch (GB)\\nTraining time 12.0× 3.0 2.7 2.5 2.1 2.6 2.3\\nper epoch (s)\\nTrainable 167169 187073 195841 215745 425217 445121 2260225 2280129\\nparameters\\n𝑅2 score 0.9934× 0.9969 0.9838 0.9974 0.9957 0.9817 0.9491\\nRMSE (mD) 3.9571× 2.6939 6.1794 2.4557 3.1903 6.5718 10.9623\\nMiRE 0.0001× 0.0003 0.0001 0.0001 0.0001 0.0001 0.0001\\nMaRE 0.4478× 0.2708 0.6973 0.2105 0.3843 0.4586 0.7567\\n5.4. Ablation studies\\nIn this section, our objective is to examine the influence of several key hyperparameters of\\nthe neural network on its performance in predicting the permeability of porous media. While\\ndesigning a neural network, it is essential to perform hyperparameter fine-tuning to achieve the\\nbest possible performance, which is referred to as ablation studies. Interpreting the obtained\\nresults provides valuable insights into the behavior of the network for the current specific\\napplication in this article. For illustration, we focus on three important parameters. The first\\nparameter is the number of Vision Mamba blocks (𝑁block). As explained in Sect. 4, each block\\ncan be connected to the subsequent one, thereby progressively deepening the network. The\\nresults of this investigation are summarized in Table 3, where the number of Vision Mamba\\nblocks (𝑁block) was varied from 1 to 5. For each configuration, we report the coefficient of\\ndetermination (i.e.,𝑅2 score), the root mean square error, as well as the maximum and minimum\\nrelative errors. As observed from Table 3, the best performance is obtained with a network\\nconsisting of three blocks (i.e.,𝑁block =3 ), based on maximizing the𝑅2 score and minimizing\\nthe root mean square error on the test set (170 data). When the number of blocks is reduced,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3d23503d-20f8-41dc-a4ad-fc2eff5eb81a', embedding=None, metadata={'page_label': '11', 'file_name': '2510.14516v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14516v1.pdf', 'file_type': 'application/pdf', 'file_size': 4321174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='100 101 102\\nRelative patch size, (\\n32\\npatch size )\\n3\\n100\\n101\\n102\\nGPU memory per epoch (GB)\\nViT (data)\\nViM (data)\\nViT fit: aN² + b  (a=0.001721, b=0.2688)\\nViM fit: aN + b  (a=0.0385, b=0.2385)\\nFig. 4. GPU memory (per epoch) scaling versus relative patch size (i.e., relative token\\nsize)forVisionMamba(ViM)andViTonlog–logaxes. Least-squaresfitsreveallinear\\nscaling for Vision Mamba and quadratic scaling for ViT.\\nViM, patch size=8ViM, patch size=16ViM, patch size=32\\nViT, patch size=8ViT, patch size=16ViT, patch size=32\\nFig. 5. Comparison of Vision Mamba (ViM) and ViT performance based on the\\n𝑅2 score. The first row corresponds to Vision Mamba (ViM), and the second row\\ncorresponds to ViT.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ac238ee0-6fb6-4105-a922-c972cfee29af', embedding=None, metadata={'page_label': '12', 'file_name': '2510.14516v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14516v1.pdf', 'file_type': 'application/pdf', 'file_size': 4321174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='𝑁block =1𝑁 block =2\\n𝑁block =4𝑁 block =5\\nFig. 6. Performance of Vision Mamba for different numbers of blocks.\\nthe network becomes shallower and the number of trainable parameters decreases, which leads\\nto a decline in performance. However, this reduction is not particularly severe. For example,\\nwhen the number of Vision Mamba blocks is reduced from 3 to 2, the𝑅2 score decreases only\\nslightly from 0.9969 to 0.9803. Even with just a single Vision Mamba block (i.e.,𝑁block =1 ),\\nthe 𝑅2 score remains at 0.9562, indicating that the network maintains reasonable performance.\\nConversely, increasing the number of blocks from 3 to 4 and 5 leads to𝑅2 scores of 0.9590 and\\n0.9517, respectively. Thus, no performance improvement is observed beyond 3 Vision Mamba\\nblocks; instead, a slight reduction in the𝑅2 score occurs. This decline may be attributed to a\\nslight overfitting on the training data set, as the number of trainable parameters increases. This\\ndecline may be attributed to the increased capacity of the network and the number of trainable\\nparameters, resulting in an unnecessarily large model for the current size of the training data.\\nThe next hyperparameter we investigate is the patch size, the concept of which was explained\\nin Sect. 3. The outcomes of this investigation are listed in Table 4. Accordingly, we consider five\\ndifferent patch sizes: 4, 8, 16, 32, and 64. Similar to the previous case, we take the𝑅2 score\\nand the root mean square error as benchmarks. Consequently, the best performance is obtained', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d32a197b-aa43-4732-ae6f-40c22aa4b59b', embedding=None, metadata={'page_label': '13', 'file_name': '2510.14516v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14516v1.pdf', 'file_type': 'application/pdf', 'file_size': 4321174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 3. 𝑅2 score, root mean square error, and minimum/maximum relative errors\\nof the test set (170 samples) for different numbers of Vision Mamba blocks in the\\nproposed neural network. The patch size is fixed at 8.\\nNumber of Vision Mamba blocks (𝑁block) 1 2 3 4 5\\n𝑅2 score 0.9562 0.9803 0.9969 0.9590 0.9517\\nRoot mean square error (mD) 10.1734 6.8171 2.6939 9.8449 10.6828\\nMinimum relative error 0.0030 0.0001 0.0003 0.0001 0.0001\\nMaximum relative error 1.4327 0.7685 0.2708 0.6538 0.6875\\nwith a patch size of 16. However, the difference in𝑅2 scores across the different patch sizes is\\nrelatively small, with the highest being 0.9974 for the patch size of 16 and the lowest 0.9805 for\\nthe patch size of 64.\\nAs shown in Table 4, the overall performance of the network decreases slightly as the patch\\nsize increases beyond 16. This trend can be explained as follows. The patch size determines how\\neach three-dimensional input is divided into smaller cubes and transformed into a sequence of\\npatches,allowingthenetworktolearnboththeirfeaturesandtheirrelationshipswithneighboring\\npatches. As discussed in Sect. 3, these sequences are constructed by scanning the input along\\nthree spatial directions. In this sense, when the patch size is 64 and the porous media samples\\nare also of size64×64×64 (i.e., 𝑛=64 ), no subdivision occurs; the entire cube is treated as a\\nsingle patch, and fine-scale details are lost. In contrast, with a patch size of 8, a64×64×64\\ncube (i.e.,𝑛=64 ) is divided into 512 smaller patches, which are then sequentially processed\\nin three spatial directions (e.g., length, width, and height). Hence, this representation enables\\nthe network to better capture local features, leading to more accurate permeability predictions\\nin porous media. Additionally, we observe from Table 4 that using the patch size of 8 does not\\nimprove performance and yields results nearly identical to those with a patch size of 16. It is\\nconjectured that this behavior is related to the spatial correlation length of the dataset, which\\nis 17 voxels (i.e.,ℓ𝑐 =17 ). This may suggest that the optimal patch size should be close to the\\nspatial correlation length (if known), since it encapsulates the dominant information embedded\\nat that scale.\\nTable 4.𝑅2 score, root mean square error, and minimum/maximum relative errors of\\nthe test set (170 samples) for different patch sizes in the Vision Mamba model. The\\nnumber of Vision Mamba blocks is fixed at three (𝑁block =3).\\nPatch size 4 8 16 32 64\\n𝑅2 score 0.9934 0.9969 0.9974 0.9817 0.9805\\nRoot mean square error (mD) 3.9571 2.6939 2.4557 6.5718 6.7914\\nMinimum relative error 0.0001 0.0003 0.0001 0.0001 0.0001\\nMaximum relative error 0.4478 0.2708 0.2105 0.4586 0.8878\\nAs described in Sect. 3, the proposed Vision Mamba–based network processes 3D porous-\\nmedia cubes along the three spatial axes (𝑥, 𝑦, and𝑧) and aggregates the resulting features by\\naveraging, as in Eq. 11. The network’s output is the permeability in the𝑥-direction (see Sect.\\n2 and Eq. 3). To test whether scanning along the other two axes helps predict𝑥−direction', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9b750909-0f8d-44eb-b386-ab066a40b7dd', embedding=None, metadata={'page_label': '14', 'file_name': '2510.14516v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14516v1.pdf', 'file_type': 'application/pdf', 'file_size': 4321174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='permeability, we conduct an ablation in which, instead of scanning along all three axes and\\naveraging, we scan exclusively along a single axis (𝑥only,𝑦only, or𝑧only). The results in Table\\n5showthat,while 𝑥-onlyscanningismoreaccuratethan 𝑦-onlyor 𝑧-only(asexpected,giventhat\\nthe target is𝑥-permeability), aggregating features from all three axes yields the best performance,\\nwithhigher 𝑅2 andlowerrootmeansquarederror. Thisoutcomeisconsistentwiththeunderlying\\nphysics, which indicates that the average velocity in Eq. 3 and the permeability in the𝑥-direction\\ndepend on the full three-dimensional pore geometry. Therefore, solving the Stokes equations\\n(see Eqs. 1–2) in three dimensions is required even when estimating a directional permeability.\\nTable 5. Comparison of the𝑅2 score, root mean square error, and minimum/maximum\\nrelative errors on the test set (170 samples) for different scan directions in Vision\\nMamba (see Eq. 11). We set𝑁block =3and the patch size to 8.\\nScan direction All three axes𝑥−axis𝑦−axis𝑧−axis\\n𝑅2 score 0.9969 0.9945 0.9829 0.9704\\nRoot mean square error (mD) 2.6939 3.5980 6.3660 8.3650\\nMinimum relative error 0.0003 0.0001 0.0001 0.0001\\nMaximum relative error 0.2708 0.3503 0.4187 0.9440\\nThe final hyperparameter examined is the batch size (B) during training. As shown in Table\\n6, a batch size of 128 (B=128 ) yields the highest𝑅2 score and the lowest root mean square\\nerror when the patch size is fixed at 8 and the number of Vision Mamba blocks is set to 3. Larger\\nbatch sizes, such as 256, accelerate training but reduce performance, with the𝑅2 score dropping\\nfrom 0.9969 to 0.9700, indicating decreased accuracy in predicting porous media permeability.\\nTable 6.𝑅2 score, root mean square error, and minimum/maximum relative errors of\\nthe test set (170 samples) for different Batch sizes in the Vision Mamba model. The\\nnumber of Vision Mamba blocks is fixed at three (𝑁block =3 ). The patch size is set to\\n8.\\nBatch size (B) 4 16 32 128 256\\n𝑅2 score 0.9750 0.9895 0.9911 0.9969 0.9700\\nRoot mean square error (mD) 7.6883 4.9847 4.5980 2.6939 8.4248\\nMinimum relative error 0.0004 0.0008 0.0001 0.0003 0.0001\\nMaximum relative error 0.5114 0.6885 0.3712 0.2708 0.5845\\n6. Summary and future research projects\\nIn this article, we presented a neural network based on Vision Mamba for predicting the\\npermeability of three-dimensional porous media. We demonstrated the effectiveness of the\\nproposed model using evaluation criteria such as the coefficient of determination, mean square\\nerror, and maximum and minimum relative errors. We discussed the advantages of Vision\\nMamba compared to CNNs and ViTs for permeability prediction. In particular, we showed that,\\nrelativetoCNNs, VisionMambarequiresfarfewertrainableparameterswhileachievingsuperior\\nperformance. Furthermore, we demonstrated that GPU memory usage in Vision Mamba scales\\nlinearly with patch size, whereas in ViTs it scales quadratically. As a result, under limited GPU', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='77b1e4a0-0e88-4751-a905-5776aa9f4cff', embedding=None, metadata={'page_label': '15', 'file_name': '2510.14516v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14516v1.pdf', 'file_type': 'application/pdf', 'file_size': 4321174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='memory, Vision Mamba was able to successfully execute the machine learning experiments with\\nsmall patch sizes, whereas ViTs could not be trained due to insufficient memory when using the\\nsame small batch sizes. Finally, we explored the impact of key hyperparameters, including the\\nnumber of Vision Mamba blocks, patch size within each block, and batch size, to highlight their\\ninfluence on model performance.\\nIn the present article, we used the classification branch of Vision Mamba, where the neural\\nnetwork input represents the geometry of the porous medium and the output is the permeability\\nas a scalar value. As one idea for future projects, one could use the segmentation branch of\\nVision Mamba such that, although the input remains the same three-dimensional cube describing\\nthe porous-medium geometry, the output is the predicted velocity field within the pore space.\\nOf course, after obtaining the velocity field, the permeability can also be computed. However,\\naccess to the full velocity field provides more information. This can be done in the form of\\nfully supervised deep learning or in the form of weakly supervised deep learning, if only sparse\\nobservations from the velocity field are available, by enforcing the governing equations (e.g., Eqs.\\n1–2) as a loss function for the Vision Mamba network.\\nAnother promising research direction could be the development of large language and vision\\nmodels for porous media based on Vision Mamba rather than transformer architectures. Such\\nfoundation models could handle variable-size and multimineral porous media, enabling the\\npredictionofphysicalandgeometricalfeatures,andofferinginteractiveenvironmentstointegrate\\nimages, codes, texts, and mathematical formulations within a unified framework.\\nAcknowledgment.The authors of this research article gratefully acknowledge financial support from the\\nShell–Stanford Collaborative Project on Digital Rock Physics.\\nData Availability.The Python implementation is openly available in a public GitHub repository and can\\nbe accessed at https://github.com/Ali-Stanford/Vision_Mamba_3D_Porous_Media.\\nReferences\\n1. H. Andra, N. Combaret, J. Dvorkin,et al., “Digital rock physics benchmarks—part i: Imaging and segmentation,”\\nComput. & Geosci.50, 25–32 (2013). Benchmark problems, datasets and methodologies for the computational\\ngeosciences.\\n2. H.Andra,N.Combaret,J.Dvorkin,etal.,“Digitalrockphysicsbenchmarks—partii: Computingeffectiveproperties,”\\nComput. & Geosci.50, 33–43 (2013). Benchmark problems, datasets and methodologies for the computational\\ngeosciences.\\n3. J. Zhu, L. Zhao, W. Zhu, and J. Geng, “Joint use of multiscale digital rock physics and effective medium theory to\\nmodel elastic properties of shale reservoir,” Geophysics90, MR323–MR334 (2025).\\n4. Y. Liang and D. Fletcher, “Computational fluid dynamics simulation of forward osmosis (fo) membrane systems:\\nMethodology, state of art, challenges and opportunities,” Desalination549, 116359 (2023).\\n5. F. S. Ferro and B. S. Carmo, “Numerical modelling and simulation of hollow fiber dense membranes for co2/ch4\\nseparation using cfd,” J. Membr. Sci. p. 124134 (2025).\\n6. M. J. Blunt, B. Bijeljic, H. Dong,et al., “Pore-scale imaging and modelling,” Adv. Water Resour.51, 197–216\\n(2013). 35th Year Anniversary Issue.\\n7. M. Yang, S. Huang, F. Zhao, and C. Yang, “A novel hybrid finite-infinite diffusion model for determining co2\\ndiffusion coefficient in oil-saturated porous media: Applications for enhanced oil recovery and geological carbon\\nstorage,” Energy316, 134621 (2025).\\n8. T. Kumeria, “Advances on porous nanomaterials for biomedical application (drug delivery, sensing, and tissue\\nengineering),” ACS Biomater. Sci. & Eng.8, 4025–4027 (2022).\\n9. M.K.Das,P.P.Mukherjee,andK.Muralidhar,PorousMediaApplications: BiologicalSystems(SpringerInternational\\nPublishing, 2018).\\n10. U. Farooq, T. Liu, and A. Jan, “Boundary layer analysis of second-order magnetic nanofluid flow with carbon\\nnanotubes and gyrotactic microorganisms for medical diagnostics,” BioNanoScience15, 113 (2025).\\n11. A. Bihani, H. Daigle, J. E. Santos,et al., “Mudrocknet: Semantic segmentation of mudrock sem images through deep\\nlearning,” Comput. & Geosci.158, 104952 (2022).\\n12. H.-B. Lee, M.-H. Jung, Y.-H. Kim,et al., “Deep learning image segmentation for the reliable porosity measurement\\nof high-capacity ni-based oxide cathode secondary particles,” J. Anal. Sci. Technol.14, 47 (2023).\\n13. Y. Han and Y. Liu, “Advanced petrographic thin section segmentation through deep learning-integrated adaptive\\nglfif,” Comput. & Geosci.193, 105713 (2024).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2e5c22d6-a3db-4c50-86dd-44a92b019132', embedding=None, metadata={'page_label': '16', 'file_name': '2510.14516v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14516v1.pdf', 'file_type': 'application/pdf', 'file_size': 4321174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14. C. Wang, H. Luo, J. Wang, and D. Groom, “Reunet: Efficient deep learning for precise ore segmentation in mineral\\nprocessing,” Comput. & Geosci.195, 105773 (2025).\\n15. Y. Meng, J. Jiang, J. Wu, and D. Wang, “Transformer-based deep learning models for predicting permeability of\\nporous media,” Authorea Prepr. (2023).\\n16. C. Xie, J. Zhu, H. Yang,et al., “Relative permeability curve prediction from digital rocks with variable sizes using\\ndeep learning,” Phys. Fluids35, 096605 (2023).\\n17. A. Kashefi and T. Mukerji, “Point-cloud deep learning of porous media for permeability prediction,” Phys. Fluids33,\\n097109 (2021).\\n18. M. Liu, R. Ahmad, W. Cai, and T. Mukerji, “Hierarchical homogenization with deep-learning-based surrogate model\\nfor rapid estimation of effective permeability from digital rocks,” J. Geophys. Res. Solid Earth128, e2022JB025378\\n(2023).\\n19. J. Hong and J. Liu, “Rapid estimation of permeability from digital rock using 3d convolutional neural network,”\\nComput. Geosci.24, 1523–1539 (2020).\\n20. J. Wu, X. Yin, and H. Xiao, “Seeing permeability from images: fast prediction with convolutional neural networks,”\\nSci. Bull.63, 1215–1222 (2018).\\n21. M. Masroor, M. Emami Niri, and M. H. Sharifinasab, “A multiple-input deep residual convolutional neural network\\nfor reservoir permeability prediction,” Geoenergy Sci. Eng.222, 211420 (2023).\\n22. H. Sun, L. Zhou, D. Fan,et al., “Permeability prediction of considering organic matter distribution based on deep\\nlearning,” Phys. Fluids35, 032014 (2023).\\n23. A. Kashefi and T. Mukerji, “A novel fourier neural operator framework for classification of multi-sized images:\\nApplication to three dimensional digital porous media,” Phys. Fluids36(2024).\\n24. K. M. Graczyk and M. Matyka, “Predicting porosity, permeability, and tortuosity of porous media from images by\\ndeep learning,” Sci. reports10, 21488 (2020).\\n25. J. Chung, R. Ahmad, W. Sun,et al., “Prediction of effective elastic moduli of rocks using graph neural networks,”\\nComput. Methods Appl. Mech. Eng.421, 116780 (2024).\\n26. C.Liu,R.Guo,andY.Su,“Adeeplearningbasedpredictionmodelforeffectiveelasticpropertiesofporousmaterials,”\\nSci. Reports15, 6707 (2025).\\n27. H. Wu, W.-Z. Fang, Q. Kang,et al., “Predicting effective diffusivity of porous media from images by deep learning,”\\nSci. reports9, 20387 (2019).\\n28. J. E. Santos, D. Xu, H. Jo,et al., “Poreflow-net: A 3d convolutional neural network to predict fluid flow through\\nporous media,” Adv. Water Resour.138, 103539 (2020).\\n29. S. Kamrava, M. Sahimi, and P. Tahmasebi, “Simulating fluid flow in complex porous materials by integrating the\\ngoverning equations with deep-layered machines,” NPJ Comput. Mater.7, 127 (2021).\\n30. A. Kashefi and T. Mukerji, “Prediction of fluid flow in porous media by sparse observations and physics-informed\\npointnet,” Neural Networks167, 80–91 (2023).\\n31. M. Liu and T. Mukerji, “Multiscale fusion of digital rock images based on deep generative adversarial networks,”\\nGeophys. Res. Lett.49, e2022GL098342 (2022).\\n32. K. M. Guan, T. I. Anderson, P. Creux, and A. R. Kovscek, “Reconstructing porous media using generative flow\\nnetworks,” Comput. & Geosci.156, 104905 (2021).\\n33. J. Phan, M. Sarmad, L. Ruspini,et al., “Generating 3d images of material microstructures from a single 2d image: a\\ndenoising diffusion approach,” Sci. Reports14, 6498 (2024).\\n34. N. Baishnab, E. Herron, A. Balu,et al., “3d multiphase heterogeneous microstructure generation using conditional\\nlatent diffusion models,” arXiv preprint arXiv:2503.10711 (2025).\\n35. K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image Recognition,” inProceedings of 2016 IEEE\\nConference on Computer Vision and Pattern Recognition,(IEEE, 2016), CVPR ’16, pp. 770–778.\\n36. P. Tang, D. Zhang, and H. Li, “Predicting permeability from 3d rock images based on cnn with physical information,”\\nJ. Hydrol.606, 127473 (2022).\\n37. C.R.Qi,H.Su,K.Mo,andL.J.Guibas,“Pointnet: Deeplearningonpointsetsfor3dclassificationandsegmentation,”\\ninProceedings of the IEEE conference on computer vision and pattern recognition,(2017), pp. 652–660.\\n38. A.Kashefi, “Pointnetwithkanversuspointnetwithmlpfor3dclassificationandsegmentationofpointsets,”Comput.\\n& Graph.131, 104319 (2025).\\n39. C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep hierarchical feature learning on point sets in a metric\\nspace,” inAdvances in Neural Information Processing Systems,vol. 30 I. Guyon, U. V. Luxburg, S. Bengio,et al.,\\neds. (Curran Associates, Inc., 2017).\\n40. Z. Li, N. Kovachki, K. Azizzadenesheli,et al., “Fourier neural operator for parametric partial differential equations,”\\narXiv preprint arXiv:2010.08895 (2020).\\n41. A. Dosovitskiy, L. Beyer, A. Kolesnikov,et al., “An image is worth 16x16 words: Transformers for image recognition\\nat scale,” (2021).\\n42. S.Geng,S.Zhai,andC.Li,“Swintransformerbasedtransferlearningmodelforpredictingporousmediapermeability\\nfrom 2d images,” Comput. Geotech.168, 106177 (2024).\\n43. C. Temizel, U. Odi, K. Li,et al., “Permeability prediction using vision transformers,” Math. Comput. Appl.30, 71\\n(2025).\\n44. Y. Meng, J. Jiang, J. Wu, and D. Wang, “Transformer-based deep learning models for predicting permeability of', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6e38f0b7-2b78-459b-bd1d-f87970861b1d', embedding=None, metadata={'page_label': '17', 'file_name': '2510.14516v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14516v1.pdf', 'file_type': 'application/pdf', 'file_size': 4321174, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='porous media,” Adv. Water Resour.179, 104520 (2023).\\n45. A. Gu and T. Dao, “Mamba: Linear-time sequence modeling with selective state spaces,” (2024).\\n46. A. Vaswani, N. Shazeer, N. Parmar,et al., “Attention is all you need,” (2023).\\n47. L. Zhu, B. Liao, Q. Zhang,et al., “Vision mamba: Efficient visual representation learning with bidirectional state\\nspace model,” (2024).\\n48. R. Xu, S. Yang, Y. Wang,et al., “A survey on vision mamba: Models, applications and challenges,” CoRR\\nabs/2404.18861(2024).\\n49. Y. Liu, Y. Tian, Y. Zhao,et al., “Vmamba: Visual state space model,” (2024).\\n50. Y. Yue and Z. Li, “Medmamba: Vision mamba for medical image classification,” (2024).\\n51. M. Bao, S. Lyu, Z. Xu,et al., “Vision mamba in remote sensing: A comprehensive survey of techniques, applications\\nand outlook,” (2025).\\n52. J. Liu, J. Li, X. Wang,et al., “Mamba-augmented residual network for rapid wake field prediction of underwater\\nvehicles,” Ocean. Eng.341, 122474 (2025).\\n53. C.Wang,Y.Xie,Q.Chen,etal.,“Acomprehensiveanalysisofmambafor3dvolumetricmedicalimagesegmentation,”\\n(2025).\\n54. Y. Yang, Z. Xing, L. Yu,et al., “Vivim: a video vision mamba for medical video segmentation,” (2024).\\n55. OpenAI, J. Achiam, S. Adler,et al., “Gpt-4 technical report,” (2024).\\n56. Y. Chang, X. Wang, J. Wang,et al., “A survey on evaluation of large language models,” (2023).\\n57. G. Team, P. Georgiev, V. I. Lei,et al., “Gemini 1.5: Unlocking multimodal understanding across millions of tokens\\nof context,” (2024).\\n58. A. Kashefi and T. Mukerji, “Chatgpt for programming numerical methods,” J. Mach. Learn. for Model. Comput.4,\\n1–74 (2023).\\n59. A. Kashefi, “A misleading gallery of fluid motion by generative artificial intelligence,” J. Mach. Learn. for Model.\\nComput.5, 113–144 (2024).\\n60. A. Basant, A. Khairnar, A. Paithankar,et al., “Nvidia nemotron nano 2: An accurate and efficient hybrid mamba-\\ntransformer reasoning model,” arXiv preprint arXiv:2508.14444 (2025).\\n61. C. Lantuejoul,Geostatistical Simulation: Models and Algorithms(Springer, 2002).\\n62. M. Le Ravalec-Dupin, F. Roggero, and R. Froidevaux, “Conditioning truncated gaussian realizations to static and\\ndynamic data,” SPE J.9, 475–480 (2004).\\n63. Y.Keehm,T.Mukerji,andA.Nur,“Permeabilitypredictionfromthinsections: 3dreconstructionandlattice-boltzmann\\nflow simulation,” Geophys. Res. Lett.31(2004).\\n64. H.Darcy,LesfontainespubliquesdelavilledeDijon: expositionetapplicationdesprincipesàsuivreetdesformules\\nà employer dans les questions de distribution d’eau, vol. 1 (Victor Dalmont, 1856).\\n65. A. Gu and T. Dao, “Mamba: Linear-time sequence modeling with selective state spaces,” arXiv preprint\\narXiv:2312.00752 (2023).\\n66. L. Zhu, B. Liao, Q. Zhang,et al., “Vision mamba: Efficient visual representation learning with bidirectional state\\nspace model,” arXiv preprint arXiv:2401.09417 (2024).\\n67. D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980 (2014).\\n68. I. Goodfellow, Y. Bengio, and A. Courville,Deep Learning(MIT Press, 2016). http://www.deeplearningbook.org.\\n69. S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate\\nshift,” inInternational conference on machine learning,(pmlr, 2015), pp. 448–456.\\n70. N. Srivastava, G. Hinton, A. Krizhevsky,et al., “Dropout: a simple way to prevent neural networks from overfitting,”\\nThe journal machine learning research15, 1929–1958 (2014).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3ac8d26b-abb6-455f-a460-25cd512b9d0f', embedding=None, metadata={'page_label': '1', 'file_name': '2510.14946v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14946v1.pdf', 'file_type': 'application/pdf', 'file_size': 6954992, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='EdgeNavMamba: Mamba-Optimized Object\\nDetection for Energy-Efficient Edge Devices\\nRomina Aalishah, Mozhgan Navardi, and Tinoosh Mohsenin\\nDepartment of Electrical and Computer Engineering\\nJohns Hopkins University, Baltimore, MD, USA\\nAbstract—Deployment of efficient and accurate Deep Learning\\nmodels has long been a challenge in autonomous navigation,\\nparticularly for real-time applications on resource-constrained\\nedge devices. Edge devices are limited in computing power and\\nmemory, making model efficiency and compression essential.\\nIn this work, we propose EdgeNavMamba, a reinforcement\\nlearning-based framework for goal-directed navigation using an\\nefficient Mamba object detection model. To train and evaluate the\\ndetector, we introduce a custom shape detection dataset collected\\nin diverse indoor settings, reflecting visual cues common in real-\\nworld navigation. The object detector serves as a pre-processing\\nmodule, extracting bounding boxes (BBOX) from visual input,\\nwhich are then passed to an RL policy to control goal-oriented\\nnavigation. Experimental results show that the student model\\nachieved a reduction of 67% in size, and up to 73% in energy\\nper inference on edge devices of NVIDIA Jetson Orin Nano and\\nRaspberry Pi 5, while keeping the same performance as the\\nteacher model. EdgeNavMamba also maintains high detection\\naccuracy in MiniWorld and IsaacLab simulators while reducing\\nparameters by 31% compared to the baseline.\\nI. INTRODUCTION\\nEdge deployment is a key challenge for practical Deep\\nLearning (DL) applications [1], [2], particularly in autonomous\\nnavigation, medical imaging , which require real-time per-\\nformance [3]–[8]. DL models on edge devices must be\\nlightweight and efficient to provide real-time, reliable per-\\nformance despite constraints in computation and power [9]–\\n[11]. Particularly in autonomous navigation (Fig. 1), scene\\nunderstanding is critical, enabling vision models to learn envi-\\nronmental features, obstacles, and paths for navigation in both\\nnew and familiar scenarios [12], [13]. Deploying these models\\non edge devices is challenging due to their computational\\nintensity, which is necessary for high accuracy [14].\\nOptimization methods have been applied to these models\\nto improve power and memory efficiency. Since You Only\\nLook Once (YOLO) [15] revolutionized object detection by\\nusing regression on bounding boxes, several efforts have\\napplied these methods to YOLO. YOLO-ACE redesigned the\\nbackbone and applied double distillation [12], and Mamba\\nYOLO [16] integrated a state-space-model (SSM) [17] back-\\nbone for efficiency. With the introduction of these lightweight\\nyet powerful models, the deployment of edge devices for\\nnavigation tasks becomes more feasible and efficient. For the\\nnavigation phase, Reinforcement Learning (RL) has been a\\nsuccessful inspiration, as it allows the agent to learn through\\ninteractions and real-time feedback [18]. However, to the best\\nof our knowledge no existing work has attempted to combine\\n(b) RosmasterEdge Platforms:     (a) Go2 Robot Dog\\nEdge Accelerator: \\nJetson Orin Nano\\nComputing Power: 40 TOPs\\nCPU: Six-core Cortex A78AE ARMv8.2 | 2x clusters\\n(1x 4-core cluster + 128 KB L1 + 256KB L2 per core + 2MB \\nL3) + 1x 2-core cluster (128 KB L1 + 256KB L2 per core + \\n2MB L3) | System Cache: 4 MB (shared across all clusters)\\nMemory: 8GB 128-bit LPDDR5 DRAM\\nPower Mode: 7W | 15W\\nComputing Power: 40 TOPs\\nMemory: 8GB DRAM\\nPower Mode: 7W | 15W\\nFig. 1. Edge platforms with onboard Jetson Orin Nano accelerators: (a) the\\nUnitree Go2 robot dog and (b) the Yahboom Rosmaster wheeled robot.\\nMamba, Knowledge Distillation (KD), and an optimization\\nstrategy to produce a model small enough to fit into cache\\nmemory, thereby improving time and energy efficiency.\\nTo address this, we develop EdgeNavMamba, a customized\\nMamba-based detector tailored for efficient on-device per-\\nception. Unlike prior lightweight YOLO variants or state-\\nspace backbones, our design uniquely integrates the Mamba\\narchitecture with KD [21] to achieve a balance between ac-\\ncuracy, and energy efficiency. The combination of state-space\\nmodeling and distillation enables compact yet context-aware\\nfeature representations that YOLO variants cannot capture.\\nThis framework directly addresses the memory and compu-\\ntational bottlenecks of edge deployment while maintaining\\nreal-time performance. We further validate the deployment of\\nEdgeNavMamba on resource-constrained edge devices such\\nas NVIDIA Jetson Orin Nano with 8 GB memory [22] and\\nRaspberry Pi 5 with 16 GB memory [23]. The experimental\\nresults demonstrate that EdgeNavMamba successfully achieves\\nefficiency with minimal performance loss compared to the\\nteacher model. Our contributions are as follows:\\n•Development of an edge Mamba object detector through\\narchitecture modification and knowledge distillation.\\n•Power and latency analysis for the proposed EdgeNav-\\nMamba on edge devices, such as the Raspberry Pi 5 and\\nNVIDIA Jetson Orin Nano with Arm Cortex processors.\\n•Validation of object detection in simulators MiniWorld\\nand IsaacLab, as well as RL navigation validation in\\nMiniWorld with different complexities.\\narXiv:2510.14946v1  [eess.IV]  16 Oct 2025', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5f48681c-0652-4cbd-bbfc-1f5fbd23c1d6', embedding=None, metadata={'page_label': '2', 'file_name': '2510.14946v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14946v1.pdf', 'file_type': 'application/pdf', 'file_size': 6954992, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A g e n t\\nE n v i r o n m e n t\\nA c t i o n\\nS t a t e\\nR e w a r d\\nfeatures\\nlogits\\nfeatures\\nlogits\\nFrozen T eacher Model\\nKD Loss\\nGround T ruth Data\\nred blue\\n(a) (b) (c)\\nInput\\nEdge Student Model\\nConv\\ndim i/2dim i/2\\nSiLUSiLU\\nSSM\\nLinear Lineardim i dim i\\ndim i Linear\\n......\\n......\\n...\\n...\\nFig. 2. (a) Reinforcement Learning (RL) diagram, including the interaction with the environment to maximize reward [19], [20], (b) Architecture of Mamba [17],\\nused for feature extraction and model efficiency, (c) The process of knowledge distillation [21]; the teacher model is trained and frozen, the student model is\\ntrained based on the teacher features, logits, and ground truth data.\\nThe rest of this paper reviews related work, outlines key\\npreliminaries, introduces the EdgeNavMamba framework, and\\npresents experimental results, and concludes with key findings.\\nII. RELATEDWORK\\nEdge deployment is critical for real-world deep learning\\napplications [1], [24], especially in autonomous systems where\\nonboard processing requires models to be light and efficient\\nfor real-time, reliable performance [5]. Common optimization\\ntechniques include architecture modification, knowledge dis-\\ntillation [21], quantization, and pruning [2], [25] Architecture\\nchanges adjust layer types, sizes, and their repetitions to main-\\ntain performance while reducing model size [6]. Knowledge\\ndistillation is applicable wether the teacher model is open-\\nsource or not [2]. Quantization and pruning reduce memory\\nusage by decreasing bit precision and removing connections\\nin a structured or unstructured manner, respectively [25].\\nObject detection is one of the most computationally inten-\\nsive tasks in computer vision and deep learning. Due to the\\nneed for high precision to detect objects of varying sizes,\\nmodels are often large or require significant computational\\nresources [14], [26]. Compression techniques address this\\nissue. YOLO represents a major advancement in this field,\\naddressing object detection in a regression-based manner [15].\\nLighter variants, such as YOLOv9 [27], have been adapted for\\nedge object deployment. To improve precision, newer versions\\nadd an attention-based mechanism [28], but with a higher\\ncomputational cost [17]. Mamba, a more efficient alternative\\nto attention architecture, has been adopted in both full and\\nhybrid forms in detection models [16], [29], [30]. With the\\nintroduction of these lightweight yet powerful models, the\\ndeployment of edge devices for navigation tasks becomes\\nmore feasible and efficient. Mela et al. applied quantization\\nand pruning for unmanned surface vehicles [14]. Yang et al.\\nproposed a multimodal 3D object detection framework using\\nattention-guided and category-aware distillation [31].\\nReinforcement learning (RL) approaches such as deep\\nQ-networks (DQN) [32] and proximal policy optimization\\n(PPO) [33] have been applied to autonomous navigation on\\nresource-constrained edge devices by directly mapping vision\\ninputs to control commands [34]. In [35], YOLO was in-\\ntegrated into a Deep Reinforcement Learning algorithm by\\npassing the bounding-box (BBOX) coordinates ofngoals\\ninstead of raw images, improving training time and real-\\nworld performance. However, asngrows, the input vector\\nbecomes larger, complicating goal learning, and adding a\\nYOLO module adds significant edge-device overhead. To\\naddress this, a Squeezed-Edge YOLO module integrated with\\nRL was proposed to enhance the energy efficiency of the\\ndetection on edge devices [20], [26].\\nIn this work, we present an end-to-end framework for RL-\\nbased autonomous navigation with an optimized Mamba object\\ndetection model for energy-efficient edge computing. First,\\nwe design the optimized detector, which achieves competi-\\ntive accuracy while using less memory and computation and\\ntherefore less energy than existing work. Next, we integrate\\nthis model into an RL algorithm and train the navigation policy\\nin simulation. Finally, we deploy and evaluate the optimized\\nobject detection model on edge devices.\\nIII. PRELIMINARIES\\nReinforcement Learning (RL).Goal-directed navigation,\\nwhere the agent aims to reach an object in each episode,\\ncan be modeled as a Markov Decision Process (MDP) [36],\\ndefined by a state spaceS, action spaceA, reward function\\nr:S×A→R, initial state distributions 0, and transition prob-\\nabilityp(s t+1 |s t, at). RL [19] provides a set of algorithms\\nthat enable an agent to learn optimal policiesπ(a|s)through\\ntrial-and-error interactions with the environment, aiming to\\nmaximize the cumulative expected reward. In goal-based tasks,\\nthe objective can be formulated as a goal-oriented MDP [20],\\n[37], where RL methods learn to map states to actions that\\nlead the agent toward the goal. Fig. 2 (a) illustrates how\\nan RL agent interacts with the environment under the MDP\\nframework to receive rewards.\\nObject Detection.Object detection in computer vision aims\\nto locate an object in images or videos by providing its\\nspatial location in the form of bounding boxes and its category\\nthrough class labels. The field is divided into two types\\nof approaches: traditional techniques and machine learning-\\nbased methods. Traditional object detection methods rely on\\nhandcrafted features such as Haar [38] , combined with brute-\\nforce techniques like sliding window searches across multiple\\nscales and positions [38]. Due to their multi-stage pipelines,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ea523c2e-0310-4b82-8dd1-27cb5204c9d8', embedding=None, metadata={'page_label': '3', 'file_name': '2510.14946v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14946v1.pdf', 'file_type': 'application/pdf', 'file_size': 6954992, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Agent - Object Level (Reasoning)\\nAgent\\nV\\niew\\nAgent\\nSub-Goals\\nEnvironment - Ground Level (Doing)\\nImage\\nBBox Coordinates\\nPre-processing\\nModule\\nState\\nReward\\nAction\\n4 x n\\nDiscrete\\nAction\\n5 x n + a\\nGoal BBox\\nRL  Model\\nLast Action\\nn a\\nEdgeNavMamba\\nFig. 3. The proposed architecture of EdgeNavMamba, consisting of two\\nbranches of convolution and SSM for feature extraction. The same architecture\\nis used for teacher and student models with a different set of dimensions.\\nFeatures, then, undergo a detection process, and the bounding boxes are given\\nto the RL model for navigation to the goal.\\nthe introduction of YOLO as a real-time approach helped\\naddress it as a single regression problem [15].\\nMamba and State Space Models.Mamba [17] architecture\\nintroduces Selective State Space Models, an efficient alterna-\\ntive to Transformers [39], reducing computational complexity\\nwhile maintaining feature extraction capabilities. The state\\nspace representation in Mamba is formulated as follows:\\ny(t) =Cx(t)(1)\\nd\\ndtx(t) =Ax(t) +Bu(t)(2)\\nwherex(t)represents the hidden state,u(t)is the input\\nsignal andA,B, andCare learnable matrices. This structure\\nenables Mamba to capture long-range dependencies efficiently\\nwhile requiring fewer parameters than traditional self-attention\\nmechanisms. As a result, several efforts have been made to\\napply this method across various tasks. Fig. 2 (b) demonstrates\\nits architecture as a part of the network.\\nKnowledge Distillation (KD).Knowledge distillation trans-\\nfers knowledge from a larger teacher model to a smaller\\nstudent model to achieve similar performance with fewer\\nparameters. In our setting, the student is trained using a combi-\\nnation of the standard YOLO detection loss, a distillation loss\\non classification logits, and a feature matching loss between\\nintermediate representations:\\nL=L det +λ kdLKD +λ featLfeat (3)\\nwhereL det is the standard YOLO detection loss computed\\nfrom ground truth boxes and labels.L KD is a temperature-\\nscaled Kullback–Leibler divergence between the teacher and\\nstudent classification logits controlled by a temperature param-\\neterT.L feat is the mean squared error between intermediate\\nfeature maps of the teacher and student. The hyperparameters\\nλkd andλ feat control the relative contributions of the distilla-\\ntion and feature matching terms.\\nLite-Conv-SSM Block \\nEdgeNavMamba\\nPatch\\nEmbedding\\nLite-Conv-SSM\\nBlock\\nPatch\\nMerging\\nLite-Conv-SSM\\nBlock\\nPatch\\nMerging\\nLite-Conv-SSM\\nBlock\\nPatch\\nMerging\\nLite-Conv-SSM\\nBlock\\nPatch\\nMerging\\nDetector\\nGoal Detection\\nSplit\\nDWConv\\nBlock\\nPWConv\\nBlock\\nLN\\nLinear\\nLiteSS2D Block\\nConcatenate\\nShuf fle\\nConv Branch\\nSSM Branch\\nPatch\\nLinear\\nBN\\nDetector\\nDWConv\\nConv2D (1x1)\\nLinear\\nReLU\\nA vgPooling\\nx2 x2 x4 x2\\ndim 1\\ndim 2\\ndim 3\\ndim 4\\ndim teacher student\\n1 64 32\\n2 128 64\\n3 256 128\\n4 512 256\\nFig. 4. The proposed architecture of EdgeNavMamba, consisting of two\\nbranches of convolution and SSM (including LiteSS2D) for feature extraction.\\nThe same architecture is used for teacher and student models with a different\\nset of dimensions. Features, then, undergo a detection process, and the\\nbounding boxes are given to the RL model for navigation to the goal.\\nIV. PROPOSEDMETHODOLOGY\\nIn this section, we introduce the end-to-end framework\\ncalled EdgeNavMamba for energy-efficient autonomous navi-\\ngation, utilizing an optimized Mamba object detection model\\nfor on-device edge computing. Fig. 3 and Algorithm 1 provide\\nan overview of the proposed system. At each timestep, the\\nagent captures an image of its environment, which the detector\\nprocesses to extract BBOX coordinates of objects. These\\ncoordinates are then encoded as a feature vector and passed to\\nan RL policy for goal navigation. Together, these components\\nenable the agent to navigate autonomously to the goal while\\nminimizing computations and energy usage. The RL policy is\\ntrained in MiniWorld and IsaacLab simulation environments.\\nIn the following section, we present our detailed approach.\\nA. Sim-to-Real Goal Navigation Framework\\nThe navigation framework consists of two modules: an\\nobject detection network and an RL policy to reach the goal.\\nFirst, the EdgeNavMamba processes the input image, divides it\\ninto a fixed resolution, and outputs normalized bounding boxes\\nwith confidence scores forndetected objects. The resulting\\nBBox coordinates(x 1, y1, x2, y2), producing a1×(4n)vector.\\nThis is concatenated with a one-hot encoded sub-goal vector\\nof size1×n, and a one-hot encoded last-action vector of size\\n1×a, whereais the number of discrete actions. resulting in\\na1×(5n+a)state vector. During each episode in simula-\\ntion, the PPO policy receives the full state vector, including\\nall detected boxes plus one-hot goal, while reward shaping\\nfocuses on the BBox coordinates of the current goal object.\\nThe action space is discrete:{left,right,forward}. The\\nPPO policy receives this state at each step and outputs an\\naction. A task is considered complete when the agent comes\\nwithin a predefined proximity threshold of the correct goal\\nobject, which checks the Euclidean distance between the agent\\nand the target. Navigation is guided by the reward function\\nshown in Table I. Distance change is to encourage the agent to\\nreduce its distance to the goal at each step. First goal visibility', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a4e317c8-61eb-4281-b5ab-7ab0acd89bad', embedding=None, metadata={'page_label': '4', 'file_name': '2510.14946v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14946v1.pdf', 'file_type': 'application/pdf', 'file_size': 6954992, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Linear\\nDWConv\\nSiLU\\nLN\\nLiteSS2D\\nLiteSS2D Block\\n1 2 3 ...\\n1 4 7 ...\\n9 8 7 ...\\n9 6 3 ...\\n1 2 3 ...\\n1 4 7 ...\\n9 8 7 ...\\n9 6 3 ...\\nScan Expanding Scan Merging\\nS6\\nLiteSS2D\\n1 2 3\\n4 5 6\\n7 8 9\\n1 2 3\\n4 5 6\\n7 8 9\\nFig. 5. The architecture of LiteSS2D block, which is in the SSM branch of EdgeNavMamba, following the overall flow of the SS2D proposed by VMamba [40],\\nbut with modifications for better efficiency, mentioned in MedMambaLite [6].\\nTABLE I\\nREWARD COMPONENTS FOR NAVIGATION TASK INMINIWORLD\\nCondition Reward\\nCorrect goal reached+10.0\\nWrong goal / wall collision−2.0\\nPer step penalty−0.01\\nOpposite turn actions−0.05\\nDistance change0.5·(∆d prev −∆d curr)\\nFirst goal visibility+0.1\\nExploration (forward, no goal)+0.01\\nExploration (turn, no goal)+0.005\\nand exploration rewards provide additional guidance when the\\ngoal is not yet in view, preventing the agent from remaining\\nin states with no other positive reward signals.\\nB. Edge-Optimized Mamba Object Detection\\nModel Architecture.Fig. 4 shows the overview of the\\nproposed architecture for object detection, inspired by Med-\\nMambaLite [6], which includes five main units as follows:\\n1) Patch Embedding:The input image is split into patches\\nand projected into a higher-dimensional space.\\n2) Lite-Conv-SSM-Block:Features pass through a series\\nof Lite-Conv-SSM blocks, including convolutional and State-\\nSpace Modeling (SSM) components. Convolutional branch\\ncaptures local features using depthwise and pointwise convolu-\\ntions. Meanwhile, the SSM branch utilizes a Lite 2D Selective\\nScan Mamba (LiteSS2D) module to capture long-range depen-\\ndencies and global features. The outputs are concatenated and\\nshuffled to fuse global and local features. A number of these\\nblocks form stages in a hierarchical architecture.\\n3) Lite 2D-Selective-Scan:Fig. 5 shows Lite 2D-Selective-\\nScan (LiteSS2D), which shares weights across four directions\\nto reduce computation. The block starts by projecting the\\ninput features into a higher dimension, applies row-wise and\\ncolumn-wise convolutions, then runs a four-way Selective\\nScan with a shared SSM core.\\nScan Expanding:flattens the input along four directions.\\nS6 block:processes each sequence with shared weights.\\nScan Merging:sums directional outputs and reshapes them.\\nThis approach provides memory efficiency by avoiding\\nrepeated tensor reshaping and using compact representations.\\nCompared to available object detection models, we introduced\\nimportant changes to provide an efficient model. Efficiency\\nis improved by factorizing convolutions, sharing projection\\nweights, and reusing Mamba weight matrices across blocks.\\n4) Patch Merging:Between stages, patch merging layers\\nreduce spatial resolution while increasing channel depth, build-\\ning a hierarchical representation.\\nAlgorithm 1EdgeNavMamba Proposed Approach\\nRequire:DatasetD, teacher modelT, student modelS, RL\\npolicyπ\\nEnsure:Trained edge detectorS ⋆ and navigation policyπ ⋆\\n1:Train Teacher:TrainTonDusing detection loss.\\n2:Distill Student:FreezeTand trainSusingL=L det +\\nλkdLKD +λ featLfeat.\\n3:Train RL Policy:UseSto extract object bounding boxes\\nand feed them as state input to PPO agentπin MiniWorld.\\n4:Deploy on Edge:ExportS ⋆ andπ ⋆ to edge devices for\\nreal-time goal navigation.\\n5) Detector:The detector processes extracted features to\\nidentify the presence and bounding box of a target object.\\nIt uses depthwise and pointwise convolutions, followed by\\npooling and a linear layer to output the goal detection result.\\nKnowledge Distillation.Fig. 2 (c) illustrates our knowledge\\ndistillation framework, where an edge student model is trained\\nbased on a frozen teacher model and ground truth data. Models\\nhave a similar architecture, as shown in Fig. 4, but with\\nvarying channel dimensions. During training, each input batch\\nis processed by both teacher and student, and the student\\nparameters are updated using a combined KD Loss according\\nto the Eq. 3, and optimization is performed on the student\\nwhile keeping the teacher fixed.\\nV. EXPERIMENTALEVALUATION\\nA. Experimental Setup\\n1) Datasets:Two datasets were prepared for training and\\ndeployment of teacher and student models: a real-world dataset\\ncontaining 1,800 images and a simulated MiniWorld dataset\\nwith around 5,500 images. Both include three object classes,\\nred, blue, and black boxes, and are split into training and\\nvalidation sets with a 90/10 split.\\n2) Training Details:For object detection model and knowl-\\nedge distillation experiments, we set the temperature toT=\\n2.0, the KL divergence weight toλ kd = 1.0, and the feature-\\nmatching weight toλ feat = 0.25. The teacher is first trained,\\nthen frozen, and distillation is performed into the student\\nconfigured with depths[2,2,4,2]and channel dimensions\\n(32,64,128,256)on the same dataset. We use the Adam\\noptimizer with learning ratelr = 10 −4, batch size32, and\\na learning rate scheduler that reduces the rate when validation\\nMean Average Precision (mAP) shows no improvement. Inputs\\nare resized to224×224and normalized. Evaluation uses the\\nmAP metric. During training and validation we periodically', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6dd3baa2-5bdb-47d8-90b0-d337db060f29', embedding=None, metadata={'page_label': '5', 'file_name': '2510.14946v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14946v1.pdf', 'file_type': 'application/pdf', 'file_size': 6954992, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='USB Power Meter\\nPower (W) = I (A) x V (V) Raspberry Pi 5\\n16 GB Memory\\nJetson Orin Nano \\n8GB Memory with CUDA GPU \\nOnboard INA3221 Power Monitor\\nNVIDIA Jetson Orin Nano\\nCPU GPU\\n4x 256 KB L2\\n4x A78\\nGPC\\n4x 128 KB L1\\n2x A78\\n2x 128 KB L1\\n2x 256 KB L2\\n2 MB L3\\n2 MB L3\\n4 MB L2\\n8x 192 KB L1\\n4 MB Sys. Cache\\nRaspberry Pi 5\\n4x 512 KB L2\\n4x A76\\n2 MB L3\\n16 GB \\nGlobal Memory \\nCPU\\n4x 128 KB L1\\n8 GB \\nGlobal Memory \\nSM\\nSM\\nSM\\nSM SM\\nSM\\nSM\\nSM\\n(b) (c) (d)(a)\\nISAAC Simulator \\nFig. 6. Experimental setup for energy-efficient multi-goal autonomous naviga-\\ntion: (a) simulation environment in NVIDIA Isaac Simulator and MiniWorld.\\n(b) Edge robotic platforms: Yahboom Rosmaster wheeled robot and Unitree\\nGo2 dog robot; (c) Raspberry Pi 5 edge node (4× Cortex-A76 CPU, 16\\nGB LPDDR5, multi-level cache); (d) NVIDIA Jetson Orin Nano 8 GB edge\\nAI accelerator (6-core Cortex-A78AE CPU, Ampere GPU, 8 GB LPDDR5,\\nmulti-level cache); (c) Cache hierarchy and power-measurement setup using\\na USB power meter and onboard INA3221 monitor.\\ndecode detections with confidence thresholds(0.25,0.45)\\nfor qualitative inspection. The trained student is exported to\\nONNX for deployment and integration into the RL network.\\nNavigation policy is trained in MiniWorld environment,\\nconsisting of a rectangular room with three colored boxes (red,\\nblue, black) placed at random non-overlapping positions. At\\nthe beginning of each episode, one of the objects is randomly\\nselected as the target, and its class is encoded as a one-hot\\ngoal vector. The policy is trained with the PPO algorithm.\\nWe use a learning rate of3×10 −4, a batch size of128, and\\nepisode length of1024steps. The agent is trained for a total of\\n500,000timesteps. The reward function is described in Table I,\\ncombining sparse success and failure signals with dense terms\\nfor distance reduction, exploration, and first-goal visibility. All\\nexperiments are done on an NVIDIA 4090 GPU.\\n3) Hardware Deployment Platforms:To validate our ap-\\nproach in real-world settings, we deployed it on two edge\\nplatforms, an NVIDIA Jetson Orin Nano (8 GB RAM) and\\na Raspberry Pi 5 (16 GB RAM), mounted on the legged\\nUnitree Go 2 robot and the Yahboom Rosmaster wheeled robot\\n(Fig. 6 (b)). Power consumption was measured on both de-\\nvices, as illustrated in Fig. 6 (c), and their memory hierarchies\\nand CPU/GPU architectures are shown in Fig. 6 (d).\\nB. Results and Discussion\\n1) Mamba Model Optimization:Table II presents the per-\\nformance of the EdgeNavMamba teacher and student models\\nin mAP compared to the existing shape detection models.\\nKnowledge distillation effectively reduces model size and\\nFLOPs, without degrading performance. Meanwhile, our stu-\\ndent model achieves a 31% reduction in the number of param-\\neters compared to the baseline, while maintaining competitive\\naccuracy. Detections are evaluated in both MiniWorld and\\nIsaacLab simulators for comprehensive analysis. Fig. 7 illus-\\ntrates these environments along with examples of detections\\nmade by the agent in various scenarios.\\nFig. 7. MiniWorld and IsaacLab samples of environments and object\\ndetections by the agent during exploration using EdgeNavMamba-ST. The\\nenvironment contains three boxes placed at random, non-overlapping posi-\\ntions, with one randomly chosen as the target each episode.\\nFig. 8. Success rate of navigation toward a defined goal during training in\\ndifferent environment complexities. Each value is calculated over the last 100\\nepisodes. In each case, one box is designated as the goal, while the others\\nserve as distractions.\\n2) RL-Driven Goal Navigation:We evaluated EdgeNav-\\nMamba for navigation in MiniWorld using three scenarios.\\nIn each, one box was designated as the goal while the others\\nserved as distractions. In the first case, only one object was\\npresent; in the second, two objects were present, one being\\nthe goal; and in the third, three objects including one goal\\nwere placed. Fig. 8 shows success rates for these scenarios\\nover the last 100 training episodes. In the first case, the agent\\nachieved a 100% success rate, confirming accurate detection\\nduring navigation. In the second and third cases, the agent\\nachieved 94% and 90% success rates, respectively.\\n3) On-Device Energy Profiling:In Fig. 9, we\\nevaluate knowledge distillation by comparing the\\nbaseline EdgeNavMamba-TR with its distilled variant,\\nEdgeNavMamba-ST, on two representative edge platforms.\\nOn the Jetson Orin Nano, EdgeNavMamba-ST achieves a 63%\\nreduction in energy per inference while improving throughput.\\nLikewise, on the Raspberry Pi 5, EdgeNavMamba-ST delivers\\na 73% energy reduction, demonstrating substantial efficiency\\ngains with only negligible power overhead.\\nVI. CONCLUSION\\nIn this work, we presented EdgeNavMamba, an RL-based\\nframework designed for goal navigation using an efficient\\nMamba-based object detection model. By combining archi-\\ntectural modifications and knowledge distillation on the object', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5c71ca0f-1b69-48ee-81ad-fe95a1304e16', embedding=None, metadata={'page_label': '6', 'file_name': '2510.14946v1.pdf', 'file_path': '/content/drive/My Drive/data/2510.14946v1.pdf', 'file_type': 'application/pdf', 'file_size': 6954992, 'creation_date': '2025-11-02', 'last_modified_date': '2025-10-18', 'last_accessed_date': '2025-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TABLE II\\nCOMPARISON OFEDGENAVMAMBA WITH PRIOR MODELS ON THE\\nSHAPES DATASET. YOLOV5S[26], [37] (32-BIT)ANDSQUEEZEDEDGE\\nYOLO [26] (8-BIT)DO NOT REPORTFLOPS.\\nBlockParams Size FLOPs mAP\\nYOLOv5s [26] 7.3 M 237 MB - 0.96\\nSqueezed Edge YOLO [26] 931 k 7.5 MB - 0.95\\nEdgeMambaNav-TR 2.4 M 9.1 MB 0.47 G 0.93\\nEdgeMambaNav-ST 639 k 2.5 MB 0.15 G 0.93\\nFig. 9. Energy and performance comparison of proposed EdgeNavMamba-\\nTR and EdgeNavMamba-ST on Jetson Orin Nano and Raspberry Pi 5 16GB.\\ndetection model, we achieved a 31% reduction in the number\\nof parameters compared to the baselines while preserving de-\\ntection accuracy. The student model also, achieved a reduction\\nof 67% in size, and up to 73% in energy per inference on edge\\ndevices of NVIDIA Jetson Orin Nano and Raspberry Pi 5,\\nwhile keeping the same performance as the teacher model,\\nemphasizing the efficiency of the edge model. Navigation\\nresults in the MiniWorld simulator demonstrate over 90%\\nsuccess rate in various environment complexities.\\nREFERENCES\\n[1] Y . Wanget al., “Computation-efficient deep learning for computer\\nvision: A survey,”Cybernetics and Intelligence, pp. 1–24, 2024.\\n[2] M. Navardiet al., “Genai at the edge: Comprehensive survey on\\nempowering edge devices,”Proceedings of the AAAI SSS, 2025.\\n[3] U. Kallakuriet al., “ATLAS: Adaptive landmark acquisition using llm-\\nguided navigation,” inProceedings of the First Vision and Language for\\nAutonomous Driving and Robotics Workshop. OpenReview.net, 2024.\\n[4] M. Walczaket al., “ATLASv2: Llm-guided adaptive landmark acquisi-\\ntion and navigation on the edge,”arXiv:2504.10784, 2025.\\n[5] N. Tahiret al., “Edge computing and its application in robotics: A\\nsurvey,”arXiv preprint arXiv:2507.00523, 2025.\\n[6] R. Aalishahet al., “Medmambalite: Hardware-aware mamba for medical\\nimage classification,” 2025, 21st IEEE Biomedical Circuits and Systems\\nConference (BioCAS) 2025.\\n[7] Y . Xuet al., “Edge deep learning in computer vision and medical\\ndiagnostics: a comprehensive survey,”Artificial Intelligence Review,\\nvol. 58, no. 93, 2025.\\n[8] S. H. Leeet al., “Fast on-device learning framework for single-image\\nsuper-resolution,”IEEE Access, vol. 12, pp. 37 276–37 287, 2024.\\n[9] R. Aalishahet al., “Mambalitesr: Image super-resolution with low-rank\\nmamba using knowledge distillation,” inProceedings of the Interna-\\ntional Symposium on Quality Electronic Design (ISQED), 2025.\\n[10] M. Navardiet al., “Metatinyml: End-to-end metareasoning framework\\nfor tinyml platforms,”IEEE Embedded Systems Letters, 2024.\\n[11] A. N. Mazumderet al., “A survey on the optimization of neural\\nnetwork accelerators for micro-ai on-device inference,”IEEE Journal\\non Emerging and Selected Topics in Circuits and Systems, 2021.\\n[12] Y . Xieet al., “YOLO-ACE: A Vehicle and Pedestrian Detection\\nAlgorithm for Autonomous Driving Scenarios Based on Knowledge\\nDistillation of YOLOv10,”IEEE IoT Journal, Aug. 2025.\\n[13] M. Walczaket al., “Eden: Entorhinal driven egocentric navigation\\ntoward robotic deployment,”arXiv preprint arXiv:2506.03046, 2025.\\n[14] J. L. Melaet al., “Yolo-based power-efficient object detection on edge\\ndevices for usvs,”Journal of Real-Time Image Processing, 2025.\\n[15] J. Redmonet al., “You only look once: Unified, real-time object\\ndetection,” inProceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR). IEEE, 2016, pp. 779–788.\\n[16] Z. Wanget al., “Mamba yolo: A simple baseline for object detection with\\nstate space model,”Proceedings of the AAAI Conference on Artificial\\nIntelligence, vol. 39, no. 8, pp. 8205–8213, 2025.\\n[17] A. Gu and T. Dao, “Mamba: Linear-time sequence modeling with\\nselective state spaces,” arXiv preprint arXiv:2312.00752, 2023.\\n[18] W. Zixianget al., “Research on autonomous robots navigation based on\\nreinforcement learning,”2024 3rd International Conference on Robotics,\\nArtificial Intelligence and Intelligent Control (RAIIC), pp. 78–81, 2024.\\n[19] R. S. Sutton and A. G. Barto, “Reinforcement learning: An introduction,”\\nMIT Press, 1998.\\n[20] M. Navardiet al., “Metae2rl: Toward metareasoning for energy-efficient\\nmulti-goal reinforcement learning with squeezed edge yolo,”IEEE\\nMicro, 2023.\\n[21] G. Hintonet al., “Distilling the knowledge in a neural network,” 2015,\\nnIPS 2014 Deep Learning Workshop.\\n[22] NVIDIA, “Jetson orin nano developer kit getting started — nvidia\\ndeveloper,” https://developer.nvidia.com/embedded/learn/get-started-\\njetson-orinnano-devkit, 2025, accessed: 2025-06-06.\\n[23] Raspberry Pi, “Getting started with your raspberry pi,”\\nhttps://www.raspberrypi.com/documentation/computers/getting-\\nstarted.html, 2025, accessed: 2025-06-06.\\n[24] M. Navardiet al., “Genai at the edge: Comprehensive survey on\\nempowering edge devices,” inProceedings of the AAAI Symposium\\nSeries, vol. 5, no. 1, 2025, pp. 180–187.\\n[25] S. Hanet al., “Deep compression: Compressing deep neural networks\\nwith pruning, trained quantization and huffman coding,”arXiv preprint\\narXiv:1510.00149, 2015.\\n[26] E. Humeset al., “Squeezed edge yolo: Onboard object detection on\\nedge devices,”ML with New Compute Paradigms (MLNCP) Workshop\\nat NeurIPS, arXiv preprint arXiv:2312.11716, 2023.\\n[27] C.-Y . Wang and H.-Y . M. Liao, “Yolov9: Learning what you want to\\nlearn using programmable gradient information,” 2024.\\n[28] Y . Tian, Q. Ye, and D. Doermann, “Yolov12: Attention-centric real-time\\nobject detectors,”arXiv preprint arXiv:2502.12524, 2025.\\n[29] L. Zhuet al., “Vision mamba: Efficient visual representation learning\\nwith bidirectional state space model,” inProceedings of the International\\nConference on Machine Learning (ICML), 2024.\\n[30] A. Hatamizadeh and J. Kautz, “Mambavision: A hybrid mamba-\\ntransformer vision backbone,” arXiv preprint arXiv:2407.08083, 2024.\\n[31] B. Yanget al., “Multidistiller: Efficient multimodal 3d detection via\\nknowledge distillation for drones and autonomous vehicles,”Drones,\\nvol. 9, no. 5, p. 322, 2025.\\n[32] V . Mnihet al., “Human-level control through deep reinforcement learn-\\ning,”nature, vol. 518, no. 7540, pp. 529–533, 2015.\\n[33] J. Schulmanet al., “Proximal policy optimization algorithms,”arXiv\\npreprint arXiv:1707.06347, 2017.\\n[34] S. Nahavandiet al., “A comprehensive review on autonomous naviga-\\ntion,”ACM Computing Surveys, vol. 57, no. 9, pp. 1–67, 2025.\\n[35] M. Navardiet al., “Toward real-world implementation of deep rein-\\nforcement learning for vision-based autonomous drone navigation with\\nmission,”UMBC Student Collection, 2022.\\n[36] R. Bellman, “A markovian decision process,”Journal of Mathematics\\nand Mechanics, vol. 6, no. 5, pp. 679–684, 1957.\\n[37] T. Manjunathet al., “Reprohrl: Towards multi-goal navigation in the\\nreal world using hierarchical agents. on 37th aaai conference on artificial\\nintelligence,” inThe 1st Reinforcement Learning Ready for Production\\nworkshop, 2023.\\n[38] P. Viola and M. Jones, “Rapid object detection using a boosted cascade\\nof simple features,” inProceedings of the 2001 IEEE Computer Society\\nConference on Computer Vision and Pattern Recognition. CVPR 2001,\\nvol. 1. IEEE, 2001, pp. I–I.\\n[39] K. Heet al., “Deep residual learning for image recognition,” inPro-\\nceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), 2016.\\n[40] Y . Liuet al., “Vmamba: Visual state space model,”arXiv preprint\\narXiv:2401.10166, 2024.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are a helpful AI assistant.\"\n",
        "query_wrapper_prompt = SimpleInputPrompt(\"User: {query_str}\\nAssistant:\")\n"
      ],
      "metadata": {
        "id": "C1L2SMk5TvhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXTeb6oEUI1L",
        "outputId": "f3ab81bd-c919-4ee0-9a05-27b628ef5a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `RAG_Llamaindex` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `RAG_Llamaindex`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import Llama2 from higgingface\n",
        "import torch\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    model_name=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    device_map=\"auto\",\n",
        "    # uncomment this if using CUDA to reduce memory usage\n",
        "    model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "49fd0a22a703476a98bd316e9261ed74",
            "6c0932b1a3314525bd5c9d30d18a11ce",
            "2e6588199da84d258391c37bdd986829",
            "8541fe90412f4b12b8b433d4ad12c328",
            "ca6d6668f730434588235d0c9b02dcef",
            "0a4e9c5b02f74e8a8518733fb3e498cb",
            "3f18e9580002401d80fbe78148185a0d",
            "660eee45c00142b4aba235fba23919b3",
            "8a37a0e09c1e4b719ad2e90a2a6a14cc",
            "b1518dac52df455b82fe68ebce94fe8a",
            "ef708cb61047483e85ac6a70241febd8"
          ]
        },
        "id": "pWfctSq4VINu",
        "outputId": "ebfa876e-4abf-4473-8bd4-086dffd6a4a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49fd0a22a703476a98bd316e9261ed74"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MCfYif4CdxMI",
        "outputId": "f5783948-747d-4853-daf7-c33678f7d10b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-core<2.0.0,>=1.0.1 (from langchain-community)\n",
            "  Downloading langchain_core-1.0.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
            "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.44)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.38)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
            "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.0.2-py3-none-any.whl (469 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.3/469.3 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
            "Installing collected packages: requests, langchain-core, langchain-text-splitters, langchain-classic, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.79\n",
            "    Uninstalling langchain-core-0.3.79:\n",
            "      Successfully uninstalled langchain-core-0.3.79\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.11\n",
            "    Uninstalling langchain-text-splitters-0.3.11:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.11\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.2 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-classic-1.0.0 langchain-community-0.4.1 langchain-core-1.0.2 langchain-text-splitters-1.0.0 requests-2.32.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              },
              "id": "2939e424080443049449b9e5dc7f6e4d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from llama_index.embeddings import LangchainEmbedding\n",
        "\n",
        "\n",
        "embed_model = LangchainEmbedding(\n",
        "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563,
          "referenced_widgets": [
            "dd2134e6a93b4782a621e5f846cfda66",
            "029c3a2d739b49219d34349bcb764616",
            "adef4b2a3d4247e4993232cd16bc2ebb",
            "88a4335481a74622ad6c25baf6d9c807",
            "b234f89a3d9246828f0521c64ab8ad51",
            "ac299f00366e44bd911585a6dea563de",
            "c64ce4ea9b4945638cd6ea32e757d461",
            "c3398db0639e41189ea15ee105db2413",
            "4ad406d7fdea462c9df688ed8ed52946",
            "c1972cf2036045269b46f9ff4949fc0e",
            "f60246288cb144f3923dc93c7d3b28cf",
            "1d25771935f04b58bcbceba442e9aa38",
            "43107ac58603417b8d969172a4111052",
            "0ac106b655bf490e8c0e5d1968eab211",
            "a68f2ceb5b204c1da8e94aee985ad953",
            "eb5066bddfd049ed8c7e1c8a5ab2d295",
            "22dd85941b444d18b8cc86c85ca7ad81",
            "8b534547180f44f792d95d15f419c586",
            "10365bca7c754f9c9ff1a9ca4591517d",
            "9f8e3c5ef0ad4bdea8b74ed859adc3a8",
            "5b4c9691a7484215859acb0a6b2d279d",
            "90baffc645154e878945bbcb24565718",
            "3bafa1024bf34306b1f4a2436169ac34",
            "e0e829d07e764b9f816265336930fe69",
            "5d8578a89e9f497fbfa7310012be1b3e",
            "979ff162edbd48dcb70a289594ab6b9c",
            "5f70d34a3c094d36b11ead728a120431",
            "a1f4928df1fd4fa6bc21abd09c1a4d1b",
            "aca7c1e6df67444989400763f9ee20b8",
            "f68c1d4fbe3548a689e85fd1f9872a3b",
            "3bf887e6344348fe8e1f28f81446579a",
            "e7a8f89a6e7b4276a8e1d41efed84bb0",
            "2e955c31f097427181c3aca0f87b310d",
            "6910efaac6b64421b78576dc53a6372d",
            "a0e8c788dc4a466a9e903449987048c5",
            "d6397ab75dc843eebed37ec6e40ab970",
            "0afd88d2da8b4cc0b5de3754e6f9db62",
            "b5bb0860480a4708ad5b1c45213d121c",
            "22c3b48b90d445d281e98b761f4d7361",
            "965911bd10bc45d0b663e110adee3b4f",
            "dfc1e79d0cc0486195658c287f0abf5f",
            "8e50729ace2b494c9da2527c553450a2",
            "f5ea6257ac0145dc9792b769363692d4",
            "8b8a01a1f69e4b919e1b09774b78d663",
            "2d454ce9842b469aac37f61c5528f94d",
            "f339376e683547b29a2187c72c975e68",
            "6dad64b3b7294cb4a8ac45c4e0fc6e91",
            "c797014e38a4423daf5baef827d5c4b0",
            "8dd118b46068440c95fbc3a7e5aa4ea6",
            "20c9ea8706734da9b4ae5dec93c0064d",
            "fb6d6ac09d7541188aefff77de117bb1",
            "c5c95f1e45c54bbc98e5ec0409148991",
            "7e691d9ec1564da9ad7a0d327fbb4cbd",
            "ca684af7fcc64a9dbf1f39f056d7f62b",
            "4b2c2d57de9545ccb57bc693c2568bea",
            "5c6eb26aa29a4afda69adb849a887216",
            "66099114eb504dcb94e5a03463e3262b",
            "a319ec6da6ce44e5a59650845c88f15a",
            "c6736e9129324b2a85afdc27847b1322",
            "9f7bf65d4b3d48f2bd50a95dd5ff298e",
            "aa01e8198277423db3fcd402d22a8686",
            "371797d2729f4e84aa77b895dd110741",
            "4bf1af5ac103433d95b00500316354b8",
            "153213fe543b4ee7b0a34f58201534d0",
            "55997742487047088d85891f6f0ffdd2",
            "f87dba09070549e8b2f80713588fd6b4",
            "d2b4b1733e5b47fb99f3b17ac8f18c4f",
            "4c6c7e43545e4e21832d9835a610956e",
            "23360c8b656b4a00940a7384f6bd4d41",
            "b1d679b4ecdd40d280059094ae5a9fb5",
            "64130740650649bf9f4f8eb0056151d6",
            "bcb41b6dfb774e6ca16a36ace8d5f077",
            "bbc62430d9f14704b4606fab90edb024",
            "83ed99d9525742b3b9951e7cdb52fec8",
            "ed322c2276cd41e8a205063f8b6578cd",
            "c7e01b5258e94596b5e22f3a086878f1",
            "90077be705f142b8905f6adad8cd9294",
            "bfd55beb52034cf99d7ed758f306198f",
            "1af68146784f4386a63709ab39aa2c81",
            "8d5080a5563848b598ac402c632e20fc",
            "77c8efd93702405399ce607dde784d98",
            "b6d5bf55d810409899cd4882c09f2e8a",
            "b1393e2465af41d9bb7d1c5f3ae675b4",
            "c995725e9a1a4407962e4d61c0a7207d",
            "ef4dd98b26ee476c8cf1e6586e7dce9b",
            "9855a3c97d784f8aa04dd053d0e21f08",
            "83f4587b85364b6da8ea0171ec16dbb9",
            "0c6838823c8740568af6b3f8638217dd",
            "3bb540da26fd43718874ee412466d5cb",
            "c7af6c630b2145ef95e37bb590b75c09",
            "35e70828b8b64670adb7e201b20fc4ed",
            "6b86b57e43ba4d37bbc18a48b3a7ae4f",
            "4fd7a6f76f664a2e8943483097100d5f",
            "5256782bb432494c8a3dc13480926ea0",
            "d0d4cd481762471b94409ea301dd7714",
            "d5e816d23f124001a6edc6950b2ec15b",
            "0d4de21ef00a489ebcb3fcf2ebb32846",
            "65d055bf6cdd4384b87800784ad04181",
            "2f255a3010074f03bd1ac29068fb0fe1",
            "209c9260b073497ea1aa8222ad082852",
            "8002272b82864edc9d74a7ef517f0e13",
            "00d9680e3b584e22be1077b9c7f3d1c7",
            "dd6e75cd15a7492a99ee325260e377e8",
            "c1d8c66d3de349398baa8eeddcabb26b",
            "56a197f01dff436aba2ee4793364c1e2",
            "03a19c4ded8c4ecfbcdbc14dfa66fab5",
            "370492c059f44ec0bd686d9ebbfab9cc",
            "f8fcecf14ce548468bff6deefcb87b37",
            "f8275feb675f49dda07dc608e8a3af88",
            "c43632a6c7e941058009a5dfc500e914",
            "6d9bf0561f5c4bedb24523a0a7158d29",
            "03d43d2f67334953b70139a9f7fdd28e",
            "ec5464faf9224fc4b5ca4cc06044d72e",
            "c06cb371a57a4235a5a3da3211dece1a",
            "59dd32a0629f4c52ad1eded48dcf40d9",
            "12b0e3d3ad664ea898666b0c0216a216",
            "22026d7249d24415a5e87ddb1b3c331e",
            "9935101def584032a27ce7cec267defd",
            "5517af0c55ab4ba9bff2b3f48777e5f4",
            "0777704a77414c13a65715b60627da31",
            "c3b35735dfb744ce911fcbe9384045fe"
          ]
        },
        "id": "tgwNPxnxdM32",
        "outputId": "b709e74f-9b7e-4dc5-d11c-0f20756f4f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/llama_index/download/module.py:12: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n",
            "/tmp/ipython-input-2024854530.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd2134e6a93b4782a621e5f846cfda66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d25771935f04b58bcbceba442e9aa38"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3bafa1024bf34306b1f4a2436169ac34"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6910efaac6b64421b78576dc53a6372d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d454ce9842b469aac37f61c5528f94d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c6eb26aa29a4afda69adb849a887216"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2b4b1733e5b47fb99f3b17ac8f18c4f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bfd55beb52034cf99d7ed758f306198f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3bb540da26fd43718874ee412466d5cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "209c9260b073497ea1aa8222ad082852"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d9bf0561f5c4bedb24523a0a7158d29"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "service_context=ServiceContext.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "id": "5cc2D6XFeLg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjAtFYo5hIw8",
        "outputId": "03aba73a-94d9-431e-853d-e64553870381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ServiceContext(llm_predictor=LLMPredictor(system_prompt=None, query_wrapper_prompt=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>), prompt_helper=PromptHelper(context_window=4096, num_output=256, chunk_overlap_ratio=0.1, chunk_size_limit=None, separator=' '), embed_model=LangchainEmbedding(model_name='sentence-transformers/all-mpnet-base-v2', embed_batch_size=10, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7835c2eef1d0>), transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7835c2eef1d0>, id_func=<function default_id_func at 0x7837773fab60>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')], llama_logger=<llama_index.logger.base.LlamaLogger object at 0x7835c30c9c10>, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7835c2eef1d0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index=VectorStoreIndex.from_documents(documents,service_context=service_context)"
      ],
      "metadata": {
        "id": "1VdVut0ahMrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine=index.as_query_engine()"
      ],
      "metadata": {
        "id": "nFBWntXBiNvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=query_engine.query(\"what is EdgeNavMamba\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lnkc_vsiQaq",
        "outputId": "aef639fb-bf6b-473e-a9eb-035d798bca8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EdgeNavMamba is a computer vision model designed for edge devices, specifically for object detection and navigation tasks. It consists of an object detection model and a navigation policy. The object detection model, called EdgeNavMamba-Detector, is based on a modified version of MedMambaLite architecture. It includes units such as Patch Embedding, Lite-Conv-SSM-Block, Lite 2D-Selective-Scan, Patch Merging, and a Detector. The model is optimized for edge devices by factorizing convolutions, sharing projection weights, and reusing Mamba weight matrices across blocks. The navigation policy, on the other hand, uses the object detection results to navigate in a simulated environment called MiniWorld. The approach includes training the teacher model, distilling the student model, and training the RL policy using the student model for object detection.\n"
          ]
        }
      ]
    }
  ]
}